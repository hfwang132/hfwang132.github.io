---
title: "量子计量学（Quantum Metrology）简明入门"
date: 2024-04-23T19:05:46+08:00
draft: false
tags: ["量子计量"]
categories: ["量子信息"]
---

# 一、量子计量学介绍

所谓量子计量学（Quantum Metrology），就是利用量子物态的量子性质进行精密测量的学问。

之所以要研究量子计量学，是因为任何物理量的测量精度都由量子力学中的海森堡不确定性原理所限制，这叫做海森堡极限（Heisenberg Limit）。如何逼近以及提升这个极限，就是量子计量学的目标。

当然，日常生活中的大部分测量都不需要达到海森堡极限（例如，称体重）。但是在高精度成像以及各种高精度科学实验中，人们的确早就已经接近了海森堡极限。

> **引力波**：**LIGO**（Light Interferometer Gravitational-wave Observatory）是人们用来探测引力波的光干涉仪。它能探测到的干涉臂的长度变化达到了 $10^{-19}$ m 的级别，只有一个质子的大小的万分之一，更是光的波长的亿万分之一。在这个疯狂的精度级别下，LIGO 利用了光场的**压缩态**来降低量子噪声。


当然，量子噪声并非只在 LIGO 这样的极端情况下出现。下面我们给出两个关于成像的例子。


> **散粒噪声**：你一定用过 CMOS 相机。你是否注意过，在环境较暗的情况下，拍出来的照片会有很多噪点？其实它们是由 **散粒噪声**（Shot noise）  导致的。什么是散粒噪声？光子激发出来的电子是一个一个的，且这些电子的产生符合泊松分布（产生的时间完全随机），其相对方差反比于平均电子数，这个方差就是散粒噪声。可见，当电子数量较少的时候，其相对方差会比较大，信噪比（SNR）较低，导致了噪点的出现。


> **衍射极限**：成像精度除了时间上的衡量（即信噪比）以外，还有空间上的衡量，即像素点的个数。如果我们无限增加像素点的个数，有可能达到无限的精度吗？学过光学的你知道这不可能，因为有**衍射极限**的存在。实际上，衍射极限也是一种海森堡极限，因为它对应一种动量-位置不确定性。如果我们需要更准确的位置，衍射极限告诉我们，在不改变波长的前提下，需要增加入瞳（aperture）的大小。从海森堡原理的角度来看，这是因为以不同角度进入入瞳的光子拥有不同的动量，这些动量信息在成像的那一刻丢失了，而丢失的动量信息越多，光子的位置就测量地越准确。反过来，在不改变入瞳大小的前提下，我们需要减小波长，这是因为波长越小，动量越大，相同入瞳所接收到的光子的动量范围也越大。

这些例子都很有趣。希望你 enjoy。当然，更有趣的还在后头呢：人们用什么样的数学工具刻画量子噪声？人们怎样设计量子态以达到海森堡极限？看完这篇文章，你会深入了解这些问题，并且完成对量子计量学的入门。

# 二、极大似然估计

什么是测量？首先，我们需要给测量下一个定义。注意到，对同一个物理量进行测量总是会以一定概率给出一系列各不相同的结果（无论是经典物理还是量子物理）。我们通常把这些结果的平均值作为最终结果。

可见，测量过程实际上给出了一系列**随机变量**（单次测量结果），而我们通过构造像平均值这样的**统计量**（即随机变量的函数）来估计物理量本身。我们把这个统计量叫做估计量（estimator）。

这个估计量本身也是一个随机变量，也有均值和方差。我们将该估计量的均值称为测量结果，其方差称为测量误差。相对误差就是误差除以均值；测量精度就是相对误差的倒数。

到此为止，我们就定义好了测量过程，它本质上是一个**参数估计**问题：

> **参数估计**：给定一个概率密度函数 $p(\mathbf{x};\theta)$，其中 $\mathbf{x}$ 是测量结果，$\theta$ 是要测量的物理量（它是概率密度函数的一个参数，而非自变量），构造估计量 $\hat{\theta} = \hat{\theta}(\mathbf{x})$ 使得它的均值等于或尽量接近待估计的参数（**无偏性**），且方差尽可能地小（**有效性**）。另外，还有一个比较隐蔽的条件：我们要求样本数量越大，估计量就越接近参数的真实值（**一致性**）。

> **无偏性**：无偏性就是指 $\mathbb{E}(\hat{\theta}(X)) = \theta$，即估计量的均值等于参数的真实值。

> **有效性**：怎样刻画有效性？后面我们会介绍，估计量的方差有 CR 下界：$\delta \hat{\theta}^2 \ge \frac{1}{F(\theta)}$，其中 $F(\theta)$ 叫做 Fisher 信息。有效性可以用 $F(\theta)/\delta \hat{\theta}^2 \le 1$ 来刻画。当 $F(\theta)/\delta \hat{\theta}^2 = 1$，即达到下界时，我们就称该统计量是有效的。

> **一致性**：一致性要求 $\lim_{n\rightarrow \infty} P(|\hat{\theta} - \theta|>\epsilon) = 0$ 对于任意正数 $\epsilon > 0$ 成立。其中 $n$ 是样本个数。这也叫做依概率收敛 $\plim_{n\rightarrow \infty} \hat{\theta} = \theta $。它比较 subtle，但不是本文的重点。

统计学上对付参数估计问题最常用的方法就是极大似然估计（Maximum Likelihood Estimation，MLE）。它的思想很简单，就是在众多参数中找到这样一个参数，使得在该参数的前提下，测量到已有结果的概率是最大的。

具体步骤也很简单：

> **极大似然估计**：设似然函数（likelihood function） $f(\theta; \mathbf{x}) = p(\mathbf{x}; \theta)$，并找它的最大点 $\hat{\theta}(\mathbf{x}) = \operatorname{arg\\,max} f(\theta; \mathbf{x})$。
> 
> $\hat{\theta}(\mathbf{X})$ 就是极大似然估计所给出的估计量。
> 
> 在实际操作中，人们通常先对似然函数取对数来简化计算，即 $\log f(\theta; \mathbf{x})$，称为对数似然函数（log-likehihood function）。

实际上，极大似然估计通常不满足无偏性，但是人们依然非常青睐极大似然估计。这是因为极大似然估计总是渐近有效（样本趋于无穷时，方差能够达到 CR 下界），且渐近无偏的（样本数量趋于无穷时，估计量的均值趋于参数真实值）。

在量子测量中，人们在绝大部分情况下也使用极大似然估计。因此这篇文章将会集中在极大似然估计上。

> 其他的参数估计方法还有贝叶斯估计、矩估计。它们和极大似然估计构成最重要的三种参数估计方法。

# 三、Fisher 信息和 CR 下界

似然函数是极大似然估计的核心。我们来看看从似然函数出发能得到哪些量。

首先得到的量叫做 Score Function（不知道中文翻译），它是对数似然函数关于参数真实值的导数：

> **Score Function**: Score Function $s(\theta; \mathbf{x})$ 定义为：
> 
> $s(\theta; \mathbf{x}) = \frac{\partial}{\partial \theta} \log f(\theta; \mathbf{x})$，
> 
> 其中 $f(\theta; \mathbf{x})$ 为似然函数。

它的意义也很明确：就是参数值的变化对似然函数的影响。如果导数为正，说明参数值需要变大，因为参数真实值变大会导致似然函数变大（即概率更大）；反之，如果导数为负，说明参数值需要变小。那么什么时候参数值是理想的呢？那当然就是导数为零的时候啦。这也很好理解：似然函数最大点的导数为零。

当然，导数为零只是必要条件，而不是充分条件。为了保证似然函数的导数零点对应最大值，还需要二阶导数为负。于是我们再定义 Fisher 信息：

> **Fisher 信息（Fisher Information）**：Fisher 信息是对数似然函数（关于参数 $\theta$ 的）负二阶导数的（关于随机变量 $X$ 的）期望值：
> 
> $F(\theta) = \mathbb{E}\left(-\frac{\partial^2}{\partial \theta^2} f(\theta, X)\right)$

实际上可以证明，Fisher 信息总是非负的。因此令似然函数的导数为零总是能得到极大值。

> **Fisher 信息的意义**：Fisher 信息的意义是：似然函数在最大值附近对于参数的敏感程度。直觉上来说，似然函数越敏感，我们的估计就越有效。因为在似然函数对参数敏感的情况下，参数的少量变动就会导致似然函数的急剧下降，因此我们能以较高的确信度将参数确定下来。反之，如果似然函数对参数不敏感，那我们就不能将参数确定在较小的范围内，这是因为参数在较大的范围内变动不会导致似然函数的显著下降。

理解了 Fisher 信息的意义之后，我们就能很快理解 Cramer-Rao 下界了：

> **Cramér-Rao Bound**：估计量的方差有如下的下界：
> 
> $(\Delta \theta)^2 \le \frac{1}{F(\theta)}$
> 
> 其中 $F(\theta)$ 是 Fisher 信息。这叫做 Cramér-Rao 下界。

这其实就是将 Fisher 信息的意义量化为了一个不等式。Fisher 信息越大，似然函数对参数越敏感，参数的估计量的方差就可以越小。反之，Fisher 信息越小，似然函数对参数越不敏感，参数的估计量的方差就越大。

到此为止，我们介绍的极大似然估计、Fisher 信息以及 CR 下界，跟量子力学都没有任何关系，但它们都是必要的基础。在下一节中，我们将会引入量子 Fisher 信息和量子 CR 下界，并将它们应用到量子测量中。

# 四、量子 Fisher 信息和量子 CR 下界



# 五、量子相位估计



# 六、量子成像与量子照明



# 七、其他的量子参数估计问题

## 7.1 Gain sensing

## 7.2 Noise sensing

