[{"categories":["Mathematical Physics"],"content":"One Differential is an Infinitesimal? Physicists like to think of differentials as very small quantities, which is convenient for calculations but gives a feeling of lack of rigor. In fact, it is indeed lacking rigor, and the second mathematical crisis arose from this. Rigor and clarity are always complementary. Treating differentials as infinitesimal satisfies intuition but cannot withstand rational scrutiny. ","date":"2024-03-13","objectID":"/understanding_differentials/:1:0","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Mathematical Physics"],"content":"Two Differential as Linear Function I like to think of differentials as a machine, for example, $f: f(x,y)=x^2+2y^2$ The differential at $(1,1)$ is a machine like this: It takes two numbers as input and produces one number as output. Written out, it looks like this: $\\mathrm{d}f|_{(1,1)}(a,b)=\\frac{\\partial f}{\\partial x}\\bigg|_{(1,1)}a+\\frac{\\partial f}{\\partial y}\\bigg|_{(1,1)}b=2a+4b$ Since the partial derivatives may vary at each point, if we say the differential at a certain point is a machine, then the differentials collectively form an ocean of machines. You may also notice that this machine is “linear”; it is simply a linear map from $\\mathbb{R}^2$ to $\\mathbb{R}$. The knowledge of linear algebra tells us that the collection of linear functions forms a linear space. In other words, linear functions themselves are “vectors” that can be added and scaled. Going back to the analogy above, it means we can add two machines: $\\mathrm{d}f|_p+\\mathrm{d}g|_p$ , and also scale a machine by a factor: $\\lambda \\cdot \\mathrm{d}f|_p $ . The subscript $p$ indicates the differential at point $p$. What does it mean to add machines? For example, if $\\mathrm{d}f|_p(a,b)=2a+4b,,, \\mathrm{d}g|_p(a,b)=a-b$ , then $\\mathrm{d}f|_p(a,b)+\\mathrm{d}g|_p(a,b)=3a+3b$ . This is similar to adding two vectors: $(2,4)+(1,-1)=(3,3)$ . In fact, they are vectors. Yes, you heard it right, differentials are vectors (fields). ","date":"2024-03-13","objectID":"/understanding_differentials/:2:0","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Mathematical Physics"],"content":"Three Differential as Covector Field ","date":"2024-03-13","objectID":"/understanding_differentials/:3:0","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Mathematical Physics"],"content":"3.1 Tangent Space If you want to study differentials on a curved surface, then you need to redefine some concepts. For example, differentials are linear, but manifolds are curved, so we need to define something linear within this curvature. This linear thing is the tangent space. As the name suggests, the tangent space is a tangent plane to a curved surface at a certain point. But how do we define it? The curvature here is not embedded in another space but intrinsic to space itself. We cannot find the equation of a tangent plane as in classical analytic geometry. Let’s try a different approach: Firstly, it is linear, which means it is a vector space. A vector space is a group equipped with scalar multiplication. Specifically, its elements (called vectors) satisfy the following properties: (1) Commutativity of vector addition (2) Associativity of vector addition (3) Existence of an identity element for vector addition (similar to zero) (4) Existence of an inverse element for vector addition (similar to the negative number) The above four properties indicate that the vector space is an abelian group with respect to vector addition. (5) Existence of a multiplicative identity for scalar multiplication (6) Associativity of scalar multiplication (7) Distributivity of scalar multiplication with respect to vector addition (8) Distributivity of scalar multiplication with respect to scalar addition A vector space does not have to be $\\mathbb{R}^n,,\\mathbb{C}^n$, as long as a set is defined with addition and scalar multiplication operations satisfying the above properties, it is a vector space. We can define such a vector space, where the elements are linear maps from a set of smooth functions to the real number field: $v:\\mathcal{F}_M\\rightarrow\\mathbb{R}$ such that (1) $v(\\lambda f+\\mu g)=\\lambda v(f)+\\mu v(g)$ (linearity) (2) $v|_p(f\\cdot g)=f|_p\\cdot v(g)+g|_p\\cdot v(f)$ (Leibniz law) where $\\mathcal{F}_M$ is the set of all smooth functions on the differential manifold $M$. It can be proven that the collection of $v$ forms a vector space. Although this definition seems complicated, it is actually just finding the directional derivative of a function at a certain point. Taking a bivariate function as an example, we can explicitly write out an instance of $v$: $\\left(a\\frac{\\partial}{\\partial x}+b\\frac{\\partial}{\\partial y}\\right)\\bigg|_p$ If $(\\frac{\\partial}{\\partial x}\\bigg|_p,\\frac{\\partial}{\\partial y}\\bigg|_p)$ is taken as the basis, then its coordinates are $(a,b)$. This linear space is the tangent space at point $p$ on the manifold, denoted as $T_pM$. All these operations are at a certain point $p$, where each point grows a tangent space. You can imagine that every point on a curved surface grows a tangent plane. ","date":"2024-03-13","objectID":"/understanding_differentials/:3:1","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Mathematical Physics"],"content":"3.2 Cotangent Space We just introduced the tangent space $T_p M$ . Now let’s call the dual space of the tangent space $T_p^*M$ the cotangent space. The differential at a point is precisely an element in the cotangent space. In other words, the differential at a certain point is a cotangent vector. Elements in the dual space (cotangent vectors) are linear functionals that map elements in the original space to a number. This is exactly what we said before about machines: But now the input should be elements in the tangent space: $\\left(a\\frac{\\partial}{\\partial x}+b\\frac{\\partial}{\\partial y}\\right)\\bigg|_p$ . We said that differentials form an ocean of machines. Therefore, the ocean of cotangent vectors (i.e., differentials) can be called the cotangent vector field. In mathematics, the differential (cotangent vector field) has a cool name called the cotangent bundle projection. For more details, see: https://zhuanlan.zhihu.com/p/629852598 After talking so much abstract nonsense, let’s define it below: The differential at a point $p$ on manifold $M$ is such a linear functional that acts on elements $v|_p\\in T_pM$ in the tangent space and produces $\\mathrm{d}f|_p(v|_p)=v|_p(f)$ Let’s take a more explicit example, for a bivariate function: $\\mathrm{d}f|_{p}:\\mathrm{d}f|_{p}(v|_p)=v|_p(f)= \\frac{\\partial f}{\\partial x} \\bigg|_{p} a+\\frac{\\partial f}{\\partial y}\\bigg|_{p}b$ , where $v|_p=\\left(a\\frac{\\partial}{\\partial x}+b\\frac{\\partial}{\\partial y}\\right)\\bigg|_p$ ","date":"2024-03-13","objectID":"/understanding_differentials/:3:2","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Mathematical Physics"],"content":"Four Differential as Linear Mapping Characterized by Jacobian Matrix So far, we have discussed functions that map to domains $\\mathbb{R},\\mathbb{C}, \\cdots$. What if the target domain can be $\\mathbb{R}^2, \\mathbb{R}^3,\\cdots$? For example, $\\bm{f}:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ . In this case, $\\bm{f}$ can be seen as $m$ functions: $f_i:\\mathbb{R}^n\\rightarrow\\mathbb{R},,,i=1,\\cdots,m$ Then we have $\\begin{bmatrix}\\mathrm{d}f_1 \\\\ \\mathrm{d}f_2 \\\\ \\vdots \\\\ \\mathrm{d}f_m \\end{bmatrix}= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1}\u0026\\frac{\\partial f_1}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1}\u0026\\frac{\\partial f_2}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1}\u0026\\frac{\\partial f_m}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\\begin{bmatrix}\\mathrm{d}x_1 \\\\ \\mathrm{d}x_2 \\\\ \\vdots \\\\ \\mathrm{d}x_n \\end{bmatrix}$ where $\\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1}\u0026\\frac{\\partial f_1}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1}\u0026\\frac{\\partial f_2}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1}\u0026\\frac{\\partial f_m}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}$ is the Jacobian matrix. In fact, the differential is the linear mapping characterized by the Jacobian matrix. Below, we explain in detail. For a univariate function, the Jacobian matrix is just the derivative. Although we are used to thinking of derivatives as slopes, let’s change our perspective: If you consider the action of a univariate function as stretching along the number line, then the derivative is the local stretching ratio. The advantage of this perspective is its ease of generalization to multivariate functions. Because, for multivariate functions, you often cannot draw a curve as you would for univariate functions. So, as an alternative method, you can think of a multivariate function as a space being stretched, such as $f: R^2→R^2$. You can imagine that each point in $R^2$ is stretched to a new point by $f$. If you zoom in on this local stretch, it looks like the case of a univariate function—linear—parallel lines are stretched to parallel lines. And what we call the differential is precisely this local linear mapping! The matrix of this linear mapping is the Jacobian matrix. For more detailed explanations, see lectures 71-72 of Khan Academy’s Multivariable Calculus course: https://www.youtube.com/watch?v=bohL918kXQk ","date":"2024-03-13","objectID":"/understanding_differentials/:4:0","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Quantum Information"],"content":"In the previous section, we discussed the principle of measuring the quantum second-order correlation function with a non-photon-number-resolving single-photon detector in the HBT experiment. https://zhuanlan.zhihu.com/p/679453473 In this section, let’s see what the quantum second-order correlation function is used for. In addition to the common uses such as distinguishing between super-Poissonian statistics/Poissonian statistics/sub-Poissonian statistics and distinguishing between photon bunching/anti-bunching, the HBT experiment can also be used to measure the spectral purity of multimode squeezed states. ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:0:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"Generation of Multimode Squeezed States via SPDC SPDC (Spontaneous Parametric Down-Conversion) is an experimental method used to generate correlated photon pairs. By shining a laser onto a nonlinear crystal, spontaneous parametric down-conversion occurs if the phase matching condition is satisfied. In this process, one pump photon is converted into two photons with equal frequencies, known as the signal photon and the idler photon, respectively, satisfying energy conservation and momentum conservation. The Hamiltonian for this process is given by: $H = i\\hbar g,a^\\dag_s a^\\dag_i a_p + h.c.$ where $g$ represents the interaction strength. When the pump photon is strong enough that its intensity is almost unaffected, we can focus only on the subspace of signal and idler photons. In this case, the Hamiltonian becomes: $H = i\\hbar g \\alpha_p ,a^\\dag_s a^\\dag_i + h.c.$ Without loss of generality, let $\\alpha_p$ be a real number, and let $\\chi=g\\alpha_p$. Then, $H = i\\hbar \\chi (a^\\dag_s a^\\dag_i - a_s a_i)$ In reality, due to the finite spatial extent of the pump, there will be uncertainty in momentum, manifested as a finite linewidth in the spectrum. In this case, the Hamiltonian becomes: $\\begin{aligned} H=i\\hbar \\chi \\int d\\omega_s d\\omega_i [f(\\omega_s,\\omega_i) a_s^\\dag(\\omega_s) a_i^\\dag(\\omega_i) -f^*(\\omega_s,\\omega_i) a_s(\\omega_s) a_i(\\omega_i)] \\end{aligned}$ Now, we expand $f(\\omega_s,\\omega_i)$ using a set of orthogonal basis functions (e.g., Hermite functions) ${\\varphi_k}_{k\\in \\mathbb{N}}$: $\\begin{aligned} f(\\omega_s,\\omega_i)=\\sum_{k,l} C_{kl}\\varphi_k(\\omega_s)\\varphi_l(\\omega_i) \\end{aligned}$ and denote $a_k^\\dag = \\int d\\omega_s ,\\varphi_k(\\omega_s)a_s^\\dag(\\omega_s),\\quad b_k^\\dag = \\int d\\omega_i ,\\varphi_k(\\omega_i) a_i^\\dag(\\omega_i)$ , then we have $\\begin{aligned} H=i\\hbar \\chi \\sum_{k,l}\\left( C_{kl}a^\\dag_k b^\\dag_l - C^*_{kl} a_kb_l\\right) \\end{aligned}$ Performing a Schmidt decomposition (singular value decomposition) on it, we get $\\begin{aligned} H=i\\hbar \\chi \\sum_{k}\\left( \\lambda_k A^\\dag_k B^\\dag_k - \\lambda_k A_k B_k\\right) \\end{aligned}$ where $\\sum_k\\lambda^2_k=1$. Note: Since the spectrum of singular value decomposition is always non-negative real numbers, $\\lambda_k$ is real and greater than zero. This is actually the Hamiltonian of multimode squeezed states. Review of various squeezed states: Single-mode squeezed state: $\\begin{aligned} H=i\\hbar\\left(g^*\\frac{a^2}{2}-g\\frac{a^{\\dag2}}{2}\\right) \\end{aligned}$ Two-mode squeezed state: $\\begin{aligned} H=i\\hbar\\left(g^*a b-g a^\\dag b^\\dag\\right) \\end{aligned}$ Multimode squeezed state: $\\begin{aligned} H=i\\hbar\\sum_{k}\\left(g_k^*a_k b_k-g_k a_k^\\dag b_k^\\dag\\right) \\end{aligned}$ Now let’s separately look at the system evolution in the Schrödinger picture and the Heisenberg picture. In the Schrödinger picture, $\\begin{aligned} |\\Psi_{\\text{out}}\\rangle \u0026=e^{\\frac{H\\tau}{i\\hbar}}|\\Psi_{\\text{in}}\\rangle \\\\ \u0026= \\sum_{k} \\frac{1}{\\cosh(r_k)}\\sum_{n=0}^{\\infty}\\tanh^n(r_k)|n_k,n_k\\rangle \\end{aligned}$ where $r_k=\\chi \\tau \\lambda_k$ . Note that it resembles a thermal light field. When $r_k$ is small, we have $|\\Psi_{\\text{out}}\\rangle\\approx \\sum_{k} \\left(|0_k,0_k\\rangle + r_k|1_k,1_k\\rangle\\right)$ This is also the case that most experiments conform to. Conditioned on detecting photons, the conditional state is $|\\Psi_{\\text{conditional}}\\rangle=\\sum_k\\lambda_k|1_k,1_k\\rangle$ . This is the principle of heralded single-photon sources. In the Heisenberg picture, $\\begin{aligned} A_k \u0026\\rightarrow e^{\\frac{H\\tau}{i\\hbar}} A_k e^{-\\frac{H\\tau}{i\\hbar}} \\\\ \u0026= A_k + [e^{\\frac{H\\tau}{i\\hbar}},A_k]+[e^{\\frac{H\\tau}{i\\hbar}},[e^{\\frac{H\\tau}{i\\hbar}},A_k]] + \\cdots \\\\ \u0026=\\cosh(r_k) A_k - \\sinh(r_k)B_k^\\dag \\end{aligned}$ $\\begin{aligned} B_k \u0026\\rightarrow e^{\\frac{H\\tau}{i\\hbar}} B_k e^{-\\frac{H\\tau}{i\\hbar}} \\\\ \u0026= B_k + [e^{\\frac{H\\tau}{i\\hbar}},B_k]+[e^{\\frac{H\\tau}{i\\hbar}},[e^{\\frac{H\\tau}{i\\hbar}},B_k]] + \\cdots \\\\ \u0026=\\cosh(r_k) B_k - \\sinh(r_k)A_k^\\","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:1:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"How to Characterize Spectral Correlation? Spectral correlation refers to the correlation between the spectra of the signal photon and the idler photon. An uncorrelated joint spectrum can be written as the product of the spectra of the two photons: $f(\\omega_s,\\omega_i)=\\phi(\\omega_s)\\psi(\\omega_i)$ . If it cannot be written in this form, it implies spectral correlation. How to characterize spectral correlation specifically? We can use the Schmidt number to quantify the degree of spectral correlation, defined as follows: $\\begin{aligned} K= \\frac{1}{\\sum_k\\lambda^4_k} \\end{aligned}$ Example For an uncorrelated joint spectrum, ${\\lambda_k}={1}$ , $K=1$ . For a joint spectrum with two modes and equal coefficients, ${\\lambda_k}={\\frac{1}{2},\\frac{1}{2}}$ , $K=2$ . For a joint spectrum with n modes and equal coefficients, $K=n$ . For a joint spectrum with n modes and coefficients not completely equal to each other, $1\u003cK\u003cn$ . Why do we care about spectral correlation? Because it is closely related to the purity of photons: Property When $r_k$ is small and photons are detected, the quantum state is $|\\Psi_{\\text{conditional}}\\rangle=\\sum_k\\lambda_k|1_k,1_k\\rangle$ . For this quantum state, tracing out one of the two photons (performing partial trace operation), the purity of the remaining photon state is $P=\\frac{1}{K}$ . This is because: $\\begin{aligned} P \u0026= \\operatorname{tr}[\\operatorname{tr}_2[|\\psi\\rangle\\langle\\psi|]^2] \\\\ \u0026= \\operatorname{tr}\\left[\\operatorname{tr}_B\\left(\\sum_{k}\\lambda_k \\left| 1_k, 1_k\\right\\rangle\\sum_{l}\\lambda_l \\left\\langle 1_l ,1_l\\right|\\right)^2\\right] \\\\ \u0026= \\operatorname{tr}\\left[\\left(\\sum_{k}\\lambda_k^2 \\left|1_k \\right\\rangle\\left\\langle 1_k\\right|\\right)^2\\right] \\\\ \u0026= \\operatorname{tr}\\left[\\sum_{k}\\left(\\lambda_k^2\\left| 1_k \\right\\rangle\\left\\langle 1_k\\right|\\right)^2\\right] \\\\ \u0026= \\operatorname{tr}\\left[\\sum_{k}\\lambda_k^4 |1_k\\rangle\\langle 1_k|\\right] \\\\ \u0026=\\sum_{k}\\lambda_k^4=\\frac{1}{K} \\end{aligned}$ It can be seen that the purity is inversely proportional to the Schmidt number. That is, the stronger the spectral correlation, the lower the purity of the photons in the frequency domain. This is unfavorable for heralded single-photon sources and various multi-photon interference experiments. For the former, spectral correlation will reduce the frequency-domain indistinguishability of single photons, and for the latter, spectral correlation will reduce the interference contrast. ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:2:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"Measuring Spectral Correlation with HBT Experiment In the previous section, we mentioned that the HBT experiment detects: $\\begin{aligned} \\frac{C_{12}R}{S_1S_2}=\\frac{\\langle n(n-1)\\rangle}{\\langle n\\rangle^2} \\end{aligned}$ where $C_{12}$ is the coincidence count rate of two detectors, $S_1$ and $S_2$ are the count rates of the first and second detectors, respectively, and $R$ is the photon flux. When the frequencies of various modes are the same or similar, $\\begin{aligned} \\frac{\\langle n(n-1)\\rangle}{\\langle n\\rangle^2} \\approx g^{(2)}(0) \\end{aligned}$ . The reason why we say that the HBT experiment can measure spectral correlation is because $\\begin{aligned} \\frac{\\langle n(n-1)\\rangle}{\\langle n\\rangle^2} = 1+\\frac{1}{K} \\end{aligned}$ . The proof is as follows: $\\begin{aligned} \\frac{\\langle n(n-1)\\rangle}{\\langle n\\rangle^2} \u0026=\\frac{\\langle n^2\\rangle-\\langle n\\rangle}{\\langle n\\rangle^2} \\\\ \u0026=\\frac{\\sum_{k} \\langle n_k^2 \\rangle + 2\\sum_{i\u003ej} \\langle n_i n_j \\rangle - \\langle n\\rangle}{\\langle n\\rangle^2} \\\\ \u0026= \\frac{ \\sum_{k} \\left(\\langle n_k \\rangle + 2\\langle n_k\\rangle^2 \\right) + 2\\sum_{i\u003ej} \\langle n_i \\rangle \\langle n_j \\rangle - \\langle n\\rangle}{\\langle n\\rangle^2} \\\\ \u0026= \\frac{\\langle n\\rangle + \\langle n\\rangle^2 +\\sum_{k} \\langle n_k\\rangle^2 - \\langle n\\rangle}{\\langle n\\rangle^2} \\\\ \u0026= 1+\\frac{\\sum_{k} \\langle n_k\\rangle^2}{ \\left\\langle \\sum_{k} n_k\\right\\rangle^2} \\\\ \u0026= 1 + \\frac{\\sum_k \\sinh^4(r_k)}{\\left(\\sum_k \\sinh^2(r_k)\\right)^2} \\\\ \u0026\\approx 1+\\frac{\\sum_k r_k^4}{\\left(\\sum_k r_k^2\\right)^2} \\\\ \u0026= 1 + \\frac{\\sum_k \\lambda_k^4}{\\left(\\sum_k \\lambda_k^2\\right)^2} \\\\ \u0026= 1+\\sum_k \\lambda_k^4 \\\\ \u0026= 1+\\frac{1}{K} \\\\ \u0026= 1+P \\end{aligned}$ where the third equality uses the result: for thermal light fields, $\\langle n_k^2 \\rangle =\\langle n_k \\rangle + 2\\langle n_k\\rangle^2$ , and there is no correlation between different modes $\\langle n_i n_j\\rangle=\\langle n_i\\rangle\\langle n_j\\rangle$ . The sixth equality is because in the Heisenberg picture, $\\begin{aligned} \\langle n_k\\rangle\u0026=\\langle 0|A_k^\\dag A_k|0\\rangle \\\\ \u0026\\rightarrow\\langle 0| (\\cosh(r_k) A_k^\\dag - \\sinh(r_k)B_k)(\\cosh(r_k) A_k - \\sinh(r_k)B_k^\\dag)|0\\rangle \\\\ \u0026= \\sinh^2(r_k) \\end{aligned}$ ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:3:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"Conclusion With the help of the HBT experiment, we can measure spectral correlation and purity, which is quite remarkable. Because ordinary methods require tunable filters to measure JSI (Joint Spectral ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:4:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"Note The earliest literature I could find for the derivation above is [1], but the literature [1] skips too many steps, and the logical chain in between was filled in by myself. The literature on measuring spectral correlation using HBT experiments can be found in [2] and [3], both of which cite literature [1]. ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:5:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"References [1] Christ, A., Laiho, K., Eckstein, A., Cassemiro, K. N. \u0026 Silberhorn, C. Probing multimode squeezing with correlation functions. New J. Phys. 13, 033027 (2011). [2] Faruque, I. I. et al. Estimating the Indistinguishability of Heralded Single Photons Using Second-Order Correlation. Phys. Rev. Applied 12, 054029 (2019). [3] Paesani, S. et al. Near-ideal spontaneous photon sources in silicon quantum photonics. Nat Commun 11, 2505 (2020). ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:6:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"HBT Experiment Those who have conducted quantum optics experiments must be familiar with the Hanbury Brown and Twiss (HBT) experiment, which can be used to measure the second-order correlation function g2. In this experiment, a beam of light is split into two using a 50:50 beamsplitter, and then each beam is detected separately by two detectors. The correlation of the intensities on both sides is measured as a function of delay, as shown in the figure below: Hanbury Brown and Twiss Experiment The classical normalized g2 is defined as: $\\begin{aligned} g^{(2)}(r_1, r_2; \\tau) \u0026= \\frac{\\langle E^*(r_1, t) E^*(r_2, t + \\tau) E(r_2, t + \\tau) E(r_1, t) \\rangle}{\\langle |E(r_1, t)|^2 \\rangle \\langle |E(r_2, t + \\tau)|^2 \\rangle} \\\\ \u0026= \\frac{\\langle I(r_1, t) I(r_2, t + \\tau) \\rangle}{\\langle I(r_1, t) \\rangle \\langle I(r_2, t + \\tau) \\rangle} \\end{aligned}$ where $E$ represents the electric field and $I$ represents the intensity. As per the definition, the HBT experiment measures the classical g2 by normalizing the correlated intensities with the product of the individual intensities. ","date":"2024-03-10","objectID":"/hbt_g2/:1:0","tags":["Quantum Optics"],"title":"Probing g2 with non-photon-number-resolving detectors","uri":"/hbt_g2/"},{"categories":["Quantum Information"],"content":"Quantum Second-Order Correlation Function The definition of the quantum normalized g2 is: $\\begin{aligned} g^{(2)}(r_1, r_2; \\tau) \u0026= \\frac{\\langle E^-(r_1, t) E^-(r_2, t + \\tau) E^+(r_2, t + \\tau) E^+(r_1, t) \\rangle}{\\langle E^-(r_1, t) E^+(r_1, t) \\rangle \\langle E^-(r_2, t + \\tau) E^+(r_2, t + \\tau) \\rangle} \\end{aligned}$ In the quantum case, the operators in the numerator should be replaced with the counter-rotating term $E^-$ and the co-rotating term $E^+$, which involve creation and annihilation operators. Additionally, the creation operator must precede the annihilation operator since the detection of a photon is an annihilation process, unlike the classical case. In classical coherent states $|\\alpha\\rangle$, the annihilation operator $a$ is an eigenstate $a|\\alpha\\rangle = \\alpha|\\alpha\\rangle$, hence the presence or absence of a photon makes no difference. In the case of single-mode, since $E^+ = i\\sqrt{\\frac{\\hbar \\omega}{2\\epsilon_0 V}}a$ and $E^- = i\\sqrt{\\frac{\\hbar \\omega}{2\\epsilon_0 V}}a^\\dagger$, the quantum g2 can be simplified to: $\\begin{aligned} g^{(2)}(r_1, r_2, \\tau) \u0026= \\frac{\\langle a^\\dagger(r_1, t) a^\\dagger(r_2, t + \\tau) a(r_2, t + \\tau) a(r_1, t) \\rangle}{\\langle n(r_1, t) \\rangle \\langle n(r_2, t + \\tau) \\rangle} \\end{aligned}$ Therefore, to measure quantum g2, we need to replace intensity detectors with photon detectors, and these photon detectors should be capable of distinguishing photon numbers. However, the problem arises because currently available single-photon detectors that can distinguish photon numbers are not very practical: TES detectors have long recovery times; schemes for Demultiplexing in space and time only offer pseudo-photon number resolution; and SNSPDs are expensive, require low temperatures, and their multi-photon efficiency may not be very high. ","date":"2024-03-10","objectID":"/hbt_g2/:2:0","tags":["Quantum Optics"],"title":"Probing g2 with non-photon-number-resolving detectors","uri":"/hbt_g2/"},{"categories":["Quantum Information"],"content":"NPNR-SPD for Measuring g2 Fortunately, single-photon detectors that cannot distinguish photon numbers (NPNR-SPDs, Non-Photon-Number-Resolving Single-Photon-Detectors) can still measure g2, even though they cannot resolve photon numbers! Comment: “Cannot distinguish photon numbers” means they can only differentiate between “no photons” and “one or more photons.” Common APD detectors cannot distinguish photon numbers. The method is simple: just make sure the quantum efficiency of the NPNR-SPD is low enough! Quantum efficiency $\\eta$ refers to the probability that the detector can detect a photon given that a photon enters it. Conversely, the probability of the photon being “lost” is $1 - \\eta$. At first, it seemed strange to me why lower efficiency is better. But with a little thought, the principle becomes clear: When the efficiency is low enough, the probability that $k$ photons are not all lost is $1 - (1 - \\eta)^k \\approx k\\eta$. In other words, the probability of the NPNR-SPD detecting a photon is proportional to $k$. Thus, we achieve “photon number resolution” indirectly through “low efficiency”. The following is a formal (boring) mathematical derivation, which can be skipped: [Derivation equations] Measuring g2 has several uses. Besides the common textbook distinctions of [super-Poissonian statistics/Poissonian statistics/sub-Poissonian statistics] and [photon bunching/antibunching], there is an unexpected application: measuring spectral correlations of photons, which we will discuss in the next part. ","date":"2024-03-10","objectID":"/hbt_g2/:3:0","tags":["Quantum Optics"],"title":"Probing g2 with non-photon-number-resolving detectors","uri":"/hbt_g2/"},{"categories":["Quantum Information"],"content":"Conclusion: Multi-Mode Scenario Previously, we assumed that the detected light was single-mode. Hence $E^+$ could be proportional to $a$: $E^+ = i\\sqrt{\\frac{\\hbar \\omega}{2\\epsilon_0 V}}a$. But what if the detected light is multi-mode? In this case, $E^+ = i\\sum_k\\sqrt{\\frac{\\hbar \\omega_k}{2\\epsilon_0 V}}a_k$, $E^+$ is no longer proportional to $a$, but has a frequency dependence: $E^+_k \\propto \\sqrt{\\omega_k}a_k$. The additional term $\\sqrt{\\omega_k}$ in the equation is not surprising since the intensity, which is proportional to the frequency times the number of photons, contributes to it. However, as long as the frequencies of the various modes are the same, we can still eliminate $\\sqrt{\\omega_k}$, and obtain: $\\begin{aligned} g^{(2)}(0) = \\frac{\\langle n(n-1) \\rangle}{\\langle n \\rangle^2} \\end{aligned}$ Thus, even if multiple modes are present, if these modes have (approximately) the same frequency, the method of measuring quantum g2 using the HBT experiment remains effective. ","date":"2024-03-10","objectID":"/hbt_g2/:4:0","tags":["Quantum Optics"],"title":"Probing g2 with non-photon-number-resolving detectors","uri":"/hbt_g2/"},{"categories":["Quantum Information"],"content":"Projection Measurement Traditionally, a measurement, in the sense of Von Neumann, is a series of projection operators. By performing spectral decomposition on the self-adjoint operator corresponding to the observable, denoted as $ O = \\sum_i \\lambda_i |\\varphi_i\\rangle\\langle\\varphi_i| $, we obtain these projection operators $ |\\varphi_i\\rangle\\langle\\varphi_i| $. This part is well-known to students who have studied elementary quantum mechanics. Besides the Von Neumann measurement, there is a more general type of measurement called Generalized Measurements or Positive Operator Valued Measures (POVMs). Generalized Measurements (POVMs) Definition: POVM A POVM is a mapping $ \\mathsf{E}: X \\rightarrow \\mathcal{L}(\\mathcal{H}) $, satisfying $ \\mathsf{E}(x) \\ge 0, \\quad \\forall x \\in X $ $ \\sum_ {x \\in X} \\mathsf{E}(x) = \\mathbb{I}_{\\mathcal{H}} $ where $ X $ represents the set of possible measurement outcomes, and $ \\mathcal{L}(\\mathcal{H}) $ represents the set of all bounded operators on $ \\mathcal{H} $. Born’s Rule The probability of obtaining result $ x $ when measuring a quantum state $ \\rho $ with a POVM $ \\mathsf{E} $ is given by $ p_{\\rho}^{\\mathsf{E}}(x) = \\operatorname{tr}[\\rho \\mathsf{E}(x)] $. When I first encountered POVMs, I was not clear about how they are implemented. This is because traditional measurements involve projection operators satisfying $ \\mathsf{E}(x)^2 = \\mathsf{E}(x), \\quad \\forall x \\in X $. However, general POVMs do not satisfy this condition. In fact, traditional projection measurements, also known as PVMs (Projection Valued Measures), are a special type of POVM. PVMs are to POVMs what pure states are to mixed states. In other words, a POVM is a statistical mixture of PVMs, just as a mixed state is a statistical mixture of pure states. Implementing POVMs So, how do we implement POVMs? We can actually use PVMs and composite systems to implement POVMs. Consider a system with a Hilbert space $ \\mathcal{H} $ and a state $ \\rho $. Next, we couple system $ \\mathcal{H} $ with another system $ \\mathcal{K} $. Suppose the initial state of system $ \\mathcal{K} $ is $ \\sigma $. Then, the initial state of the composite system is $ \\rho \\otimes \\sigma $ in $ \\mathcal{H} \\otimes \\mathcal{K} $. After that, we let the composite system evolve for a period of time, resulting in the system’s state becoming $ U(\\rho \\otimes \\sigma) U^\\dag $. Finally, we perform a PVM $ \\mathsf{Z} $ measurement on subsystem $ \\mathcal{K} $. According to Born’s rule, the probability of obtaining result $ x $ is $ \\operatorname{tr}[U(\\rho\\otimes \\sigma) U^\\dag(\\mathbb{I}_{\\mathcal{H}}\\otimes \\mathsf{Z}(x))] $. Thus, we have implemented a POVM $ \\mathsf{E} $ such that $ \\operatorname{tr}[\\rho \\mathsf{E}(x)] = \\operatorname{tr}[U(\\rho\\otimes \\sigma) U^\\dag(\\mathbb{I}_{\\mathcal{H}}\\otimes \\mathsf{Z}(x))] $. Measurement Model Let’s summarize the physical objects involved in implementing POVMs: the ancilla system $ \\mathcal{K} $, the initial state $ \\sigma $ of system $ \\mathcal{K} $, the evolution $ U $ of the composite system, and the projection measurement $ \\mathsf{Z} $ on the ancilla system. These physical objects together realize a POVM measurement. So, we can define $ \\mathfrak{M} = (\\mathcal{K},\\sigma,U,\\mathsf{Z}) $ as a Measurement Model. In actual experiments, the ancilla system $ \\mathcal{K} $ can be considered as a probe of the instrument, and $ \\mathsf{Z} $ is the readout of the probe. Post-Measurement State POVMs themselves cannot determine the post-measurement state. The actual determination of the post-measurement state depends on the specific implementation of the measurement, that is, the measurement model. For a measurement model $ \\mathfrak{M} = (\\mathcal{K},\\sigma,U,\\mathsf{Z}) $, the post-measurement state of system $ \\mathcal{H} $ is $ \\rho_x = \\operatorname{tr}_{\\mathcal{K}}[U(\\rho\\otimes \\sigma) U^\\dag(\\mathbb{I}_{\\mathcal{H}}\\otimes \\mathsf{Z}(x))] $, where $ \\operatorname{tr}_{\\mathcal{K}}[\\cdot] $ denotes the partial","date":"2024-03-10","objectID":"/povm/:0:0","tags":["Quantum Measurement Theory"],"title":"POVM: a brief introduction","uri":"/povm/"},{"categories":["Quantum Information"],"content":"The wave function of a photon in the spacetime representation is: $\\Psi(\\mathbf{r},t)=\\langle \\mathbf{r},t|\\psi\\rangle=\\langle 0 |E^{+}(\\mathbf{r},t)|\\psi\\rangle$ Where $\\begin{aligned} |\\mathbf{r},t\\rangle = E^{-}(\\mathbf{r},t) |0\\rangle = \\sum_{\\mathbf{k},\\lambda} \\sqrt{\\frac{\\hbar \\omega}{2 \\epsilon_0 V}} e^{\\mathrm{i}(\\mathbf{k}\\cdot \\mathbf{r}-\\omega_{\\mathbf{k}} t)} a^\\dag_{\\mathbf{k},\\lambda} |0\\rangle \\end{aligned}$. Intuitively, this is to let the field operator $E^{-}(\\mathbf{r},t)$ create a state $|\\mathbf{r},t\\rangle$ at the spacetime point $(\\mathbf{r},t)$, and then calculate the overlap between this state and $|\\psi\\rangle$. When we talk about the spacetime modes of photons, such as Gaussian pulses, hyperbolic secant pulses, etc., we are actually referring to the wave function in the spacetime representation described above. Of course, this wave function is not in the “Schrödinger sense” because the Schrödinger equation belongs to non-relativistic quantum mechanics, while photons are relativistic particles. When we say that photons do not have a well-defined wave function, we are actually saying that they do not have a well-defined position operator. But we do have field operators $E^{\\pm}(\\mathbf{r},t)$, so we don’t need a position operator either. The tern “photon wave function” is nowadays common in literature. Few examples: [1] Xu, Y., Choudhary, S. \u0026 Boyd, R. W. Efficient Measurement of the Bi-photon Spatial Mode Entanglement with Stimulated Emission Tomography. Preprint at http://arxiv.org/abs/2403.05036 (2024). [2] Tian, Z., Liu, Q., Tian, Y. \u0026 Gu, Y. Wavepacket interference of two photons: from temporal entanglement to wavepacket shaping. Preprint at http://arxiv.org/abs/2403.04432 (2024). [3] Moura, A. G. da C. \u0026 Monken, C. H. Einstein-Podolsky-Rosen correlations in spontaneous parametric down-conversion: Beyond the Gaussian approximation. Preprint at http://arxiv.org/abs/2403.04561 (2024). ","date":"2024-03-10","objectID":"/photon_wave_func/:0:0","tags":["Quantum Optics"],"title":"Wave Function of Photons","uri":"/photon_wave_func/"},{"categories":["Mathematical Physics"],"content":"Chinese version here Baker-Campbell-Hausdorff Formula can be used to compute operator evolution in the Heisenberg picture: $e^X Y e^{-X}=Y+[X,Y]+\\frac{1}{2!}[X,[X,Y]]+\\frac{1}{3!}[X,[X,[X,Y]]]+\\cdots$ This formula is actually just a younger sibling of the BCH formula. Because the evolution rule of operators in the Heisenberg picture is $A\\rightarrow UAU^{\\dag}$, where $U$ is a unitary evolution operator. If $U$ is generated by $H$, then it becomes $A\\rightarrow e^{\\frac{t}{i\\hbar}H}Ae^{-\\frac{t}{i\\hbar}H}$. Example 1: Phase Shifter The Hamiltonian is $H=\\varphi n$, and the annihilation operator $a$ evolves as: $\\begin{aligned} e^{-i\\varphi n} a e^{i\\varphi n}\u0026= a + i\\varphi [n, a] - \\frac{\\varphi}{2!} [n,[n,a]] - \\cdots \\\\ \u0026= a (1+i\\varphi -\\frac{\\varphi^2}{2!} - \\cdots)\\\\ \u0026= e^{i\\varphi} a \\end{aligned}$ Example 2: Beam Splitter The Hamiltonian is $H= \\theta e^{i\\varphi} a^{\\dag}b + \\theta e^{-i\\varphi} a b^\\dag$, the evolution is as follows: $\\begin{aligned} e^{-iH} a e^{iH} \u0026= \\cos \\theta ,a + i \\sin\\theta, e^{i\\varphi} b \\\\ e^{-iH} b e^{iH} \u0026= i \\sin\\theta ,e^{-i\\varphi} a + \\cos \\theta ,b \\end{aligned}$ This is the familiar Bogoliubov Transformation. These two examples seem like using a cannon to kill a mosquito. Let’s move on to something more useful: Example 3: Two-mode Squeezed State The Hamiltonian is: $\\begin{aligned} H=i\\hbar\\left(g^*a b-g a^\\dag b^\\dag\\right) \\end{aligned}$ $\\begin{aligned} a \u0026\\rightarrow e^{\\frac{H}{i\\hbar}} a e^{-\\frac{H}{i\\hbar}} \\\\ \u0026= a + \\left[{\\frac{H}{i\\hbar}},a\\right]+\\left[{\\frac{H}{i\\hbar}},\\left[{\\frac{H}{i\\hbar}},a\\right]\\right] + \\cdots \\\\ \u0026=\\cosh(r) a - \\sinh(r)e^{i\\xi}b^\\dag \\end{aligned}$ Where $g = re^{i\\xi}$. This is well-known for SPDC and OPO. It’s difficult to compute in the Schrödinger picture, but it’s much easier using the Heisenberg picture with BCH formula. Example 4: Rotated Quadratures The generator is the total particle number $N$, canonical position operator $Q$, and canonical momentum operator $P$ evolve as: $\\begin{aligned} e^{i\\theta N} Q e^{-i\\theta N} = \\cos \\theta, Q + i \\sin \\theta, P \\\\ e^{i\\theta N} P e^{-i\\theta N} = i \\sin \\theta, Q + \\cos \\theta P \\end{aligned}$ It looks like rotating the phase space, similar to the Phase Shifter. This transformation is useful for handling Squeezed states. Even a subordinate of the BCH formula is so useful, isn’t the original formula even more powerful? Now, let’s introduce the original: $e^{X}e^{Y}=e^Z$ $Z=X+Y+{\\frac {1}{2}}[X,Y]+{\\frac {1}{12}}[X,[X,Y]]-{\\frac {1}{12}}[Y,[X,Y]]+\\cdots $ Uh, it seems a bit less practical. In reality, we often use a special case, where $[X,Y]$ is a constant, in which case the BCH formula becomes $Z=X+Y+{\\frac {1}{2}}[X,Y]$ Example 5: Displacement Operator $\\begin{aligned} D(\\alpha)\u0026=e^{\\alpha a^\\dag-\\alpha^* a} \\\\ \u0026=e^{-\\frac{1}{2}|\\alpha|^2}e^{\\alpha a^\\dag}e^{-\\alpha^* a} \\\\ \u0026=e^{\\frac{1}{2}|\\alpha|^2}e^{-\\alpha^* a}e^{\\alpha a^\\dag} \\end{aligned}$ It’s evident that the BCH formula allows us to switch flexibly between normal ordering (creation operators before annihilation operators) and anti-normal ordering. In fact, the displacement operator itself is neither normally ordered nor anti-normally ordered but symmetrically ordered. Example 6: Weyl Operator $W(q,p) = e^{i(-qP+pQ)} = e^{\\frac{i}{2}qp}U(q)V(p) = e^{-\\frac{i}{2}qp}V(p)U(q)$ Where $U(q)=e^{-iqP}$ is the position translation operator, $V(p)=e^{ipQ}$ is the momentum translation operator. According to the BCH formula, we can decouple them. This $W:\\mathbb{R}^2\\rightarrow \\mathcal{L}(L^2(\\mathbb{R}))$ is called the Weyl Representation. More generally, the Weyl Transformation can quantize a probability density function in classical phase space into a density operator, conversely, the Wigner Transformation can map a density operator to a probability density function in phase space, which is called the Wigner Representation in quantum optics. Example 7: Glauber P-, Husimi Q- and Wigner Representation The Wigner representation of a dens","date":"2024-03-10","objectID":"/bch/:0:0","tags":["Quantum Optics"],"title":"Baker-Campbell-Hausdorff Formula","uri":"/bch/"},{"categories":["Diary"],"content":" Projects Date Remark An FM Radio Receiver based on PYNQ-Z2 and RTL-SDR Jun 2023 Jupyter Notebook App rather than web app WIFI Weather Clock Jan 2023 Haven’t learned LvGL yet … The GUI is rather simple Light Cube Jan 2023 Ordered the BOM and soldered them together, but the firmware is not mine … Conway’s Game of Life Dec 2022 Touch screen used~ ","date":"2023-07-06","objectID":"/side_proj/:0:0","tags":["Embedded Systems"],"title":"Some fun side-projects","uri":"/side_proj/"},{"categories":["EECS"],"content":"1 Basics SPI has three-wire mode and four-wire mode. The three-wire mode consists of three wires - SS (Slave Select), SCK (SPI Clock), and MOSI (Master-In-Slave-Out). The four wire mode has an extra line called MISO (Master-In-Slave-Out). Signals Full Name SS Slave Select SCK SPI Clock MOSI Master-Out-Slave-In MISO Master-In-Slave-Out SPI clock has four modes: CPOL=0/1 and CPHA=0/1. CPOL stands for clock polarity – clock low or high when in idle. CPHA stands for clock phase – data valid at 0 degree or 180 degrees. ","date":"2023-07-01","objectID":"/spi/:0:1","tags":["SPI","Embedded Systems"],"title":"HW \u0026 SW Design of the AXI Quad SPI IP Core","uri":"/spi/"},{"categories":["EECS"],"content":"2 AXI Quad SPI IP Core When we run out of Zynq PS SPI controllers for some reason, we can turn to PL SPI IP cores, which is called AXI Quad SPI. AXI Quad SPI IP core In the picture, io1_i connects to MISO; ext_spi_clk and s_axi_clk can be connected to a same system clock. ip2intc_irpt can be connected to the Zynq interrupt pl_ps_irq. You can double-click on the IP core to configure clock divider, number of slave selects, and more. ","date":"2023-07-01","objectID":"/spi/:0:2","tags":["SPI","Embedded Systems"],"title":"HW \u0026 SW Design of the AXI Quad SPI IP Core","uri":"/spi/"},{"categories":["EECS"],"content":"3 Bare-Metal Programming The programming sequence is as follows: First, initialize the SPI controller. Set the value of all registers to their default values. Next, depending on your needs, configure the SPI controller. For example, the clock mode (CPHA and CPOL) and slave select mode (auto or manual). Then, depending on the number of bytes need to be written and read, write to the DTR (Data Transmit Register) the corresponding number of bytes. Each written byte will be shifted into the TX FIFO. Note: If the number of written bytes is n_tx and the number of read bytes is n_rx, then we should not only write n_tx bytes to the DTR, but also write another n_rx “dummy” bytes. This is because SPI, in nature, is a full-duplex protocol – in order to receive n_rx bytes, you also need to send n_rx bytes. Finally, read the bytes out of the DRR (Data Receive Register). When the DRR is read, the read byte will be dequed. Note: depending on the scene, when receiving from the slave, the programmer may need to discard the first n_tx bytes. This is because when the master is sending data/instructions, the slave may not yet respond. Rather, the slave will not respond until the master has sent all of the bytes (instruction \u0026 data). But due to the full-duplex nature of the SPI protocol, the first n_tx dummy bytes will also be pushed into the RX FIFO. Therefore, one may need to discard those bytes. If we use the manual slave select mode, we should assert the SS signal before the transfer and de-assert the SS signal after the transfer. The following code sends two bytes [0x00 0x37] to the slave and reads one byte from the slave. #include \"xspi.h\" // axi quad spi #include \"xparameters.h\" #include \"xstatus.h\" #include \"xplatform_info.h\" #include \"xil_printf.h\" #include \"sleep.h\" #define SPI_DEVICE_ID XPAR_SPI_0_DEVICE_ID #define SPI_BASEADDR XPAR_SPI_0_BASEADDR XSpi Spi; int main() { /* * SPI Initialize */ XSpi_Config *spi_config_ptr; spi_config_ptr = XSpi_LookupConfig(SPI_DEVICE_ID); if (spi_config_ptr == NULL) { return XST_DEVICE_NOT_FOUND; } status = XSpi_CfgInitialize(\u0026Spi, spi_config_ptr, spi_config_ptr-\u003eBaseAddress); if (status != XST_SUCCESS) { return XST_FAILURE; } // Start the SPI driver so that the device is enabled. XSpi_Start(\u0026Spi); // Disable Global interrupt to use polled mode operation XSpi_IntrGlobalDisable(\u0026Spi); /* * 1. Enable master mode. * 2. CPHA = 1, CPOL = 0 * 3. Manual Slave Select * 4. TX/RX FIFO Reset */ u32 control; control = XSpi_ReadReg(SPI_BASEADDR, XSP_CR_OFFSET); control |= XSP_CR_MASTER_MODE_MASK | // Master Mode XSP_CR_CLK_PHASE_MASK | // Clock Phase XSP_CR_MANUAL_SS_MASK | // Manual Slave Select XSP_CR_TXFIFO_RESET_MASK| // TX FIFO Reset XSP_CR_RXFIFO_RESET_MASK // RX FIFO Reset ; XSpi_WriteReg(SPI_BASEADDR, XSP_CR_OFFSET, control); // write [0x00 0x37] and then read one byte XSpi_WriteReg(SPI_BASEADDR, XSP_DTR_OFFSET, 0x00); XSpi_WriteReg(SPI_BASEADDR, XSP_DTR_OFFSET, 0x37); XSpi_WriteReg(SPI_BASEADDR, XSP_DTR_OFFSET, 0x00); /* * SPI write */ XSpi_WriteReg(SPI_BASEADDR, XSP_SSR_OFFSET, 0xE); // slave select // 0xE: 0b1110 // initiate a transfer control = XSpi_ReadReg(SPI_BASEADDR, XSP_CR_OFFSET); control |= XSP_CR_ENABLE_MASK; control \u0026= ~XSP_CR_TRANS_INHIBIT_MASK; XSpi_WriteReg(SPI_BASEADDR, XSP_CR_OFFSET, control); /* * SPI read */ // wait for the transmit FIFO to be empty while (!(XSpi_ReadReg(SPI_BASEADDR, XSP_SR_OFFSET) \u0026 XSP_SR_TX_EMPTY_MASK)); control = XSpi_ReadReg(SPI_BASEADDR, XSP_CR_OFFSET); control |= XSP_CR_TRANS_INHIBIT_MASK; XSpi_WriteReg(SPI_BASEADDR, XSP_CR_OFFSET, control); // read data receive register while ((XSpi_ReadReg(SPI_BASEADDR, XSP_SR_OFFSET) \u0026 XSP_SR_RX_EMPTY_MASK) == 0) { data = XSpi_ReadReg(SPI_BASEADDR, XSP_DRR_OFFSET); } // we know (in advance) that the slave will return one byte, so we know this loop will be executed three times. // slave de-select XSpi_WriteReg(SPI_BASEADDR, XSP_SSR_OFFSET, 0xF); // 0xF: 0b1111 xil_printf(\"MISO: 0x%x\\n\\r\", data); return 0; } ","date":"2023-07-01","objectID":"/spi/:0:3","tags":["SPI","Embedded Systems"],"title":"HW \u0026 SW Design of the AXI Quad SPI IP Core","uri":"/spi/"},{"categories":["EECS"],"content":"4 Linux Programming In order to use the AXI Quad SPI IP core in Linux, we should add a spidev node to the device tree, so that we could achieve SPI communication by reading from or writing to the /dev/spidevx.y device. In /dev/spidevx.y, x stands for the x-th SPI controller, and y stands for the y-th chip. The device tree is usually initialized during boot-up and is read-only. However, after the 4.14 version of the Linux kernel, we can use the “device tree overlay” (briefed as DTO from now on) to dynamically add incremental device trees. Below is a code snippet of DTO. /dts-v1/; /plugin/; / { fragment@0 { target = \u003c\u0026amba\u003e; overlay0: __overlay__ { axi_quad_spi_0: axi_quad_spi@80000000 { ... status = \"okay\"; #address-cells = \u003c1\u003e; #size-cells = \u003c0\u003e; spidev0: spidev@0 { compatible = \"spidev\"; reg = \u003c0\u003e; spi-max-frequency = \u003c5000000\u003e; }; }; }; }; }; Some points to be noted: We should add one more line /plugin/; after /dts-v1/; to show that this is an DTO file rather than an ordinary device tree file. target stands for which node is to be modified. Here it is \u003c\u0026amba\u003e, and it will be extended to the phandle of the node which has the symbol amba. For example, if the phandle of amba is 70, then \u003c\u0026amba\u003e virtually stands for \u003c70\u003e. A phandle uniquely denotes a node and is usually allocated by the device tree compiler. In most cases, we should add the -@ compiler option, which enables support for symbol. Otherwise we can only reference the nodes by their phandle, which requires de-compiling the device tree binaries. ","date":"2023-07-01","objectID":"/spi/:0:4","tags":["SPI","Embedded Systems"],"title":"HW \u0026 SW Design of the AXI Quad SPI IP Core","uri":"/spi/"},{"categories":["CS"],"content":"Let’s look at a code snippet in the Jupyter Notebook: import time import ipywidgets as widgets from IPython.display import display slider = widgets.IntSlider() display(slider) while True: print(slider.value) time.sleep(1) IntSlider is an interactive Jupyter Notebook widget. When the user interacts with the slider, the slider value should change. However, if you run the above codes, you will find that the printed value of the slider won’t change at all – it will be stuck at its initial value. In fact, the UI elements won’t be updated until all code blocks are finished executing. In other words, the update of the UI elements is blocked by the execution of any code block. In order to poll the states of UI elements inside a running loop, we can use the jupyter-ui-poll library. The modified codes are as follows: import time import ipywidgets as widgets from IPython.display import display from jupyter_ui_poll import ui_events slider = widgets.IntSlider() display(slider) while True: with ui_events() as poll: poll(1) # poll one event print(slider.value) time.sleep(1) ","date":"2023-07-01","objectID":"/ui_poll/:0:0","tags":["Python","Jupyter Notebook"],"title":"Polling Jupyter Widget UI Events in Runtime","uri":"/ui_poll/"},{"categories":null,"content":"Still Working on it :) ","date":"2023-06-22","objectID":"/publication/:0:1","tags":null,"title":"Publication","uri":"/publication/"},{"categories":["CS"],"content":"Recently, I have been using Python coroutines/asynchronous programming in a project. Now, I will summarize my experience. ","date":"2023-06-13","objectID":"/python_asyncio/:0:0","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["CS"],"content":"Import import asyncio If asyncio is to be used in an IPython environment, we have to add two more lines： import nest_asyncio nest_asyncio.apply() import asyncio ","date":"2023-06-13","objectID":"/python_asyncio/:1:0","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["CS"],"content":"Coroutines Coroutines are the core of asynchronous programming in Python. To define a coroutine, you need to use async def. async def main(): # do something print(\"Hello world!\") To execute the coroutine, you cannot directly call main(). Instead, you need to use run(): asyncio.run(main()) To nest one coroutine within another, similar to nesting one function within another, you can use await: import asyncio async def coro_1(): print(\"I am the coroutine 1.\") async def coro_2(): print(\"I am the coroutine 2.\") async def main(): await coro_1() await coro_2() print(\"Hello World!\") asyncio.run(main()) When nesting coroutines within another coroutine, you need to use await to invoke them. If you write it directly as follows: async def main(): coro_1() coro_2() print(\"Hello World!\") You will receive the following warnings: 01.py:10: RuntimeWarning: coroutine 'coro_1' was never awaited coro_1() RuntimeWarning: Enable tracemalloc to get the object allocation traceback 01.py:11: RuntimeWarning: coroutine 'coro_2' was never awaited coro_2() RuntimeWarning: Enable tracemalloc to get the object allocation traceback Hello World! As can been seen, coro_1 and coro_2 has not been called. ","date":"2023-06-13","objectID":"/python_asyncio/:2:0","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["CS"],"content":"Tasks We would lose the point if we use coroutines in the way as shown in the previous section. The true significance of coroutines lies in their ability to be executed concurrently. Consider the following code: import asyncio async def coro_1(): print(\"I am the coroutine 1.\") async def coro_2(): print(\"I am the coroutine 2.\") async def main(): task_1 = asyncio.create_task(coro_1()) task_2 = asyncio.create_task(coro_2()) await task_1 await task_2 print(\"Hello World!\") asyncio.run(main()) In this code, we used create_task to create tasks task_1 and task_2 for coro_1 and coro_2 respectively. They are actually executed together. To verify this, we can add some delays: import asyncio import time async def coro_1(): print(\"I am the coroutine 1.\") await asyncio.sleep(1) async def coro_2(): print(\"I am the coroutine 2.\") await asyncio.sleep(1) async def main(): st = time.time() task_1 = asyncio.create_task(coro_1()) task_2 = asyncio.create_task(coro_2()) await task_1 await task_2 et = time.time() print(\"Elapsed: %f s\" % (et - st)) asyncio.run(main()) The results are: I am the coroutine 1. I am the coroutine 2. Elapsed: 1.001298 s If we do not use create_task but rather directly await the two coroutines: import asyncio import time async def coro_1(): print(\"I am the coroutine 1.\") await asyncio.sleep(1) async def coro_2(): print(\"I am the coroutine 2.\") await asyncio.sleep(1) async def main(): st = time.time() await coro_1() await coro_2() et = time.time() print(\"Elapsed: %f s\" % (et - st)) asyncio.run(main()) Then the outcomes are: I am the coroutine 1. I am the coroutine 2. Elapsed: 2.002527 s In other words, if we don’t use create_task to create tasks, coro_2() will not be executed until coro_1() has finished. From this example, we can observe that the true meaning of await is “wait for the task to complete”. Furthermore, we can use gather to run multiple coroutines concurrently: import asyncio async def coro_1(): print(\"Coroutine 1 starts\") await asyncio.sleep(1) print(\"Coroutine 1 finishes\") async def coro_2(): print(\"Coroutine 2 starts\") await asyncio.sleep(2) print(\"Coroutine 2 finishes\") async def main(): print(\"Starting main coroutine\") await asyncio.gather(coro_1(), coro_2()) print(\"Main coroutine finished\") asyncio.run(main()) ","date":"2023-06-13","objectID":"/python_asyncio/:3:0","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["CS"],"content":"Asynchronous For loop Now we can look at async for. import asyncio import time async def async_generator(): for i in range(10): await asyncio.sleep(1) yield i async def custom_coroutine(): async for item in async_generator(): print(item) time.sleep(1) st = time.time() asyncio.run(custom_coroutine()) et = time.time() print(\"Elasped: %f s\" % (et - st)) The result is: 0 1 2 3 4 5 6 7 8 9 Elasped: 10.013134 s By combining async for and yield, we can create an asynchronous generator. In reality, an asynchronous generator is an instance of a class that has __aiter__ and __anext__ methods. Here’s an equivalent implementation of the previous code: import asyncio import time class AsyncGenerator: def __init__(self, N): self.i = 0 self.N = N def __aiter__(self): return self async def __anext__(self): i = self.i if i \u003e= self.N: raise StopAsyncIteration await asyncio.sleep(1) self.i += 1 return i async def main(): async for p in AsyncGenerator(10): print(p) st = time.time() asyncio.run(main()) et = time.time() print(\"Elasped: %f s\" % (et - st)) ","date":"2023-06-13","objectID":"/python_asyncio/:3:1","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["CS"],"content":"Example At last, I will give an example of using asyncio in a project: sdr = RtlSdr() sdr.center_freq = 92_700_000 # an FM radio station running at 92.7MHz async def main(): async for data in sdr.stream(): # perform FM demodulation audio = fm_demodulation(data) # play the audio display(Audio(audio, autoplay=True, rate=48000, normalize=False)) asyncio.run(main()) This is an FM radio application that uses async for to read data from the receiver, demodulate the data into audio signals and finally play the audio in a streamed manner. In other words, we are essentially processing and streaming the audio data in real time. ","date":"2023-06-13","objectID":"/python_asyncio/:4:0","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["EECS"],"content":"Windows Set a network adapter that can access the internet in the Control Panel and share it with Ethernet. The IP address of Ethernet will change to 192.168.137.1 (this is the default behavior in Windows). Then, set the gateway as 192.168.137.1 in the terminal of the development board: sudo route add default gw 192.168.137.1 Set the IP address as 192.168.137.x, where x is any value except 1 and 255 (gateway address and broadcast address): sudo ifconfig \u003cyour-ifdev\u003e 192.168.3.x If it still doesn’t work, check if the DNS in /etc/resolv.conf is correct. You can add a line: nameserver 8.8.8.8 ","date":"2023-06-12","objectID":"/boards_networking/:0:1","tags":["Networking","Linux"],"title":"Access the Internet from evaluation boards via PC","uri":"/boards_networking/"},{"categories":["EECS"],"content":"Ubuntu Change the IPv4 of Ethernet in the PC’s network settings to shared mode. The IP address of Ethernet will change to 10.42.0.1 (this is default in Ubuntu Linux, but I’m not sure if it is the same in other Linux distributions). Then, set the gateway as 10.42.0.1 in the terminal of the development board: sudo route add default gw 10.42.0.1 Set the IP address to any IP within the same subnet, as long as the subnet part of the IP is not the gateway address and broadcast address: sudo ifconfig 10.42.0.2 If it still doesn’t work, check if the DNS in /etc/resolv.conf is correct. You can add a line: nameserver 8.8.8.8 ","date":"2023-06-12","objectID":"/boards_networking/:0:2","tags":["Networking","Linux"],"title":"Access the Internet from evaluation boards via PC","uri":"/boards_networking/"},{"categories":["EECS"],"content":"Troubleshooting First, check if you can ping the host. If you can’t ping the host, the gateway may not be set correctly. If you can ping the host but not the external IP, it may be because you haven’t shared the Ethernet for the development board in the PC, or the proxy is not set correctly, or the gateway is not set correctly. If you can ping the external IP but not the domain name, it is most likely a DNS configuration issue. ","date":"2023-06-12","objectID":"/boards_networking/:0:3","tags":["Networking","Linux"],"title":"Access the Internet from evaluation boards via PC","uri":"/boards_networking/"},{"categories":["CS"],"content":"Xilinx’s toolchain consumes soooo much memory! Sometimes it causes the system to freeze… After all, my laptop only has 8GB of RAM. So there’s no other choice but to add virtual memories. After increase the swapfile, the system performance has improved a lot: sudo swapoff /swapfile sudo dd if=/dev/zero of=/swapfile bs=1M count=16384 sudo mkswap /swapfile sudo swapon /swapfile ","date":"2023-06-12","objectID":"/increase_swapfile/:0:0","tags":["Storage/Partition","Linux"],"title":"Increasing the swapfile for Linux","uri":"/increase_swapfile/"},{"categories":["EECS"],"content":"To partition and format an SD card in Linux, follow these steps: First, connect the SD card to your PC. Then, use the fdisk command in the bash command line to partition the SD card. Finally, use the mkfs command to create a file system (format) on the SD card. The main commands are as follows: First, use sudo fdisk -l to confirm which device in /dev corresponds to the SD card. Assuming we have determined that the SD card corresponds to /dev/sde, we can enter sudo fdisk /dev/sde to start the partitioning operation: Enter d to delete partitions. Keep entering until all partitions are deleted. Enter n to create the first partition as the boot partition. Enter t to change the partition type to W95 FAT32. Boot partitions are generally of this type. Enter a to set the partition as the boot partition. Enter n to create the second partition as the root file system partition. Since the root file system partition type is usually Linux, and the default partition type is already Linux, there is no need to enter t to change the partition type. Enter w to save and exit. Finally, after partitioning, we can create the file systems: sudo mkfs.vfat -F 32 -n \"BOOT\" /dev/sde1 sudo mkfs.ext4 -L \"rootfs\" /dev/sde2 In the above code, the first line creates a FAT32 file system for the boot partition and names it as BOOT. The second line creates an ext4 file system for the root file system partition and names it as rootfs. ","date":"2023-06-12","objectID":"/linux_sd_card/:0:0","tags":["Storage/Partition","Linux"],"title":"Partitioning and Formatting SD Cards with Linux","uri":"/linux_sd_card/"},{"categories":["CS"],"content":"To mount the EFI partitions, run the following commands in PowerShell: diskpart list disk # make sure which is the disk that contains the EFI partition. usually 0 select disk 0 list partition # make sure which is the EFI partition. normally 0 select disk 0 assign letter=z Then we will mount the EFI partition as drive Z. This operation may be useful when using a dual-boot system with Windows and Ubuntu: Sometimes, after removing the Ubuntu system, only the Windows system remains, but the GRUB interface still appears every time the computer boots. In this case, you need to delete the old Ubuntu EFI partition. ","date":"2023-06-12","objectID":"/efi_partition/:0:0","tags":["Storage/Partition","Linux"],"title":"Mounting the EFI partition in Windows OS","uri":"/efi_partition/"},{"categories":["Mathematical Physics"],"content":"1 Differential Forms Before introducing the concept of curl, we need to first introduce differential forms and the exterior derivative. An n-form can be defined as an alternating multilinear mapping $\\omega:(T_p^*M)^n\\rightarrow \\mathbb{R}$. It maps multiple vectors to a real number. Additionally, it satisfies the alternating property, meaning that exchanging two input vectors results in an output multiplied by a negative sign. Therefore, an n-form can be explicitly defined as follows: $$ \\omega^1\\wedge \\omega^2\\wedge\\cdots\\wedge \\omega^n(v_1,v_2,\\cdots,v_n)= \\begin{vmatrix} \\omega^1(v_1) \u0026 \\cdots \u0026 \\omega^{1}(v_n) \\\\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\\ \\omega^n(v_1) \u0026 \\cdots \u0026 \\omega^n(v_n) \\end{vmatrix}\\in \\mathbb{R} $$ The set of all n-forms on $T_p^*M$ can be written as $\\bigwedge_n(T_p^*M)$. As for “differential forms,” they differ from “forms” in that they are the “forms’ fields,” meaning that at each point on the manifold, there resides a form. In other words, “forms” are specific to a point, while “differential forms” are specific to the entire manifold. ","date":"2023-06-10","objectID":"/curl_in_n_dimension/:0:1","tags":["Vector Calculus"],"title":"Curl in high dimension","uri":"/curl_in_n_dimension/"},{"categories":["Mathematical Physics"],"content":"2 Exterior Derivative The definition of the exterior derivative is as follows: The exterior derivative is a mapping $\\mathrm{d}: \\bigwedge^n(T_p^*M) \\rightarrow \\bigwedge^{(n+1)}(T_p^*M)$, which acts on an differential n-form: $\\varphi = \\sum_I f_I \\mathrm{d}x^I = \\sum_{(i_1,\\cdots,i_n)} f_{(i_1,\\cdots,i_n)} \\mathrm{d}x^{i_1}\\wedge\\cdots\\wedge\\mathrm{d}x^{i_n}$, and yields an differential (n+1)-form: $\\mathrm{d}\\varphi = \\sum_I \\sum_i \\frac{\\partial f_I}{\\partial x_i} \\mathrm{d}x_i\\wedge x_I$. Comment These indices are quite all over the place. Let’s see the example below instead. Example For a differential 2-form on a 3-dimensional manifold: $\\varphi = z^2\\mathrm{d}x\\wedge\\mathrm{d}y + x\\sin y\\,\\mathrm{d}y\\wedge \\mathrm{d}z$, its exterior derivative is given by: $$\\begin{aligned} \\mathrm{d}\\varphi \u0026= \\left(\\frac{\\partial z^2}{\\partial x}\\mathrm{d}x + \\frac{\\partial z^2}{\\partial y}\\mathrm{d}y + \\frac{\\partial z^2}{\\partial z}\\mathrm{d}z\\right)\\wedge\\mathrm{d}x\\wedge\\mathrm{d}y \\\\\\ \u0026 + \\left(\\frac{\\partial (x\\sin y)}{\\partial x}\\mathrm{d}x + \\frac{\\partial (x\\sin y)}{\\partial y}\\mathrm{d}y + \\frac{\\partial (x\\sin y)}{\\partial z}\\mathrm{d}z\\right)\\wedge\\mathrm{d}y\\wedge\\mathrm{d}z \\\\\\ \u0026= (2z + \\sin y)\\,\\mathrm{d}x\\wedge\\mathrm{d}y\\wedge\\mathrm{d}z \\end{aligned} $$ ","date":"2023-06-10","objectID":"/curl_in_n_dimension/:0:2","tags":["Vector Calculus"],"title":"Curl in high dimension","uri":"/curl_in_n_dimension/"},{"categories":["Mathematical Physics"],"content":"3 Curl in N Dimensions In the following example, we will explore the relationship between curl and exterior derivative: Example: On a three-dimensional manifold, when the exterior derivative operator $\\mathrm{d}$ acts on a 1-form, it yields a 2-form: $$ \\begin{aligned} \\mathrm{d}(f_i\\mathrm{d}x^i) \u0026= \\frac{\\partial f_1}{\\partial x^1}\\mathrm{d}x^1\\wedge\\mathrm{d}x^1 + \\frac{\\partial f_1}{\\partial x^2}\\mathrm{d}x^2\\wedge\\mathrm{d}x^1 + \\frac{\\partial f_1}{\\partial x^3}\\mathrm{d}x^3\\wedge\\mathrm{d}x^1 \\\\ \u0026\\quad+ \\frac{\\partial f_2}{\\partial x^1}\\mathrm{d}x^1\\wedge\\mathrm{d}x^2 + \\frac{\\partial f_2}{\\partial x^2}\\mathrm{d}x^2\\wedge\\mathrm{d}x^2 + \\frac{\\partial f_2}{\\partial x^3}\\mathrm{d}x^3\\wedge\\mathrm{d}x^2 \\\\ \u0026\\quad+ \\frac{\\partial f_3}{\\partial x^1}\\mathrm{d}x^1\\wedge\\mathrm{d}x^3 + \\frac{\\partial f_3}{\\partial x^2}\\mathrm{d}x^2\\wedge\\mathrm{d}x^3 + \\frac{\\partial f_3}{\\partial x^3}\\mathrm{d}x^3\\wedge\\mathrm{d}x^3 \\end{aligned} $$ Applying the Hodge star operator once, it yields a 1-form: $$ \\begin{aligned} \\star \\mathrm{d}(f_i\\mathrm{d}x^i) \u0026= \\phantom{+\\frac{\\partial f_1}{\\partial x^2}\\mathrm{d}x^3} -\\frac{\\partial f_1}{\\partial x^2}\\mathrm{d}x^3 + \\frac{\\partial f_1}{\\partial x^3}\\mathrm{d}x^2 \\\\ \u0026\\phantom{=}+ \\frac{\\partial f_2}{\\partial x^1}\\mathrm{d}x^3 \\phantom{\\,\\,\\,-\\frac{\\partial f_1}{\\partial x^2}\\mathrm{d}x^3} -\\frac{\\partial f_2}{\\partial x^3}\\mathrm{d}x^1 \\\\ \u0026\\phantom{=} -\\frac{\\partial f_3}{\\partial x^1}\\mathrm{d}x^2 + \\frac{\\partial f_3}{\\partial x^2}\\mathrm{d}x^1 \\\\ \u0026= \\left(\\frac{\\partial f_3}{\\partial x^2}-\\frac{\\partial f_2}{\\partial x^3}\\right)\\mathrm{d}x^1 + \\left(\\frac{\\partial f_1}{\\partial x^3}-\\frac{\\partial f_3}{\\partial x^1}\\right)\\mathrm{d}x^2 + \\left(\\frac{\\partial f_2}{\\partial x^1}-\\frac{\\partial f_1}{\\partial x^2}\\right)\\mathrm{d}x^3 \\end{aligned} $$ It can be expressed as the inner product of the curl and a tangent vector: $\\star\\mathrm{d}f:\\star\\mathrm{d}f(v)=\\langle \\nabla \\times f^{\\sharp}\\mid v \\rangle$ For now, without introducing the Hodge star operator, you only need to know that on a three-dimensional manifold, the Hodge operator applied to an differential n-form yields an differential (3-n)-form. Specifically, if the manifold is equipped with an inner product $\\langle\\mathrm{d}x^i,\\mathrm{d}x^j\\rangle=\\delta^{j}_i$, then we have: $$ \\star(\\mathrm{d}x^i \\wedge \\mathrm{d}x^j) = \\mathrm{d}x^k $$ $$\\star\\mathrm{d}x^i = \\mathrm{d}x^j\\wedge \\mathrm{d}x^k$$ $$\\star(f\\, \\mathrm{d}x^i \\wedge\\mathrm{d}x^j\\wedge \\mathrm{d}x^k) = f$$ $$\\star f = f\\mathrm{d}x^i \\wedge\\mathrm{d}x^j\\wedge \\mathrm{d}x^k$$ where $(i,j,k)$ is an even permutation of $(1,2,3)$. Also, note that we use a raised symbol: $\\sharp$. This is because the curl operates on tangent vector fields, not cotangent vector fields. Specifically, the curl $\\nabla\\times$ maps a tangent vector field to another tangent vector field: $\\Gamma(TM)\\rightarrow \\Gamma(TM)$. However, $f\\in \\Gamma(T^*M)$ is a cotangent vector field (a 1-form), so we first need to “raise” it to a tangent vector field: $f^\\sharp\\in \\Gamma(TM)$. This is actually the well-known index raising and lowering in physics: $g^{ij}X_i=X^j$. Similarly, $\\flat$ represents lowering a tangent vector field to a cotangent vector field: $g_{ij}X^i=X_j$. Returning to the previous example, we have a canonical isomorphism induced by the inner product: $$ \\sharp: \\star \\mathrm{d}f \\mapsto \\nabla\\times f^\\sharp $$ or written as: $$ (\\star \\mathrm{d}f)^\\sharp = \\nabla\\times f^\\sharp $$ or written as: $$ \\star \\mathrm{d}f(v) = \\langle\\nabla\\times f^\\sharp\\mid v\\rangle $$ where $f$ is a 1-form. We can also write it as: $\\nabla\\times F=(\\star \\mathrm{d} (F^\\flat))^\\sharp$ where $F=f^\\sharp$ is a tangent vector field, and $f$ is a cotangent vector field (a 1-form). Inspired by the above example, we can define the curl as: $$ \\begin{aligned} (\\nabla\\times) : \\quad \u0026 \\Gamma(TM)\\rightarrow \\Gamma\\left(\\bigwedge^{n-2}TM\\right) \\\\ \u0026 F \\mapsto (\\star \\mathrm{d} (F^\\flat))^\\sharp \\end{aligned} $$ whe","date":"2023-06-10","objectID":"/curl_in_n_dimension/:0:3","tags":["Vector Calculus"],"title":"Curl in high dimension","uri":"/curl_in_n_dimension/"},{"categories":["Mathematical Physics"],"content":"1 Introduction In physics textbooks, we often come across the terms “pseudo-vector” and “pseudo-scalar.” In fact, on a 3-dimensional manifold, a “pseudo-vector” is the exterior product of two tangent vectors, denoted as $v\\in T_pM\\wedge T_pM=\\bigwedge^2(T_pM)$, while a “pseudo-scalar” is the exterior product of three tangent vectors, denoted as $s\\in T_pM\\wedge T_pM\\wedge T_pM=\\bigwedge^3(T_pM)$. When equipped with an inner product (or non-degenerate bilinear form), there exists a Hodge duality between $\\bigwedge^2(T_pM)$ and $\\bigwedge^1(T_pM)$, which leads us to mistakenly consider the pseudo-vector as a vector. Similarly, due to the Hodge duality between $\\bigwedge^3(T_pM)$ and $\\bigwedge^0(T_pM)$ (scalar fields), we mistakenly treat the pseudo-scalar as a scalar. In fact, a pseudo-vector on a 3-dimensional manifold is mathematically equivalent to a 2-vector (bivector). It is similar in definition to a 2-form, with the distinction that a 2-vector belongs to the exterior product of the tangent space, $v\\in\\bigwedge^2(T_pM)$, while a 2-form belongs to the exterior product of the cotangent space, $\\omega\\in\\bigwedge^2(T_p^*M)$. ","date":"2023-06-10","objectID":"/pseudovectors/:1:0","tags":["Exterior Algebra"],"title":"What on earth are the pseudovectors?","uri":"/pseudovectors/"},{"categories":["Mathematical Physics"],"content":"2 Space Inversion Recalling why we call “pseudo-vectors” pseudo-vectors in the first place. It is because they exhibit exotic behaviors under space inversion transformations. However, if we consider them as the exterior product of two vectors, all the peculiar behaviors can be explained. Specifically, as shown in the figure below, under a space inversion transformation, the magnetic field reverses its direction. It’s similar to you moving in one direction, but the reflection of you in the mirror moves in the opposite direction, which is a spooky and paranormal event. The magnetic field is a bivector. If you consider it as a vector, you will encounter this spooky phenomenon in the picture: the magnetic field takes a look in the mirror and finds its head turned into feet. In fact, the magnetic field at point $p$ is not a vector but a bivector, denoted as $B|_p\\in (T_pM)\\wedge (T_pM)$. Its basis consists of $\\frac{\\partial}{\\partial x}\\wedge \\frac{\\partial}{\\partial y},\\,\\frac{\\partial}{\\partial y}\\wedge \\frac{\\partial}{\\partial z},\\,\\frac{\\partial}{\\partial z}\\wedge \\frac{\\partial}{\\partial x}$. So, there is no paranormal event happening. Under a space inversion transformation, both vectors corresponding to the magnetic field actually point in the correct directions. It’s just that humans insist on using the right-hand rule and treat a bivector as a vector, which leads to the appearance of this strange phenomenon. Although bivectors are well-defined in mathematics, we still want to ask the question: How do we visualize a bivector? The answer is: We can represent a bivector as an oriented surface element. Different-shaped surface elements that are parallel to each other and have the same area represent the same bivector. In other words, the same bivector can be depicted in infinitely many ways, as long as they are parallel, have the same area, and have the same orientation. Parallel surface elements with the same area and orientation represent the same bivector. In practice, treating a bivector as a vector using the right-hand rule is only applicable in three-dimensional manifolds. This is because only in three-dimensional manifolds does three minus two exactly equal one. In four-dimensional manifolds, since four minus two equals two, we can only dualize a bivector into another bivector, not a vector. In five-dimensional manifolds, a bivector can be dualized into a trivector, and so on. In an n-dimensional manifold, a bivector can be dualized into an (n-2)-vector. ","date":"2023-06-10","objectID":"/pseudovectors/:2:0","tags":["Exterior Algebra"],"title":"What on earth are the pseudovectors?","uri":"/pseudovectors/"},{"categories":["Mathematical Physics"],"content":"3 Maxwell Equations in Exterior Algebra By the way, the electric field is also a bivector. Its basis consists of $\\frac{\\partial}{\\partial t}\\wedge \\frac{\\partial}{\\partial x},\\,\\frac{\\partial}{\\partial t}\\wedge \\frac{\\partial}{\\partial y},\\,\\frac{\\partial}{\\partial t}\\wedge \\frac{\\partial}{\\partial z}$, involving a time dimension. The reason it doesn’t appear as a “pseudo-vector” is that we only consider space inversion transformations rather than time inversion transformations. If we consider the electromagnetic 2-form in the cotangent bundle (using natural units): $$ \\begin{aligned} F\u0026=E_x\\mathrm{d}t\\wedge\\mathrm{d}x+E_y\\mathrm{d}t\\wedge\\mathrm{d}y+E_z\\mathrm{d}t\\wedge\\mathrm{d}z \\\\ \u0026+B_x \\mathrm{d}y\\wedge\\mathrm{d}z + B_y \\mathrm{d}z\\wedge\\mathrm{d}x + B_z\\mathrm{d}x\\wedge\\mathrm{d}y \\end{aligned} $$ Since we can raise or lower the indices of a tensor with the help of a metric tensor, we can also write a $(2,0)$-type electromagnetic tensor with the basis formed by the exterior product of $\\frac{\\partial}{\\partial t},\\frac{\\partial}{\\partial x},\\frac{\\partial}{\\partial y},\\frac{\\partial}{\\partial z}$. However, for now, let’s adopt the $(0,2)$-type antisymmetric tensor (differential form) because it allows us to utilize the symbol $\\mathrm{d}$ for the exterior derivative. Now, let’s consider the current 1-form: $J=-\\rho\\mathrm{d}t+J_x\\mathrm{d}x+J_y\\mathrm{d}y+J_z\\mathrm{d}z$. Thus, under the Minkowski metric $\\text{diag}(-1,1,1,1)$, Maxwell’s equations can be expressed in a concise form: $$ \\left\\{\\quad \\begin{aligned} \\mathrm{d}F\u0026=0 \\\\ \\star \\, \\mathrm{d}\\star F \u0026= J \\end{aligned} \\right. $$ Here, $\\star$ represents the Hodge star operator (also known as Hodge duality). As can be seen, in the language of exterior algebra, the electromagnetic field is a 2-vector or a 2-form, rather than a vector or a 1-form. The current is indeed a vector (or a 1-form), though. ","date":"2023-06-10","objectID":"/pseudovectors/:3:0","tags":["Exterior Algebra"],"title":"What on earth are the pseudovectors?","uri":"/pseudovectors/"},{"categories":["Diary"],"content":"I recently discovered this fantastic tool for creating personal websites - Hugo. Feels really cool and user-friendly! Plan to gradually migrate my blogs from Zhihu to here in the future. ","date":"2023-06-10","objectID":"/blog_migration/:0:0","tags":["Diary"],"title":"Blog Migration","uri":"/blog_migration/"},{"categories":null,"content":"\rThis is Haifei's personal website. He loves physics (his major), mathematics (not very good at it), and EECS (just for fun). In terms of EECS, he is particularly interested in the embedded systems. In his spare time, he enjoys playing the keyboard, drums, and appreciating jazz music.\rHe graduated from Wuhan University with a bachelor's degree and is currently pursuing a Ph.D. at the National University of Singapore - Centre for Quantum Technologies.\r","date":"2023-06-09","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":["Mathematical Physics"],"content":"TBD Chinese version here ","date":"2023-06-09","objectID":"/differential_geometry/:0:0","tags":["Differential Geometry","Lie Algebra"],"title":"A Beginners Guide to Differential Geometry and Lie Algebras","uri":"/differential_geometry/"}]