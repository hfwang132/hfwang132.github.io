[{"categories":["Quantum Information"],"content":"Does the idler light passing through the optical fiber amplifier still entangle with the signal light, which is produced by SPDC? Let’s consider two extremes: Extreme 1: The gain of the amplifier is equal to 1, that is, there is no gain at all. In this case, the amplifier acts as if it has done nothing and is an identity channel. So, of course, the entanglement will still be maintained. (Actually, it’s not necessarily the case. Even if the gain is zero, additional noise may be introduced, but we ignore it here for now). Extreme 2: The gain of the amplifier is very large. Assuming that entanglement can be maintained at this point, we can obtain macroscopic Schrödinger cat states. But such states are basically unattainable experimentally because they have already decohered. (Of course, there are exceptions, such as bright squeezed states). So, between these two extremes, we can say that when the gain is relatively small, entanglement can be maintained, but when the gain is relatively large (actually, the “relatively large” magnitude estimate is about ~10, referring to the N00N state), entanglement will disappear due to noise or decoherence. Next, let’s discuss two cases separately, based on the $\\chi^{(2)}$ nonlinear parametric amplifier and the stimulated emission amplifier. ","date":"2024-05-27","objectID":"/spdc_optical_amp/:0:0","tags":["Quantum Optics"],"title":"Does the idler light passing through the optical fiber amplifier still entangle with the signal light?","uri":"/spdc_optical_amp/"},{"categories":["Quantum Information"],"content":"One, Optical Amplifier Based on $\\chi^{(2)}$ Nonlinearity If the optical fiber amplifier here is based on $\\chi^{(2)}$ nonlinearity, that is, PDC, parametric down-conversion. If PDC is used for optical amplification, it can also be called OPA, optical parametric amplification. Note that PDC here should be distinguished from SPDC, spontaneous parametric down-conversion. Assuming that the pump for both SPDC and OPA is strong enough, then their Hamiltonians can be written as: $H_\\text{SPDC}=\\mathrm{i}\\hbar(\\hat{a}^\\dag \\hat{b}^\\dag - \\hat{a} \\hat{b})$ $H_\\text{OPA} = \\mathrm{i}\\hbar(\\hat{b}^\\dag \\hat{c}^\\dag -\\hat{b}\\hat{c})$ where $\\hat{a}$ and $\\hat{b}$ represent the annihilation operators on the idler and signal modes of SPDC, and $\\hat{b}$ and $\\hat{c}$ represent the annihilation operators on the signal and idler modes of OPA. So the final quantum state is: $|\\Psi\\rangle = \\exp[\\mathrm{i}g_\\text{2}H_\\text{OPA} /\\hbar]\\exp[\\mathrm{i}g_\\text{1}H_\\text{SPDC}/\\hbar] |0,0,0\\rangle_{abc}$ where $g_1$ is the logarithm of the gain of SPDC, and $g_2$ is the logarithm of the gain of OPA. Finally, just trace out the idler of OPA: $\\rho=\\operatorname{tr}_c[|\\Psi\\rangle\\langle\\Psi|]$ The specific calculations might not be straightforward. If there are quantum information theorists interested in helping with the calculation, it would be great. Also, consider what entanglement measure to use. The PPT criterion definitely cannot be used because these are two modes, not two qubits. However, we can guess the calculation process: blindly guess that the process of passing through OPA and tracing out the OPA idler is equivalent to a dephasing channel. Then we approximate the SPDC state in the low-gain limit as a Bell state. At this point, the degree of entanglement depends directly on the strength of dephasing, which is positively correlated with the gain of OPA, and likely proportional to the gain, i.e., $\\exp(g_2)$. Considering a 20 dB gain, the degree of entanglement will decrease by $10^{20/10} = 100$ times. So after some “calculation”, we conclude: the larger the gain of OPA, the smaller the degree of entanglement. ","date":"2024-05-27","objectID":"/spdc_optical_amp/:1:0","tags":["Quantum Optics"],"title":"Does the idler light passing through the optical fiber amplifier still entangle with the signal light?","uri":"/spdc_optical_amp/"},{"categories":["Quantum Information"],"content":"Two, Optical Amplifier Based on Stimulated Emission The above is about amplifiers based on parametric down-conversion. But the most commonly used EDFA is not based on $\\chi^{(2)}$ nonlinearity, but on stimulated emission amplification, similar to the principle of lasers. At this point, the Hamiltonian is much more complicated than OPA because it involves atoms. Let’s first consider a single-mode optical field: $H = \\hbar\\omega a^\\dag a + \\sum_j \\hbar \\omega_a \\sigma^+_j \\sigma^-_j + \\hbar \\Omega \\sum_j (\\sigma^+_j a -\\sigma_j^- a^\\dag)$ where $\\omega$ is the frequency of the optical field, $\\omega_a$ is the energy level difference of the atom, $\\sigma^{\\pm}$ is the raising and lowering operators of the atomic levels, $\\Omega$ is the Rabi frequency. This is actually the Hamiltonian of the Tavis-Cummings model. There is no general analytical solution for this thing, so we can only do perturbation approximations. After some perturbation calculations [1], we can obtain: $(\\Delta X)^2 = G (\\Delta X_{0})^2 + (2 n_{\\text{sp}}-1)(G-1) \\frac{1}{4}$ where $X$ is any quadrature of the optical field, which can be understood as the real or imaginary part of the electric field intensity. $G$ is the gain. $n_\\text{sp} = \\frac{N_2}{N_2-N_1}$ represents the degree of population inversion. The larger $n_\\text{sp}$, the lower the inversion degree, and the smaller $n_\\text{sp}$, the higher the inversion degree, with a minimum of 1. It can be seen that EDFA not only amplifies the noise of the input signal by a factor of $G$, but also introduces additional noise from vacuum fields, and amplifies the noise from vacuum fields by $(2 n_{\\text{sp}}-1)(G-1)$ times. This is the Amplified Spontaneous Emission (ASE), the noise amplified by spontaneous emission. See the figure below: Wigner representation of the quantum state after passing through EDFA. Image source [1] The above is about dynamics. How do these dynamics affect entanglement? It is well known that the signal and idler obtained by SPDC form an entangled state: $|\\Psi\\rangle_\\text{SPDC} \\approx |\\text{vac}\\rangle + \\lambda \\int \\mathrm{d}\\omega_s \\mathrm{d}\\omega_i f(\\omega_s,\\omega_i)a^\\dag_s(\\omega_s) a^\\dag_i(\\omega_i) |\\text{vac}\\rangle$, where $f(\\omega_s,\\omega_i)$ is the joint frequency distribution, depending on phase matching conditions and pump linewidth (momentum conservation and energy conservation conditions). $|\\text{vac}\\rangle$ represents the vacuum state on all modes, and $\\lambda \\ll 1$ represents the average number of photons of SPDC in the low gain limit, which is commonly used in experiments. $f(\\omega_s,\\omega_i)$ is usually inseparable, that is, it cannot be written as $f(\\omega_s,\\omega_i) = f_s(\\omega_s) f_i(\\omega_i)$. Therefore, the signal and idler modes of SPDC show frequency entanglement. Since it is frequency entangled, we need to consider the multi-mode situation. For simplicity, let’s consider only two frequency modes $\\omega = \\mu,\\nu$. Then the SPDC state can be written as: $|\\text{vac}\\rangle + \\lambda (|1\\rangle \\otimes |0\\rangle \\otimes |0\\rangle \\otimes |1\\rangle+ |0\\rangle \\otimes |1\\rangle \\otimes |1\\rangle \\otimes |0\\rangle)$ where from left to right, the four modes are respectively the $\\mu$ and $\\nu$ frequency modes of the signal and the $\\mu$ and $\\nu$ frequency modes of the idler. After amplification by EDFA, we get: $|0\\rangle \\otimes |0\\rangle \\otimes |G,0\\rangle \\otimes |G,0\\rangle +\\lambda(|1\\rangle \\otimes |0\\rangle \\otimes |G,0\\rangle \\otimes |G,1\\rangle+ |0\\rangle \\otimes |1\\rangle \\otimes |G,1\\rangle \\otimes |G,0\\rangle)$ where $|G,0\\rangle,|G,1\\rangle$ represent the states obtained by $|0\\rangle ,|1\\rangle$ after passing through EDFA. I really don’t know how to calculate this thing. But we can see some clues from their Wigner function shapes. Let’s first look at the Wigner functions of $|0\\rangle$ and $|1\\rangle$: Wigner functions of vacuum state and single photon state, image source Wikipedia After passing through the high gain EDFA, they ","date":"2024-05-27","objectID":"/spdc_optical_amp/:2:0","tags":["Quantum Optics"],"title":"Does the idler light passing through the optical fiber amplifier still entangle with the signal light?","uri":"/spdc_optical_amp/"},{"categories":["Quantum Information"],"content":"Reference Inoue, K. Quantum Noise in Optical Amplifiers. in Optical Amplifiers - A Few Different Dimensions (ed. Choudhury, P. K.) (InTech, 2018). doi:10.5772/intechopen.72992. ","date":"2024-05-27","objectID":"/spdc_optical_amp/:3:0","tags":["Quantum Optics"],"title":"Does the idler light passing through the optical fiber amplifier still entangle with the signal light?","uri":"/spdc_optical_amp/"},{"categories":["Quantum Information"],"content":"Too long; didn’t read version: “Spooky action at a distance (superluminal)” is an outdated view. According to special relativity, there is no interaction between events at spacelike separation. The mainstream view in academia is to consider quantum states and measurement bases together as reality, known as contextuality. Nonlocality is a corollary of contextuality. Body: Discussing the “speed” of quantum entanglement is meaningless. If the distance between two measurement events a and b is spacelike, then a is neither in the past nor in the future of b. This is because, in special relativity, if the distance between a and b is spacelike, whether a is in the past or future of b depends on the observer’s frame of reference. Therefore, there is no definite time sequence between a and b, and thus no causal relationship. In this case, we should say: the measurement results of a and b exhibit nonlocal correlations, rather than saying that the measurement result of a affects the measurement result of b, or vice versa. See the diagram below: Source [1], although it's from an MDPI journal, this diagram is very good In the above diagram, the two events a and b are outside each other’s light cone, so there is no interaction between them. What really exists is the correlation between their measurement results. Correlation is not hard to understand because they both originate from a Bell pair. What is truly difficult to understand is: when you change the measurement basis (refer to various Bell test experiments), you will find that this correlation also depends on the measurement basis, i.e., it changes with the measurement basis. This leads to at least one of the following conclusions: The quantum state $|\\psi\\rangle$ itself is incomplete and cannot fully describe reality. Only when the measurement basis $\\mathsf{E}$ is determined, $(|\\psi\\rangle,\\mathsf{E})$ is complete. This is called contextuality, meaning reality only exists when the measurement basis is determined, and does not exist before that. In this case, nonlocality is a corollary of contextuality. If you insist that local and non-contextual reality exists and propose a local and non-contextual hidden variable theory, then you must accept negative probabilities in this theory. In this case, your theory cannot be described by classical probability theory but must use non-Kolmogorov probability theory. If you believe that negative probabilities themselves violate reality, then refer back to the first point. The contextuality mentioned in the first point is currently mainstream in academia. There are more and more papers studying contextuality, which should be considered a small trend in quantum information research. The core idea of contextuality is: quantum reality only exists when the measuring instrument is given. Some people also call this sub-reality or contextual reality. Therefore, I personally think that the translation of contextuality is not very good and should be called “context dependence.” The “context” here refers to the measuring instrument. Unfortunately, a significant number of scholars still do not understand contextuality, and it is not introduced in most standard textbooks, let alone in popular science and mass culture. But this is understandable: contextuality was proposed in the 1990s, 30 years later than Bell’s theorem, and is somewhat at the forefront of academia. In summary: “spooky action at a distance (superluminal)” is an outdated view. According to special relativity, there is no interaction between events at spacelike separation. The mainstream view in academia is to consider quantum states and measurement bases together as reality, known as contextuality. Nonlocality is a corollary of contextuality. ","date":"2024-05-27","objectID":"/contextuality/:0:0","tags":["Quantum Information"],"title":"Does the speed of quantum entanglement exceed the speed of light?","uri":"/contextuality/"},{"categories":["Quantum Information"],"content":"References Entropy 2021, 23(12), 1660 https://doi.org/10.3390/e23121660 ","date":"2024-05-27","objectID":"/contextuality/:1:0","tags":["Quantum Information"],"title":"Does the speed of quantum entanglement exceed the speed of light?","uri":"/contextuality/"},{"categories":["Quantum Information"],"content":"Common sense tells us that an actual position measurement will not produce a delta function, as the delta function itself is pathological. So what should the collapsed state of an actual position measurement look like? TL;DR: Let the wave function of the system to be measured be $\\varphi(x)$, then the collapsed wave function is: $\\varphi^{(q)}(x) = \\mathcal{A}\\psi(q-gx) \\varphi(x)$ where $\\psi(x)$ is the initial wave function of the instrument’s pointer. $q$ is the reading of the instrument’s pointer. $g$ is the coupling strength between the instrument and the system to be measured. $\\begin{aligned} \\mathcal{A} = \\left[\\int_\\mathbb{R}\\psi(q-gx) \\varphi(x)\\right]^{-1} \\end{aligned}$ is the appropriate normalization constant. ","date":"2024-05-27","objectID":"/position_measurement/:0:0","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"1. Prerequisite Knowledge ","date":"2024-05-27","objectID":"/position_measurement/:1:0","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"1.1 Notation Convention $L^2(\\mathbb{R})$ : Hilbert space of square-integrable functions on $\\mathbb{R}$. $\\mathcal{L}(\\mathcal{H})$ : (bounded) operators on Hilbert space $\\mathcal{H}$. $\\mathbb{I}_{\\mathcal{H}}$ : Identity operator on Hilbert space $\\mathcal{H}$. $\\mathcal{E}(\\mathcal{H})$ : effect on Hilbert space $\\mathcal{H}$. Defined as $\\mathcal{E}(\\mathcal{H}) := \\\\{ E\\in \\mathcal{L}(\\mathcal{H}) | E =E^*, 0 \\le E\\le \\mathbb{I}_{\\mathcal{H}}\\\\}$. $\\text{Leb}(\\mathbb{R})$ : Lebesgue sigma algebra on $\\mathbb{R}$. ","date":"2024-05-27","objectID":"/position_measurement/:1:1","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"1.2 Some Basics of Quantum Information PVM (Projection-Valued Measurement) and POVM Quantum channel and its dual I do not intend to introduce them in detail here. Wikipedia or mathematics textbooks already provide comprehensive introductions. Please refer to them yourself. You can also read my article ","date":"2024-05-27","objectID":"/position_measurement/:1:2","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"1.3 Measure and Lebesgue Integral The spectra of position and momentum operators are continuous. Strictly formulating the continuous spectrum in mathematics requires measure and Lebesgue integral. I do not intend to introduce them in detail here. Wikipedia or mathematics textbooks already provide comprehensive introductions. Please refer to them yourself. Moreover, the letter M in PVM and POVM refers to Measure. Quantum mechanics is built on the foundation of probability theory, and probability theory is built on the foundation of measure theory. So knowledge about measure is necessary. ","date":"2024-05-27","objectID":"/position_measurement/:1:3","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"1.4 Spectral Decomposition of Position and Momentum Operators The position operator $Q \\in \\mathcal{L}(L^2(\\mathbb{R}))$ on $L^2(\\mathbb{R})$ has the spectral decomposition: $Q = \\int_\\mathbb{R} x\\mathsf{Q}(\\mathrm{d}x)$. This is a Lebesgue integral. Here, $\\mathsf{Q}$ is the position projection measurement, i.e., PVM (Projection-Valued Measure, a special case of POVM, Positive Operator-Valued Measure), defined as an operator-valued measure: $\\mathsf{Q}: \\text{Leb}(\\mathbb{R}) \\rightarrow \\mathcal{E}(L^2(\\mathbb{R}))$, $[\\mathsf{Q}(A) \\varphi](x):=\\chi_A(x) \\varphi(x)$, where $\\chi_A(x)=\\begin{cases} 1, \\quad x\\in A \\\\ 0, \\quad x \\notin A \\end{cases}$ is the characteristic function of set $A$. Selected reading: The momentum PVM can be defined as $\\mathsf{P}(A) = F^* \\mathsf{Q}(A) F$, where $F$ is the Fourier-Plancherel operator $\\mathcal{F} \\in \\mathcal{L}(L^2(\\mathbb{R}))$, defined as: $(\\mathcal{F}\\varphi)(p)=\\frac{1}{\\sqrt{2\\pi}}\\int_{\\mathbb{R}}\\mathrm{e}^{-ipx}\\varphi(x)\\mathrm{d}x$ ","date":"2024-05-27","objectID":"/position_measurement/:1:4","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"2. Measurement Model The overall idea of this section is as follows: Consider a system to be measured, with its Hilbert space being $\\mathcal{H}$ and state $\\rho$. Then we couple the system $\\mathcal{H}$ with another system $\\mathcal{K}$. You can imagine $\\mathcal{K}$ as a pointer on an instrument that can give different readings. Suppose the initial state of system $\\mathcal{K}$ is $\\sigma$, then the initial state of the composite system is $\\rho \\otimes \\sigma \\in \\mathcal{H} \\otimes \\mathcal{K}$. Next, we let the composite system evolve for a period of time, and the system state becomes $U(\\rho\\otimes \\sigma) U^\\dag$. Finally, we perform a PVM $\\mathsf{Z}$ measurement on the auxiliary system $\\mathcal{K}$, and according to Born’s rule, the probability of getting result $x$ is $\\operatorname{tr}[U(\\rho\\otimes \\sigma) U^\\dag(\\mathbb{I}_{\\mathcal{H}}\\otimes \\mathsf{Z(x)})]$. In this way, we achieve a POVM $\\mathsf{E}$ on $\\mathcal{H}$, satisfying $\\operatorname{tr}[\\rho \\mathsf{E(x)}]=\\operatorname{tr}[U(\\rho\\otimes \\sigma) U^\\dag(\\mathbb{I}_{\\mathcal{H}}\\otimes \\mathsf{Z(x)})]$. Define $\\mathcal{I}_x(\\rho) :=\\operatorname{tr}_\\mathcal{K}[\\rho \\mathsf{E(x)}]=\\operatorname{tr}_\\mathcal{K}[U(\\rho\\otimes \\sigma) U^\\dag(\\mathbb{I}_{\\mathcal{H}}\\otimes \\mathsf{Z(x)})]$, which is the quantum channel induced by the measurement process. If we find the Kraus operators $K_i$ of the quantum channel, so that $\\mathcal{I}_x(\\rho) :=\\sum_i K_i \\rho K_i^*$, we can easily obtain the dual channel $\\mathcal{I}_x^*: \\mathcal{I}_x^*(\\mathcal{M}) = \\sum_i K_i^* \\mathcal{M} K_i$, and the collapsed state after measurement is $\\mathcal{I}_x^*(\\mathbb{I_\\mathcal{H}}) = \\sum_i K_i^* K_i$. The above is the overall idea, and below we provide the specific calculation process. ","date":"2024-05-27","objectID":"/position_measurement/:2:0","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"2.1 Hamiltonian of the System and the Instrument Consider the system $\\mathcal{H}$ to be measured and the instrument $\\mathcal{K}$, with $\\mathcal{H} \\cong \\mathcal{K} \\cong L^2(\\mathbb{R})$, i.e., both $\\mathcal{H}$ and $\\mathcal{K}$ are Hilbert spaces of square-integrable functions on $\\mathbb{R}$. You can imagine $\\mathcal{K}$ as a pointer on an instrument that can give different readings. To measure the position of $|\\psi\\rangle\\in \\mathcal{H}$, we need to couple the instrument with the system to be measured and let them evolve for a period of time. The Hamiltonian of this evolution process is $U =e^{-\\mathrm{i} g Q\\otimes P}$. Here, $Q$ is the position operator on $\\mathcal{H}$, $P$ is the momentum operator on $\\mathcal{K}$, and $g$ is the coupling strength (the evolution time $t$ is absorbed into it). This Hamiltonian is not difficult to understand because the pointer of the instrument needs to move on the dial to give different readings, and the generator of translation is the momentum operator. Therefore, intuitively, $U$ can be understood as generating a translation on $\\mathcal{K}$ according to the position of $|\\psi\\rangle\\in \\mathcal{H}$. ","date":"2024-05-27","objectID":"/position_measurement/:2:1","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"2.2 Evolution According to the spectral decomposition $Q = \\int_{\\mathbb{R}} x\\mathsf{Q}(\\mathrm{d}x)$, we can write $U$ as: $U = \\int_\\mathbb{R} \\mathsf{Q}(\\mathrm{d}x) \\otimes e^{-\\mathrm{i}g x P}$ Let the initial states of $\\mathcal{H}$ and $\\mathcal{K}$ be $|\\psi\\rangle$ and $|\\varphi\\rangle$ respectively, then the initial state of the composite system $\\mathcal{H} \\otimes \\mathcal{K}$ is: $|\\psi \\otimes \\varphi\\rangle = |\\psi\\rangle\\otimes |\\varphi\\rangle$. The state after evolution is: $U(|\\psi\\rangle \\otimes |\\varphi\\rangle) = (\\int_\\mathbb{R} \\mathsf{Q}(\\mathrm{d}x) \\otimes e^{-\\mathrm{i}g x P})(|\\psi\\rangle \\otimes |\\varphi\\rangle)$ We rewrite the above equation in the integral form: $U(|\\psi\\rangle \\otimes |\\varphi\\rangle)=\\int_{\\mathbb{R}} \\mathsf{Q}(\\mathrm{d}x) |\\psi\\rangle \\otimes e^{-\\mathrm{i}g x P} |\\varphi\\rangle = \\int_{\\mathbb{R}} \\psi(x) |x\\rangle \\otimes e^{-\\mathrm{i}g x P} |\\varphi\\rangle$ ","date":"2024-05-27","objectID":"/position_measurement/:2:2","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"2.3 Measurement Finally, we measure the position of the pointer of the instrument, i.e., we perform a PVM $\\mathsf{Z}$ on $\\mathcal{K}$. The probability of getting the result $q$ is: $\\operatorname{tr}[(\\mathbb{I}_{\\mathcal{H}} \\otimes \\mathsf{Z}(q)) U (|\\psi \\otimes \\varphi\\rangle \\langle \\psi \\otimes \\varphi|) U^\\dag]$ According to Born’s rule, this is: $|(\\mathbb{I}_{\\mathcal{H}} \\otimes \\mathsf{Z}(q)) U (|\\psi \\otimes \\varphi\\rangle)|^2 = |\\int_{\\mathbb{R}} \\psi(x) |x\\rangle \\otimes \\langle q|e^{-\\mathrm{i}g x P} |\\varphi\\rangle|^2$ According to the definition of the momentum operator, we have: $\\langle q| e^{-\\mathrm{i}g x P} |\\varphi\\rangle = \\varphi(q-gx)$ So, the probability of obtaining result $q$ is: $|\\int_{\\mathbb{R}} \\psi(x) |x\\rangle \\varphi(q-gx)|^2=|\\psi(x)\\varphi(q-gx)|^2 = |\\int_{\\mathbb{R}} \\psi(x) \\varphi(q-gx)|^2$. Thus, we have the POVM $\\mathsf{E}$ on $\\mathcal{H}$: $\\mathsf{E}(q) =|\\psi(x)\\varphi(q-gx)|^2$ ","date":"2024-05-27","objectID":"/position_measurement/:2:3","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"2.4 Collapsed State At this point, in $\\mathcal{I}_{A}(\\rho)$, let $A={q}$, and the quantum state of the system $\\mathcal{H}$ to be measured collapses to $\\frac{K(q) \\rho K(q)^*}{\\text{tr}[K(q) \\rho K(q)^*]}$, where $q$ is the reading of the instrument. When $\\rho = |\\varphi\\rangle\\langle\\varphi|$ is a pure state, its collapsed wave function is: $\\varphi^{(q)}(x) = \\mathcal{A}\\psi(q-gx) \\varphi(x)$ where $\\begin{aligned} \\mathcal{A} = \\left[\\int_\\mathbb{R}\\psi(q-gx) \\varphi(x)\\right]^{-1} \\end{aligned}$ is the appropriate normalization constant. Here $q$ is the reading of the probe. If the probe state is a “widened” Gaussian wave packet: $\\psi(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left[-\\frac{x^2}{2\\sigma^2}\\right]$, then $\\varphi^{(q)}(x) = \\mathcal{A} \\exp\\left[-\\frac{(gx-q)^2}{2\\sigma^2}\\right] \\varphi(x)$ This geometric interpretation is already very intuitive. In particular, if the probe state itself is a delta function $\\psi(x) = \\delta(x)$, then the collapsed state after measurement is $\\varphi^{(q)}(x) = \\delta(x-\\frac{q}{g})$ Also a delta function. ","date":"2024-05-27","objectID":"/position_measurement/:2:4","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"2.5 Measurement Model The above analysis process is very useful. We can abstract it into a measurement model: $\\mathfrak{M}=(\\mathcal{K},\\sigma,U,\\mathsf{Z})$ . Here $\\mathcal{K}$ is the ancillary system (can be understood as the instrument), $\\sigma$ is the initial state of system $\\mathcal{K}$, $U$ is the evolution of the composite system $\\mathcal{H} \\otimes \\mathcal{K}$, $\\mathsf{Z}$ is the projection measurement (PVM) on the ancillary system. These physical objects implement a POVM measurement on $\\mathcal{H}$. In actual experiments, the ancillary system $\\mathcal{K}$ can be regarded as the probe of the instrument, and $\\mathsf{Z}$ is the reading of the probe. ","date":"2024-05-27","objectID":"/position_measurement/:2:5","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"3. Quantum-Classical Transition Finally, you may wonder: what state does the probe collapse to after measurement? This is a very interesting question. Because we use PVM $\\mathsf{Q}$ to measure the position of the instrument probe, it seems that the probe state should collapse to a delta function. In this way, the above analysis seems to simply change one problem into another problem. That is: the problem of how the quantum state of the system to be measured collapses is transformed into the problem of how the probe state collapses. But in practice, our instrument is a macroscopic classical system, not a quantum system, so the actual situation is more complicated. If you understand some knowledge about POVM and quantum measurement models, you will know: (non-PVM) POVM itself can only provide the probability of measurement results. What the collapsed state is specific to can only be given by a measurement model, and this measurement model will inevitably involve an ancillary system $\\mathcal{K}$. If you want to know the quantum state of $\\mathcal{K}$, you have to measure the system $\\mathcal{K}$ with another system $\\mathcal{M}$, and so on. This cyclical process is the process of transition from quantum to classical. The transition from quantum to classical involves the measurement problem, which is an unresolved problem in quantum mechanics, and I don’t want to discuss it too much here. Interested readers can search the literature for further study. In this way, the measurement model seems to be similar to decoherence, both of which seem to solve the measurement problem, but in fact do not. Nevertheless, they still provide us with a lot of insight. Of course, the above only talks about the problem of observable quantities with continuous spectra. For observable quantities with discrete spectra, PVM is easily implemented in experiments. PVM can directly give the collapsed state after measurement, and this state has good properties and does not have the pathological properties of delta functions. ","date":"2024-05-27","objectID":"/position_measurement/:3:0","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"4. Classical Instrument To avoid the problem mentioned above, we can define a course-grained classical instrument that can only distinguish whether particles are in a certain interval: $\\mathsf{Z}: \\{0,1\\} \\rightarrow \\mathcal{E}(L^2(\\mathbb{R}))$ $\\mathsf{Z}(1) = \\int_a^b \\mathsf{Q}(\\mathrm{d}x)$ $\\mathsf{Z}(0) = \\mathbb{I}_{L^2(\\mathbb{R})} - \\int_a^b \\mathsf{Q}(\\mathrm{d}x)$ Or distinguish particle positions with a certain precision $g$: $\\mathsf{Z}: \\mathbb{Z} \\rightarrow \\mathcal{E}(L^2(\\mathbb{R}))$ $\\mathsf{Z}(n) =\\int_{gn}^{g(n+1)} \\mathsf{Q}(\\mathrm{d}x)$ Note that they are both PVMs, so the collapsed state can also be determined. The calculation of these two cases is left to the reader as an exercise. ","date":"2024-05-27","objectID":"/position_measurement/:4:0","tags":["Quantum Measurement Theory"],"title":"The collapsed state of position measurement","uri":"/position_measurement/"},{"categories":["Quantum Information"],"content":"The motivation for defining creation and annihilation operators is simple and can be entirely derived from classical mechanics. Think about how we solve the classical harmonic oscillator. Since position and momentum are coupled: $\\begin{cases} \\frac{\\mathrm{d}x}{\\mathrm{d}t} = \\omega p \\\\ \\frac{\\mathrm{d}p}{\\mathrm{d}t} = -\\omega x \\end{cases}$ That is, $\\frac{\\mathrm{d}}{\\mathrm{d}t} \\begin{bmatrix} x \\\\ p \\end{bmatrix} = \\begin{bmatrix} 0 \u0026 \\omega \\\\ -\\omega \u0026 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ p \\end{bmatrix}$ So, we just need to decouple them. By diagonalizing, we obtain the eigenvectors $a^{\\pm} = x \\pm \\mathrm{i} p$, and the derivatives of $a^{\\pm}$ only depend on themselves: $\\frac{\\mathrm{d}a^{\\pm}}{\\mathrm{d}t} = \\mp \\mathrm{i} \\omega a^{\\pm}, \\quad a^{\\pm} = a_0^{\\pm} \\exp (\\mp \\mathrm{i} \\omega t)$ In this procedure, $a^{+}$ and $a^{-}$, when quantized, become the annihilation and creation operators. Note that $a^{+} \\rightarrow \\hat{a}, \\quad a^{-} \\rightarrow \\hat{a}^\\dag$, then $[\\hat{a}, \\hat{a}^\\dag] = [\\hat{x} + \\mathrm{i}\\hat{p}, \\hat{x} - \\mathrm{i}\\hat{p}] = -\\mathrm{i}[\\hat{x}, \\hat{p}] = 1$. The rest of the story is well known. The multimode case is also simple. Each time we decouple an eigenvector, we call it a mode, corresponding to the creation and annihilation operators for that mode. This is how quasiparticles (like phonons) originate. In classical mechanics, $a^{+}$ and $a^{-}$ are just techniques extended to the complex domain for convenience in solving problems. In quantum mechanics, however, they have tangible physical meanings. Hence, quantum mechanics cannot do without complex numbers. ","date":"2024-05-27","objectID":"/annilation_op/:0:0","tags":["Mathematical Methods in Physics"],"title":"How are creation and annihilation operators derived?","uri":"/annilation_op/"},{"categories":["Quantum Information"],"content":"This article aims to derive the coherent state of a laser from basic quantum dynamics. Consider the interaction between a two-level system and a single-mode light field, where the frequency of the light field equals the energy level difference. The two-level system initially starts in the excited state $|e\\rangle$, and the light field is in the vacuum state $|0\\rangle$. According to the Jaynes-Cummings model, even if the light field is in the vacuum state, spontaneous emission will occur. After a short period, the quantum state evolves from $|e,0\\rangle$ to: $|\\psi\\rangle = a |e,0\\rangle + b |g,1\\rangle$ where $|g,1\\rangle$ represents the two-level system in the ground state, and the coupled light field mode contains one photon, which is the spontaneously emitted photon. Now consider $N$ two-level systems in the excited state, which are not coupled with each other but only with the light field. The initial spontaneously emitted photon induces stimulated emission in other two-level systems, so the overall quantum state can be written as: $|\\Psi\\rangle = C|\\psi\\rangle^{\\otimes N} = C\\sum_k \\sum_m\\mathcal{P}_m[|e,0\\rangle^{\\otimes (N-k)} \\otimes |g,1\\rangle^{\\otimes k}]$ where $C$ is a normalization constant to be determined, and $\\mathcal{P}_m$ denotes all possible permutations of $|e,0\\rangle^{\\otimes (N-k)} \\otimes |g,1\\rangle^{\\otimes k}$, with a total of $\\frac{N!}{(N-k)!,k!}$ permutations. Note that we have not yet used the Fock space (i.e., we haven’t symmetrized for identical bosons). This step is quite informal, and possibly incorrect, because it doesn’t explain why stimulated emission means that the overall quantum state is the tensor product of the spontaneous emission state $|\\psi\\rangle = a |e,0\\rangle + b |g,1\\rangle$. However, we will see later that it gives the correct result. Therefore, this derivation must contain some insight that I haven’t yet identified. The quantum state of the light field is obtained by tracing out the quantum state of the two-level systems: $\\rho = \\operatorname{Tr}_{\\text{a}}[|\\Psi\\rangle\\langle\\Psi|] = C^2 \\sum_k |a^{N-k}b^k|^2 \\sum_m\\mathcal{P}_m[(|0\\rangle\\langle 0|)^{\\otimes (N-k)} \\otimes (|1\\rangle\\langle 1|)^{\\otimes k}]$ Now we symmetrize the tensor product of identical particles and map it to Fock space, which is well-known as $|1\\rangle ^{\\otimes k} \\rightarrow \\sqrt{k!} |k\\rangle$, so the above equation becomes $\\rho = C^2 \\sum_k |a^{N-k}b^k|^2 \\left(\\frac{N!}{(N-k)!,k!}\\right)^2 k! |k\\rangle\\langle k|$ When $k \\ll N$, $\\frac{N!}{(N-k)!} \\sim \\left(\\frac{N}{e}\\right)^k$. Let $\\alpha = \\frac{b}{a} \\cdot \\frac{N}{e}$, then the above equation can be written as: $\\rho = C^2 a^N \\sum_k \\frac{|\\alpha|^{2k}}{k!} |k\\rangle\\langle k|$ Normalizing it, we get: $\\rho = e^{-|\\alpha|^2} \\sum_k \\frac{|\\alpha|^{2k}}{k!} |k\\rangle\\langle k|$ This is a Poisson distribution with an average particle number of $|\\alpha|^2$. In the coherent state representation, it can be written as: $\\rho = \\int_0^{2\\pi} \\frac{\\mathrm{d}\\theta}{2\\pi} |\\alpha e^{\\mathrm{i}\\theta}\\rangle \\langle \\alpha e^{\\mathrm{i}\\theta}|$ It is thus a phase-randomized coherent state. The coherent state $|\\alpha\\rangle$ corresponds to a classical electromagnetic wave with very well-defined amplitude and phase, but here its phase is integrated from $0$ to $2\\pi$. This represents our ignorance of the phase information, which fundamentally comes from our ignorance of the microscopic state of the two-level systems (the gain medium). Therefore, this $\\rho = \\rho_{\\text{epistemic}}$ is an epistemic quantum state. From the perspective of an ontic quantum state, the quantum state of the system should collapse to a state with a definite phase $\\rho_{\\text{ontic}} = |\\alpha\\rangle$, but we cannot know which phase it is. This is because, at optical frequencies $\\sim 10^{15} \\text{Hz}$, humans cannot observe the absolute phase. However, relative phase can be observed, so we can still observe interference phenomena. If the reader is curious abo","date":"2024-05-27","objectID":"/laser_quantum/:0:0","tags":["Quantum Optics"],"title":"A simple quantum description of lasers","uri":"/laser_quantum/"},{"categories":["Quantum Information"],"content":"References Bartlett, S. D., Rudolph, T. \u0026 Spekkens, R. W. Dialogue Concerning Two Views on Quantum Coherence: Factist and Fictionist. Int. J. Quantum Inform. 04, 17–43 (2006). ","date":"2024-05-27","objectID":"/laser_quantum/:1:0","tags":["Quantum Optics"],"title":"A simple quantum description of lasers","uri":"/laser_quantum/"},{"categories":["Quantum Information"],"content":"Preface A paper published in PRA in 1992 [1] pointed out that photons also have orbital angular momentum (OAM). Compared to spin angular momentum (i.e., polarization, SAM) which can only take $\\pm \\hbar$, orbital angular momentum can take any integer multiple of $\\hbar$. Such orbital angular momentum can be carried by helical wavefronts. You might be surprised: did people only discover this in 1992? In fact, helical wavefronts had been studied before 1992; photons carrying angular momentum greater than $\\hbar$ had already been predicted by atomic physics (though they originate from higher-order transition processes, which do not satisfy the selection rules and thus have very low probabilities, making them practically unobservable in experiments). It wasn’t until 1992 that Allen et al. pointed out that light beams with helical wavefronts carry quantized orbital angular momentum. From 1992 to 2001, research on OAM was mostly in the domain of macroscopic electromagnetic waves (coherent states of a large number of photons). In a paper in 2001 [2] by Nobel laureate Zeilinger’s group, the orbital angular momentum and its entanglement of single photons were studied for the first time. Since then, OAM has also gained the interest of researchers in quantum information and quantum optics. One potential application of OAM in quantum information is quantum precision measurement, as its $e^{\\mathrm{i}l\\varphi}$ factor helps to increase the precision of angle measurements by a factor of $l$. Additionally, SAM and OAM can interact through a q-plate, which is beneficial for quantum information processing. OAM also has many other classical applications [3], which will not be elaborated here. ","date":"2024-05-27","objectID":"/oam/:1:0","tags":["Quantum Optics"],"title":"Orbital angular momentum of photons","uri":"/oam/"},{"categories":["Quantum Information"],"content":"1. Hermite-Gaussian (HG) Beams Before introducing Laguerre-Gaussian beams that carry orbital angular momentum, it is best to introduce Hermite-Gaussian beams. Why introduce Gaussian beams? Because the electromagnetic waves obtainable in experiments are not ideal plane waves but Gaussian beams output by lasers. Waist width w0, Rayleigh length zR, and divergence angle theta of a Gaussian beam, image source Wikipedia The electric field amplitude of a common Hermite-Gaussian beam is as follows: $\\begin{aligned} E_{m,n}(x,y,z) = \u0026 ,E_0 \\color{blue}{ \\frac{w_0}{w(z)}\\exp\\left[-\\frac{x^2+y^2}{w(z)^2}\\right] } \\times \\\\ \u0026 \\color{green}{\\exp\\left[-\\mathrm{i}\\frac{k(x^2+y^2)}{2 R(z)}\\right]} \\color{orange}{\\exp\\left[\\mathrm{i}\\psi(z)\\right]} \\color{gray}{\\exp[-\\mathrm{i}kz]} \\times \\\\ \u0026 \\color{red}{H_m\\left[\\sqrt{2}\\frac{x}{w(z)}\\right]H_n\\left[\\sqrt{2}\\frac{y}{w(z)}\\right]} \\end{aligned} $ where: $\\color{blue}{w(z) = w_0 \\sqrt{1+\\left(\\frac{z}{z_R}\\right)^2}}$ represents the beam width at $z$, $z_R = \\pi\\frac{w_0^2}{\\lambda}$ is the Rayleigh length, defined as the $z$ value at which the beam width diverges to $\\sqrt{2}$ times the waist width. $\\color{green}{R(z) = \\frac{z^2 + z_R^2}{z}}$ represents the curvature radius of the wavefront at $z$. $\\color{orange}{\\psi(z)=\\arctan\\left(\\frac{z}{z_R}\\right)}$ represents the Gouy phase shift. There is an extra phase shift of $\\pi$ from negative infinity to positive infinity. $\\color{red}{H_m(t)}$ is the $m$-th order Hermite polynomial. These are also the energy eigenfunctions of the quantum harmonic oscillator. When $m,n$ are both zero, it degenerates to the fundamental Gaussian beam. Most lasers output fundamental Gaussian beams. As can be seen, remembering the Gaussian beam is not difficult, just remember three characteristics: $\\color{blue}{\\text{waist divergence}}$, $\\color{green}{\\text{curvature radius}}$, $\\color{orange}{\\text{Gouy phase shift}}$. Another useful quantity is the divergence angle. From $\\color{blue}{w(z) = w_0 \\sqrt{1+\\left(\\frac{z}{z_R}\\right)^2}}$, it can be seen that the tangent value of the divergence angle is $\\tan \\theta=\\frac{w_0}{z_R}$, so $w_0 \\tan \\theta = \\frac{\\lambda}{\\pi}$. The shape of a Gaussian beam is completely determined by any two of the wavelength, waist radius, and divergence angle. The relationship between these three is given by $w_0 \\tan \\theta = \\frac{\\lambda}{\\pi}$. $E_{m,n}(x,y,z)$ only represents the amplitude of the electric field and does not include polarization information. Adding polarization information is easy: if it is $x$-direction polarized, simply multiply $E_{m,n}(x,y,z)$ by the unit vector in the $x$ direction $\\hat{x}$. If it is circularly polarized, multiply by $\\hat{x} \\pm \\mathrm{i}\\hat{y}$. Note that $E_{m,n}(x,y,z)$ is a complex number. For classical electromagnetic waves, the real part of this is the actual electric field amplitude. For quantum electromagnetic waves, we can assign an annihilation operator to this mode, which is a linear combination of plane wave annihilation operators, with coefficients obtainable by Fourier analysis. HG modes, image source Wikipedia ","date":"2024-05-27","objectID":"/oam/:2:0","tags":["Quantum Optics"],"title":"Orbital angular momentum of photons","uri":"/oam/"},{"categories":["Quantum Information"],"content":"2. Laguerre-Gaussian (LG) Beams In the solutions of the two-dimensional quantum harmonic oscillator, besides the linear vibrations represented by Hermite polynomials, there are rotational motions represented by Laguerre polynomials. Below we give the form of the Laguerre-Gaussian beam: $\\begin{aligned} E_{l,p}(r,\\varphi,z) = \u0026 ,E_0 \\color{blue}{ \\frac{w_0}{w(z)}\\exp\\left[-\\frac{r^2}{w(z)^2}\\right] } \\times \\\\ \u0026 \\color{green}{\\exp\\left[-\\mathrm{i}\\frac{kr^2}{2 R(z)}\\right]} \\color{orange}{\\exp\\left[\\mathrm{i}\\psi(z)\\right]} \\color{gray}{\\exp[-\\mathrm{i}kz]} \\times \\\\ \u0026 \\color{red}{\\left(\\sqrt{2}\\frac{r}{w(z)}\\right)^{|l|} L_p^{|l|} \\left(2 \\frac{r^2}{w(z)^2}\\right) \\exp(-\\mathrm{i}l\\varphi)} \\end{aligned} $ where $\\color{blue}{\\text{waist divergence}}$ and $\\color{green}{\\text{curvature radius}}$ are the same as those of the Hermite-Gaussian beam. There are two differences: First, the higher-order terms change from Hermite polynomials to Laguerre polynomials $\\color{red}{L_p^{|l|} (t)}$. The additional factors $\\color{red}{\\left(\\sqrt{2}\\frac{r}{w(z)}\\right)^{|l|}}$ and $\\color{red}{\\exp(-\\mathrm{i}l\\varphi)}$ represent a singularity at the origin and a helical wavefront (carrying orbital angular momentum $\\hbar l$), respectively. Second, the Gouy phase shift $\\color{orange}{\\psi(z) = (2p + |l|+ 1)\\arctan \\frac{z}{z_R}}$ becomes $(2p + |l|+ 1)$ times that of the HG mode. LG modes, image source Wikipedia Just as rotation can be decomposed into a linear combination of vibrations, LG modes can also be represented as coherent combinations of HG modes. This is similar to circular polarization being decomposable into a combination of two linear polarizations. ","date":"2024-05-27","objectID":"/oam/:3:0","tags":["Quantum Optics"],"title":"Orbital angular momentum of photons","uri":"/oam/"},{"categories":["Quantum Information"],"content":"3. Manipulating Spin Angular Momentum and Orbital Angular Momentum with a Q-Plate A q-plate, proposed in 2006 [4], is essentially a half-wave plate with its optical axis varying with position, and the angle $\\alpha$ of the optical axis satisfies $\\alpha(r,\\varphi) = q \\varphi + \\alpha_0$. Its Jones matrix is $\\begin{bmatrix} \\cos 2\\alpha \u0026 \\sin 2\\alpha \\\\ \\sin 2\\alpha \u0026 -\\cos 2\\alpha \\end{bmatrix}$. Optical axis pattern of a q-plate, image source [4] Consider a q-plate acting on a left-handed polarized fundamental Gaussian beam, resulting in: $\\begin{bmatrix} \\cos 2\\alpha \u0026 \\sin 2\\alpha \\\\ \\sin 2\\alpha \u0026 -\\cos 2\\alpha \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{\\mathrm{i}}{\\sqrt{2}} \\end{bmatrix} = e^{\\mathrm{i}2q\\varphi} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{\\mathrm{i}}{\\sqrt{2}} \\end{bmatrix}$ As can be seen, after passing through the q-plate, the left-handed polarized light becomes right-handed polarized light carrying $2q\\hbar$ orbital angular momentum. Q-plates are currently the most commonly used method for preparing OAM states in experiments. Of course, spatial light modulators (SLM) can also be used. One reason why OAM attracts the interest of quantum information researchers is that its Hilbert space dimension, like that of a harmonic oscillator, is infinite, unlike polarization (SAM), which only has two states. The interaction between SAM and OAM leads to a straightforward idea: using SAM and OAM to make a CNOT gate, where SAM is the control bit and OAM is the target bit. There are many similar experiments, which are not listed here. ","date":"2024-05-27","objectID":"/oam/:4:0","tags":["Quantum Optics"],"title":"Orbital angular momentum of photons","uri":"/oam/"},{"categories":["Quantum Information"],"content":"References Allen, L.; Beijersbergen, M. W.; Spreeuw, R. J. C.; Woerdman, J. P. Orbital Angular Momentum of Light and the Transformation of Laguerre-Gaussian Laser Modes. Phys. Rev. A 1992, 45 (11), 8185–8189. https://doi.org/10.1103/PhysRevA.45.8185. A. Mair, A. Vaziri, G. Weihs, and A. Zeilinger, “Entanglement of the orbital angular momentum states of photons,” Nature 412(6844), 313–316 (2001). Padgett, M. J. Orbital Angular Momentum 25 Years on [Invited]. Opt. Express 2017, 25 (10), 11265. https://doi.org/10.1364/OE.25.011265. Marrucci, L.; Manzo, C.; Paparo, D. Optical Spin-to-Orbital Angular Momentum Conversion in Inhomogeneous Anisotropic Media. Phys. Rev. Lett. 2006, 96 (16), 163905. https://doi.org/10.1103/PhysRevLett.96.163905. ","date":"2024-05-27","objectID":"/oam/:5:0","tags":["Quantum Optics"],"title":"Orbital angular momentum of photons","uri":"/oam/"},{"categories":["Quantum Information"],"content":"Many physicists use an example to explain quantum entanglement to the public: Imagine you have two boxes, one containing pizza and the other a burger. Before opening the boxes, you cannot know what is inside. Alice and Bob each take a box and move to distant locations. When Alice opens her box, she instantly knows what is in Bob’s box far away. This example must be wrong… because it doesn’t involve quantum mechanics at all. Indeed, this example demonstrates classical correlation, not quantum entanglement. Let pizza be $|0\\rangle$ and burger be $|1\\rangle$, then in the above example, the state of the system is $\\rho_{\\text{classical correlation}} = \\frac{1}{2} \\left( |01\\rangle\\langle 01| + |10\\rangle\\langle 10| \\right)$ In matrix form, it is: $\\rho_{\\text{classical correlation}} = \\begin{bmatrix} 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 \\frac{1}{2} \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 \\frac{1}{2} \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \\end{bmatrix}$ However, true quantum entanglement should be as follows: $\\rho_{\\text{quantum entanglement}} = |\\psi\\rangle\\langle\\psi | = \\frac{1}{2} \\left( |01\\rangle\\langle 01| + |10\\rangle\\langle 10| + |01\\rangle\\langle 10| + |10\\rangle\\langle 01 | \\right)$ Where $|\\psi\\rangle = \\frac{1}{\\sqrt{2}} (|01\\rangle + |10 \\rangle)$ In matrix form, it is: $\\rho_{\\text{quantum entanglement}} = \\begin{bmatrix} 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 \\frac{1}{2} \u0026 \\frac{1}{2} \u0026 0 \\\\ 0 \u0026 \\frac{1}{2} \u0026 \\frac{1}{2} \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \\end{bmatrix}$ As you can see, true quantum entanglement has off-diagonal terms. These off-diagonal terms are also called coherences. Similarly, when the above quantum entangled state decoheres, classical correlation is obtained. In other words, classical correlation is the result of the decoherence of quantum entanglement. Decoherence is also an interpretation of the quantum measurement problem: when a system entangles with the environment, if we examine the partial trace of the system, we find that the system’s state is decohered. In this way, quantum entanglement becomes classical correlation, and the density matrix only retains the diagonal terms. But decoherence cannot explain the problem of wavefunction collapse. Because the following process is still caused by the collapse of the overall wavefunction of the system and the environment. $\\begin{bmatrix} 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 \\frac{1}{2} \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 \\frac{1}{2} \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \\end{bmatrix} \\xrightarrow{\\text{collapse}} \\begin{cases} \\begin{bmatrix} 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \\end{bmatrix}, \u0026 \\text{with }\\frac{1}{2} \\text{ probability} \\\\ \\begin{bmatrix} 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \\end{bmatrix}, \u0026 \\text{with }\\frac{1}{2} \\text{ probability} \\end{cases}$ This step cannot be explained by decoherence. Therefore, the measurement problem has not yet been resolved. This is also understandable: decoherence itself is still a unitary evolution process, while wavefunction collapse is non-unitary. So, we can only temporarily accept that wavefunction collapse is an axiom in modern quantum mechanics, rather than a theorem that can be derived. This leads to a completeness problem: we cannot determine through quantum mechanics alone whether a process should be a unitary evolution or a non-unitary collapse process. This problem is also called the Heisenberg cut, which is how to find such a “cut” so that below it the system evolves normally, while above it the system collapses. Of course, since our existing measuring instruments are systems with a large number of particles, we don’t need to worry about this problem—we are very certain that our measurements will induce the collapse of the system. In categorical quantum theory, describing quantum entanglement requires the tensor category of $\\mathbf{Hilb}$, while describing classical correlation only requires the tensor category of $\\mathbf{Rel}$. The tensor product of the former is the tensor product of Hilbert spaces, while the tensor product of the latter is the Cartesian pro","date":"2024-05-27","objectID":"/classical_correlation/:0:0","tags":["Quantum Measurement Theory"],"title":"The difference between quantum entanglement and classical correlation, and the Quantum Measurement Problem","uri":"/classical_correlation/"},{"categories":["Quantum Information"],"content":"I. Introduction to Quantum Metrology Quantum metrology is the science of using the quantum properties of quantum states for precise measurements. The reason for studying quantum metrology is that the precision of any physical measurement is limited by the Heisenberg Uncertainty Principle in quantum mechanics, known as the Heisenberg Limit. The goal of quantum metrology is to approach and surpass this limit. Of course, most measurements in daily life do not need to reach the Heisenberg Limit (e.g., weighing oneself). However, in high-precision imaging and various high-precision scientific experiments, people have indeed approached the Heisenberg Limit. Gravitational Waves: LIGO (Light Interferometer Gravitational-wave Observatory) is an optical interferometer used to detect gravitational waves. It can detect changes in the length of the interference arms down to $10^{-19}$ m, which is one ten-thousandth the size of a proton and one billionth the wavelength of light. At this insane level of precision, LIGO uses the squeezed state of the light field to reduce quantum noise. Of course, quantum noise does not only appear in extreme cases like LIGO. Below are two examples related to imaging. Shot Noise: You have certainly used a CMOS camera. Have you noticed that when the environment is dark, the photos you take have a lot of noise? This is caused by shot noise. What is shot noise? The electrons excited by photons come one by one, and their generation follows a Poisson distribution (completely random generation time), with the relative variance inversely proportional to the average number of electrons. This variance is the shot noise. Therefore, when the number of electrons is small, their relative variance is large, resulting in a low signal-to-noise ratio (SNR) and causing noise to appear. Shot noise is given by the particle-number-phase uncertainty principle. However, due to historical reasons, it is not called the Heisenberg Limit but the shot noise limit or the standard quantum limit, distinguishing it from the Heisenberg Limit. Diffraction Limit: Imaging precision is measured not only in time (i.e., signal-to-noise ratio) but also in space (i.e., the number of pixels). If we infinitely increase the number of pixels, can we achieve infinite precision? If you have studied optics, you know this is impossible due to the diffraction limit. In fact, the diffraction limit is a type of Heisenberg Limit because it corresponds to a momentum-position uncertainty. If we need more accurate positioning, the diffraction limit tells us that without changing the wavelength, we need to increase the aperture size. From the perspective of the Heisenberg principle, this is because photons entering the aperture at different angles have different momenta, and this momentum information is lost at the moment of imaging. The more momentum information lost, the more accurate the photon’s position measurement. Conversely, to reduce the wavelength without changing the aperture size, we need to decrease the wavelength, as smaller wavelengths correspond to larger momenta, and a larger momentum range received by the same aperture. These examples are quite interesting. I hope you enjoy them. Of course, even more interesting is to come: what mathematical tools do people use to characterize quantum noise? How do people design quantum states to achieve the Heisenberg Limit? After reading this article, you will have a deeper understanding of these issues and complete your introduction to quantum metrology. ","date":"2024-04-23","objectID":"/quantum_metrology/:1:0","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"II. Maximum Likelihood Estimation What is measurement? First, we need to define measurement. Note that measuring the same physical quantity always gives a series of different results with certain probabilities (whether in classical or quantum physics). We usually take the average of these results as the final result. Thus, the measurement process actually gives a series of random variables (i.e., individual measurement results), and we construct statistics (i.e., functions of random variables) like the average to estimate the physical quantity itself. We call this statistic an estimator. This estimator is also a random variable with its own mean and variance. We refer to the estimator’s value as the measurement result, and its variance as the measurement error. The relative error is the error divided by the mean; the measurement precision is the reciprocal of the relative error. At this point, we have defined the measurement process, which is essentially a parameter estimation problem: Parameter Estimation: Given a probability density function $p(\\mathbf{x};\\theta)$, where $\\mathbf{x}$ is the measurement result and $\\theta$ is the physical quantity to be measured (a parameter of the probability density function rather than its variable), construct an estimator $\\hat{\\theta} = \\hat{\\theta}(\\mathbf{x})$ such that its mean is equal to or close to the parameter (unbiasedness) and its variance is as small as possible (efficiency). Additionally, there is an implicit condition: we require that as the sample size increases, the estimator approaches the true parameter value (consistency). Unbiasedness: Unbiasedness means that $\\mathbb{E}(\\hat{\\theta}(X)) = \\theta$, i.e., the mean of the estimator equals the true value of the parameter. Efficiency: How do we measure efficiency? We will introduce it later, but the variance of the estimator has a CR lower bound: $(\\Delta \\hat{\\theta})^2 \\ge \\frac{1}{F(\\theta)}$, where $F(\\theta)$ is called Fisher information. Efficiency can be measured by $F(\\theta)/(\\Delta \\hat{\\theta})^2 \\le 1$. When $F(\\theta)/(\\Delta \\hat{\\theta})^2 = 1$, meaning the lower bound is reached, we call the statistic efficient. Consistency: Consistency requires $\\lim_{n\\rightarrow \\infty} P(|\\hat{\\theta} - \\theta|\u003e\\epsilon) = 0$ for any positive number $\\epsilon \u003e 0$. Here, $n$ is the number of samples. This is also called convergence in probability: $\\operatorname{plim}_{n\\rightarrow \\infty} \\hat{\\theta} = \\theta$. The most commonly used method in statistics for dealing with parameter estimation problems is Maximum Likelihood Estimation (MLE). The idea is simple: find a parameter such that, under this parameter, the probability of obtaining the observed results is the highest. The specific steps are also simple: Maximum Likelihood Estimation: Let the likelihood function $f(\\theta; \\mathbf{x}) = p(\\mathbf{x}; \\theta)$, and find its maximum point $\\hat{\\theta}(\\mathbf{x}) = \\operatorname{arg,max} f(\\theta; \\mathbf{x})$. $\\hat{\\theta}(\\mathbf{X})$ is the estimator given by maximum likelihood estimation. In practice, people often take the logarithm of the likelihood function to simplify calculations, i.e., $\\log f(\\theta; \\mathbf{x})$, called the log-likelihood function. In fact, maximum likelihood estimation usually does not satisfy unbiasedness, but people still favor it. This is because maximum likelihood estimation is always asymptotically efficient (variance reaches the CR lower bound as the sample size tends to infinity) and asymptotically unbiased (the mean of the estimator tends to the true parameter value as the sample size increases). In quantum measurements, maximum likelihood estimation is also used in most cases. Therefore, this article will focus on maximum likelihood estimation. Other parameter estimation methods include Bayesian estimation and method of moments. These, along with maximum likelihood estimation, form the three most important parameter estimation methods. ","date":"2024-04-23","objectID":"/quantum_metrology/:2:0","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"III. Fisher Information and CR Lower Bound The likelihood function is the core of maximum likelihood estimation. Let’s see what quantities can be derived from the likelihood function. First, we get a quantity called the Score Function, which is the derivative of the log-likelihood function with respect to the true parameter value: Score Function: The Score Function $s(\\theta; \\mathbf{x})$ is defined as: $s(\\theta; \\mathbf{x}) = \\frac{\\partial}{\\partial \\theta} \\log f(\\theta; \\mathbf{x})$, where $f(\\theta; \\mathbf{x})$ is the likelihood function. Its meaning is also clear: it indicates the impact of changes in the parameter value on the likelihood function. If the derivative is positive, the parameter value needs to increase because increasing the parameter value increases the likelihood function (i.e., the probability is higher). Conversely, if the derivative is negative, the parameter value needs to decrease. When is the parameter value ideal? Of course, it is when the derivative is zero. This is easy to understand: the derivative of the likelihood function at its maximum point is zero. Of course, the derivative being zero is only a necessary condition, not a sufficient condition. To ensure that the zero point of the likelihood function’s derivative corresponds to a maximum, the second derivative needs to be negative. Hence, we define Fisher Information: Fisher Information: Fisher Information is the expectation (with respect to the random variable $X$) of the negative second derivative of the log-likelihood function (with respect to the parameter $\\theta$): $F(\\theta) = \\mathbb{E}\\left(-\\frac{\\partial^2}{\\partial \\theta^2} f(\\theta, X)\\right)$ It can actually be proven that Fisher Information is always non-negative. Therefore, setting the derivative of the likelihood function to zero always yields a maximum value. In fact, Fisher Information can also be expressed using the square of the first derivative, so it is certainly non-negative: $F(\\theta) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta} f(\\theta, X)\\right)^2\\right]$ (proof omitted). Significance of Fisher Information: The significance of Fisher Information is the sensitivity of the likelihood function to the parameter near its maximum. Intuitively, the more sensitive the likelihood function is, the more efficient our estimate is. Because when the likelihood function is sensitive to the parameter, a small change in the parameter causes a sharp decrease in the likelihood function, allowing us to determine the parameter with high confidence. Conversely, if the likelihood function is not sensitive to the parameter, we cannot narrow down the parameter to a small range because the parameter can vary over a wide range without significantly decreasing the likelihood function. Understanding the significance of Fisher Information, we can quickly understand the Cramér-Rao Bound: Cramér-Rao Bound: The variance of an unbiased estimator has the following lower bound: $(\\Delta \\theta)^2 \\ge \\frac{1}{F(\\theta)}$ where $F(\\theta)$ is Fisher Information. This is called the Cramér-Rao Bound. This essentially quantifies the significance of Fisher Information into an inequality. The larger the Fisher Information, the more sensitive the likelihood function is to the parameter, and the smaller the variance of the parameter estimator can be. Conversely, the smaller the Fisher Information, the less sensitive the likelihood function is to the parameter, and the larger the variance of the parameter estimator will be. So far, the maximum likelihood estimation, Fisher Information, and CR lower bound we have introduced have nothing to do with quantum mechanics, but they are necessary foundations. In the next section, we will introduce quantum Fisher Information and quantum CR lower bound, and apply them to quantum measurements. ","date":"2024-04-23","objectID":"/quantum_metrology/:3:0","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"IV. Quantum Fisher Information and Quantum CR Bound In this section, we will apply all the statistical concepts mentioned in the previous section to quantum mechanics. This is possible because quantum measurements can also be described using probability theory. First, we need a probability density function. In quantum mechanics, given a state and a POVM, we can obtain a probability density function: Probability Density Function: $p_{\\rho}^{E}(x)=\\operatorname{tr}[\\rho E(x)]$, where $\\rho$ is the quantum state being measured, $E(x)$ is the POVM, satisfying $\\sum_{x\\in X} E(x) = \\mathbb{I}$, where $X$ is the set of all possible measurement results, and $\\mathbb{I}$ is the identity operator. In quantum mechanics, the physical quantity we want to measure is encoded in the quantum state, i.e., $\\rho(\\theta)$, so the likelihood function is: Likelihood Function: $f^{E}(\\theta; x) = p_{\\rho(\\theta)}^{E}(x) = \\operatorname{tr}[\\rho(\\theta) E(x)]$ Note that this likelihood function depends on the POVM $E$. For each POVM, we have a likelihood function. Taking the negative second derivative of the likelihood function and taking the expectation value, we get the Fisher information as follows: Fisher Information: $F^{E}(\\theta) = \\mathbb{E}\\left[(\\frac{\\partial}{\\partial \\theta} f^{E}(\\theta; X))^2\\right]$ Note that Fisher Information depends not only on the parameter $\\theta$ but also on the selected POVM, i.e., the chosen measurement method. Now we can define quantum Fisher information, which is the largest among the Fisher information corresponding to different POVMs. Quantum Fisher Information: $F(\\theta) = \\sup_{E\\in \\mathsf{E}} F^{E}(\\theta)$, where $\\mathsf{E}$ is the set of all POVMs. In other words, quantum Fisher information is the supremum of classical Fisher information. So far, we have only given a series of definitions and have not obtained any new results. Next, we will further restrict the problem to hope to get some useful results. We restrict the form of $\\rho(\\theta)$. The parameter $\\theta$ is usually encoded in a unitary process: $U = e^{-iH\\theta}$. Applying this unitary process to a state $\\rho_0$, we get $\\rho(\\theta) = e^{-iH\\theta}\\rho_0 e^{iH\\theta}$. Here $\\rho_0$ can also be called a probe state. Why Unitary Encoding Here we give an example. If we want to know the phase difference introduced by a birefringent crystal between H and V polarized light, we need to consider the transformation $e^{i\\frac{Z}{2}\\theta} = \\begin{bmatrix} e^{i\\theta/2} \u0026 0 \\\\ 0 \u0026 e^{-i\\theta/2} \\end{bmatrix}$, where $Z$ is the Pauli Z matrix. If the Hilbert space is not just a two-level system (a qubit) but can be larger (qudit or multiple qubits), the unitary transformation acting on the state vector is a larger unitary transformation that can always be diagonalized as $\\sum_{k} e^{i\\theta_k} |k\\rangle\\langle k|$. In other words, its eigenvalues are always complex numbers with a modulus of one. Therefore, the important information is the phase of these complex numbers. How to design measurement circuits to detect these phases is the problem of Quantum Phase Estimation. Quantum Phase Estimation is a fundamental and important algorithm primitive in quantum computing. Next, let’s look at the derivative of $\\rho(\\theta)$. Those familiar with quantum mechanics should be familiar with it, and the result will have a commutator: $\\frac{\\partial}{\\partial \\theta} \\rho(\\theta) = i[\\rho, H]$ But there is actually another derivative representation called the Symmetric Logarithmic Derivative (SLD): Symmetric Logarithmic Derivative (SLD): $\\frac{\\partial}{\\partial \\theta} \\rho(\\theta) = \\frac{1}{2}\\{\\rho, L_{\\rho}(H)\\} = \\frac{1}{2}[\\rho L_{\\rho}(H) + L_{\\rho}(H) \\rho]$ where $L_{\\rho}(H)$ is defined as the operator satisfying $\\frac{1}{2}\\{\\rho, L_{\\rho}(H)\\} = i[\\rho, H]$. Its explicit form is Explicit Form of SLD: $L_{\\rho}(H) = 2i\\sum_{k,l} \\frac{\\lambda_k - \\lambda_l}{\\lambda_k + \\lambda_l} \\langle k | H | l \\rangle | k \\rangle\\langle l |$, where","date":"2024-04-23","objectID":"/quantum_metrology/:4:0","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"V. Quantum Phase Estimation Previously, we mentioned quantum phase estimation, which is: Quantum Phase Estimation: Given a unitary transformation, find the phase of its eigenvalues. If the considered Hilbert space is n-dimensional, then there are (n-1) phases to be estimated (global phase can be ignored). We have not yet touched upon quantum estimation with multiple parameters (we will discuss this in the appendix). Now, we consider the case of only one parameter. There are two scenarios: two-level systems and harmonic oscillator systems. ","date":"2024-04-23","objectID":"/quantum_metrology/:5:0","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"5.1 Two-Level Systems Let’s consider a two-level system with Hamiltonian $H = \\frac{\\hat{Z}}{2}$, and the initial state of the system is a pure state, i.e., $|\\psi(\\theta)\\rangle = e^{-i \\frac{\\hat{Z}}{2} \\theta} |\\psi_0\\rangle $. In this case, the quantum Fisher information is: $ \\begin{aligned} F(\\theta) \u0026= 4 \\left(\\Delta \\frac{\\hat{Z}}{2}\\right)^2 = (\\Delta \\hat{Z})^2 \\\\ \u0026= \\left\\langle \\psi_0 \\left| \\hat{Z}^2 \\right| \\psi_0 \\right\\rangle - \\left\\langle \\psi_0 \\left| Z \\right| \\psi_0 \\right\\rangle^2 \\\\ \u0026= 1 - \\left\\langle \\psi_0 \\left| \\hat{Z} \\right| \\psi_0 \\right\\rangle^2 \\end{aligned}$ To maximize the quantum Fisher information, we require $\\left\\langle \\psi_0 \\left| \\hat{Z} \\right| \\psi_0 \\right\\rangle = 0$, in other words, $|\\psi_0\\rangle$ should lie on the equator of the Bloch sphere. This also aligns with our intuition: the closer the initial state is to the equator, the more evident the rotation of the Bloch sphere will be on the initial state. If the initial state is at the north or south pole, then the Fisher information will be zero, meaning the initial state cannot sense any rotation. Choosing the observable $X$ or $Y$ both can achieve the CR lower bound. Let’s directly choose the observable as $X + i Y$ (although this is not an observable, we can separately observe $X$ and $Y$, and then combine the results into a complex number), with an expected value of $e^{i\\theta}$ and a variance of 1. Then we have $1 =|\\Delta e^{i\\theta}| = |ie^{i\\theta}\\Delta\\theta| = \\Delta \\theta$. In other words, $\\Delta \\theta = \\frac{1}{F(\\theta)}$, achieving the CR lower bound. ","date":"2024-04-23","objectID":"/quantum_metrology/:5:1","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"5.2 Harmonic Oscillator Systems Now let’s consider a harmonic oscillator system with Hamiltonian $H = \\hat{N} = \\hat{a}^\\dag \\hat{a}$ corresponding to the unitary transformation $e^{i\\hat{N}\\theta} = \\sum_{k=0}^{\\infty} e^{i N \\theta} |N\\rangle\\langle N|$. In this case, the quantum Fisher information is: $ \\begin{aligned} F(\\theta) \u0026= 4 \\left(\\Delta \\hat{N}\\right)^2 \\\\ \u0026= 4 [\\langle \\psi | \\hat{N}^2 | \\psi \\rangle - \\langle \\psi | \\hat{N} | \\psi \\rangle^2] \\end{aligned}$ Next, we classify and discuss different initial states (probe states). ","date":"2024-04-23","objectID":"/quantum_metrology/:5:2","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"5.2.1 Number State $| \\psi_0 \\rangle = | N \\rangle$ In this case, the quantum Fisher information is: $F(\\theta) = 0$. This is easy to understand because the phase of the number state is completely random (according to the particle number-phase uncertainty principle). It can be seen that number states cannot be used to detect phase. ","date":"2024-04-23","objectID":"/quantum_metrology/:5:3","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"5.2.2 Coherent State $|\\psi_0 \\rangle = | \\alpha \\rangle$ In this case, the quantum Fisher information is: $F(\\theta) = 4 |\\alpha|^2 = 4 \\bar{n}$. It gives the CR lower bound of measurement error: $\\Delta \\theta \\ge \\frac{1}{2 \\sqrt{\\bar{n}}}$. This is the limit allowed by shot noise. It is given by the particle number-phase uncertainty relation. However, we generally do not call this the Heisenberg limit; instead, we call it the shot noise limit or the standard quantum limit. In the literature, this limit is often distinguished from the Heisenberg limit. This is because we can further increase $\\Delta N$ to reduce $\\Delta \\theta$. When $\\Delta$ is proportional to $\\bar{n}$, i.e., $\\Delta \\theta \\sim \\frac{1}{\\bar{n}}$, we call this the Heisenberg limit. […] ","date":"2024-04-23","objectID":"/quantum_metrology/:5:4","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"VI. Quantum Imaging and Quantum Illumination ","date":"2024-04-23","objectID":"/quantum_metrology/:6:0","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"VII. Other Quantum Parameter Estimation Problems ","date":"2024-04-23","objectID":"/quantum_metrology/:7:0","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"7.1 Gain Sensing ","date":"2024-04-23","objectID":"/quantum_metrology/:7:1","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"7.2 Noise Sensing ","date":"2024-04-23","objectID":"/quantum_metrology/:7:2","tags":["Quantum Metrology"],"title":"A beginner's guide to quantum metrology","uri":"/quantum_metrology/"},{"categories":["Quantum Information"],"content":"Quantum State Tomography Quantum state tomography is the process of deducing the [quantum state] based on the [results of measurements on a quantum state ensemble]. Its formulation is straightforward, as follows: Given a series of measurement operators ${\\Pi_1, \\ldots, \\Pi_n}$ and their corresponding measurement probabilities $p_k = \\operatorname{Tr}[\\rho \\Pi_k]$, find the quantum state $\\rho$. In other words, in these $n$ equations $p_k = \\operatorname{Tr}[\\rho \\Pi_k]$, given $p_k$ and $\\Pi_k$, find $\\rho$. Conversely, if $\\rho$ and $p_k$ are known, and you want to find $\\Pi_k$, that’s called Detector tomography. However, that’s not the topic of this article. Why care about state tomography? Because we want to know what the quantum state prepared in the experiment looks like. The method is state tomography. From the equation $p_k = \\operatorname{Tr}[\\rho \\Pi_k]$, you can see that if $\\rho$ has $n$ degrees of freedom, we need at least $n$ measurement operators, and they must be “independent of each other.” This condition is known as tomographical completeness or informational completeness. Why is it called “Tomography”? State tomography is called “tomography” because the tomography of continuous-variable photon states is really similar to the CT (Computed Tomography) in hospitals. CT reconstructs a 3D image from projection images from various angles, while the tomography of continuous-variable photon states reconstructs the Wigner function from projection images at various angles. The latter is a kind of “two-dimensional image.” It’s also understandable with discrete variables because (von Neumann) measurement is essentially projection, and tomography is reconstructing the original quantum state from the measured projections. In short, tomography is named because it is the process of reconstructing the whole from projections. ","date":"2024-04-22","objectID":"/state_tomography/:1:0","tags":["Quantum Optics"],"title":"Quantum state tomography - a beginner's guide","uri":"/state_tomography/"},{"categories":["Quantum Information"],"content":"State Tomography of a Single Qubit A qubit’s density operator has only three degrees of freedom (Bloch vector), so in theory, only three measurement operators are needed. For example, you can choose ${(\\mathbb{I} + X)/2,\\\\,\\\\, (\\mathbb{I} + Y)/2,\\\\,\\\\, (\\mathbb{I} + Z)/2 }$ as the three measurement operators (where $X, Y, Z$ are Pauli matrices). They correspond to the X, Y, Z axes on the Bloch sphere, which are equivalent to measuring the x, y, and z components of the Bloch vector. However, the issue is that in experiments, we do not directly measure probabilities but rather counts. If we want to know the probability $p_Z = \\operatorname{Tr}[\\rho (\\mathbb{I} + Z)/2] = \\langle0|\\rho|0\\rangle$, we must use counts $p_Z = \\frac{N_Z}{N_Z + N_{-Z}}$, while measuring $N_{-Z}$ requires another measurement operator $(\\mathbb{I} - Z)/2$. In general, since we measure counts and require normalization, the actual number of measurement operators required is always one more than the theoretical requirement. And the degrees of freedom for the density operator of $N$ qubits is $4^n - 1$, and adding one precisely equals $4^n$. For $n = 1$, we need four measurement operators. This was also the method adopted by the earliest article on photon state tomography[1]. Okay, let’s dive deeper into the state tomography of a single qubit, which will help us expand to multiple qubits later. A qubit’s density operator is $\\rho = \\frac{\\mathbb{I} + xX + yY + zZ}{2}$. Tomography involves solving for $x, y, z$. As mentioned earlier, we choose four measurement operators: ${(\\mathbb{I} + X)/2,\\\\,\\\\, (\\mathbb{I} + Y)/2,\\\\,\\\\, (\\mathbb{I} + Z)/2,\\\\,\\\\, (\\mathbb{I} - Z)/2 }$. Thus, the measured counts are: $N_X = N p_X = N \\operatorname{Tr}[\\rho \\frac{\\mathbb{I} + X}{2}] = N \\frac{1 + x}{2}$, $N_Y = N p_Y = N \\operatorname{Tr}[\\rho \\frac{\\mathbb{I} + Y}{2}] = N \\frac{1 + y}{2}$, $N_Z = N p_Z = N \\operatorname{Tr}[\\rho \\frac{\\mathbb{I} + Z}{2}] = N \\frac{1 + z}{2}$, $N_{-Z} = N p_{-Z} = N \\operatorname{Tr}[\\rho \\frac{\\mathbb{I} - Z}{2}] = N \\frac{1 - z}{2}$. By solving these four equations together, we can find the unknowns $x, y, z, N$. Okay, that’s it. It’s that simple. ","date":"2024-04-22","objectID":"/state_tomography/:2:0","tags":["Quantum Optics"],"title":"Quantum state tomography - a beginner's guide","uri":"/state_tomography/"},{"categories":["Quantum Information"],"content":"Maximum Likelihood Estimation Wait, is it really that simple? Observant readers might notice that the density operator must be positive definite, but the above solution process doesn’t guarantee positive definiteness. In fact, following the above steps, it is entirely possible to derive non-positive-definite density operators. In other words, the result might be “non-physical.” This is because experimental data is subject to error, which can lead to non-physical results under certain conditions. Note, this is not due to errors caused by an imperfect experimental system but by quantum statistical errors, or fluctuations. For example, for coherent states, the count distribution is Poissonian, with variance equal to the mean. When time is limited, and not much data can be collected, the relative error $\\frac{\\Delta N}{N} = \\frac{1}{\\sqrt{N}}$ might be quite high. Some might suggest: “What if we add a constraint to ensure positive definiteness?” In this example, as long as we require $|x|, |y|, |z| \\leq 1$, positive definiteness can be guaranteed. But here’s the problem: Adding constraints leads to no solution when faced with errors. Okay, given that no solution is caused by errors, we can resort to a different approach, converting the problem from [solving equations] to [fitting]. What is fitting? We know that two points determine a straight line. However, if the data points themselves have large errors, we might need many points, which almost certainly do not lie on a single straight line. In this case, there’s no solution for the straight-line equation. Even with no solution, we can fit! Using the least-squares method! So where does the least-squares method come from? The answer is Maximum Likelihood Estimation (MLE). The basic idea behind MLE is to find a straight line (among many) that is most likely to generate the measured points. The specific implementation involves setting a likelihood function with straight-line parameters as independent variables, where the function value represents the probability of measuring these results with the given straight-line parameters. The whole problem then becomes an optimization problem: Find the maximum value of the likelihood function and its corresponding straight-line parameters. In many cases, experimental errors are assumed to follow a normal distribution, so the likelihood function is exponential in the form $L = e^{-\\mathcal{L}}$. By taking the logarithm of the likelihood function and inverting it, we get a new likelihood function, $\\mathcal{L}$, and the optimization problem becomes finding the minimum value of the new likelihood function. This new likelihood function can generally be interpreted as “given this straight line, the total error between the theoretical and actual measurements.” All of these ideas can be directly applied to our state tomography. We can set a likelihood function with the degrees of freedom for the positive-definite matrix $\\rho(x,y,z)$, where the function value is the probability of measuring the observed results given quantum state $\\rho$. Let’s convert these ideas into an algorithm: Define a valid density operator $\\rho(x,y,z) = \\frac{\\mathbb{I} + xX + yY + zZ}{2}$, with $|x|, |y|, |z| \\leq 1$. Define the likelihood function as the probability of measuring the observed results, given quantum state $\\rho$: $ \\begin{aligned} \\mathcal{L}(x,y,z) \u0026= P(N_1 = n_1, N_2 = n_2, \\ldots) \\\\ \u0026= \\prod_k \\exp\\left[-\\frac{(n_k - \\hat{n}_k)^2}{2\\hat{n}_k}\\right] \\\\ \u0026= \\prod_k \\exp\\left[-\\frac{(n_k - N \\operatorname{Tr}[\\rho \\Pi_k])^2}{2N \\operatorname{Tr}[\\rho \\Pi_k]}\\right] \\end{aligned} $ In this case, we use the continuous normal distribution instead of the discrete Poisson distribution because when counts are large (typically a few hundred to a few thousand), the normal distribution can closely approximate the Poisson distribution. More importantly, the likelihood function of the normal distribution provides an unbiased estimate, just as effective as the Poisson ","date":"2024-04-22","objectID":"/state_tomography/:3:0","tags":["Quantum Optics"],"title":"Quantum state tomography - a beginner's guide","uri":"/state_tomography/"},{"categories":["Quantum Information"],"content":"State tomography of multiple qubits The density matrix of n qubits has $4^n -1$ degrees of freedom. In this case, the corresponding Bloch vector consists of Pauli Correlators: $\\begin{aligned} \\rho = \\frac{1}{2^n}\\sum_{i_1,\\ldots,i_n=0}^{3} c_{i_1i_2\\ldots i_n} \\sigma_{i_1} \\otimes \\sigma_{i_2} \\otimes \\ldots \\sigma_{i_n} \\end{aligned}$ where $\\sigma_i,\\\\,i=0,1,2,3$ represent the identity matrix and the Pauli matrices $\\mathbb{I},X,Y,Z$. The coefficients $c_{i_1i_2\\ldots i_n}$ are called correlators. In this case, we need at least $4^n$ measurement operators to measure and count, and then we can do maximum likelihood estimation. The state $\\rho$ can be set as $\\rho(\\mathbf{t})=\\frac{T^\\dag(\\mathbf{t})T(\\mathbf{t})}{\\operatorname{Tr}(T^\\dag(\\mathbf{t})T(\\mathbf{t}))}$, where $T(\\mathbf{t})=\\begin{bmatrix} t_1 \u0026 0 \u0026 0 \u0026 \\ldots \u0026 0 \\\\ t_{2^n+1}+it_{2^n+2} \u0026 t_2 \u0026 0 \u0026 \\ldots \u0026 0 \\\\ t_{2^n+5}+it_{2^n+6} \u0026 t_{2^n+3}+it_{2^n+4} \u0026 t_3 \u0026 \\ldots \u0026 0 \\\\ \\ldots \u0026 \\ldots \u0026 \\ldots \u0026 \\ddots \u0026 \\ldots \\\\ t_{4^n - 1}+it_{4^n} \u0026 t_{4^n - 3}+it_{4^n-2} \u0026 t_{4^n - 5}+it_{4^n-4} \u0026 \\ldots \u0026 t_{2^n} \\end{bmatrix}$ This ensures that $\\rho(\\mathbf{t})$ is positive definite. Compared to treating the correlators $c_{i_1i_2\\ldots i_n}$ as parameters and constraining their absolute values to be no greater than one, the advantage of parameterizing with T(t) is that it transforms the upcoming maximum likelihood estimation from a constrained problem into an unconstrained problem (as there are no constraints on t). The likelihood function in this case is: $\\begin{aligned} \\mathcal{L}(\\mathbf{t})=\\sum_k\\frac{\\left(p_k-\\hat{p}_k\\right)^2}{2\\hat{p}_k}=\\sum_k\\frac{\\left(p_k-\\operatorname{Tr}[\\rho(\\mathbf{t})\\Pi_k]\\right)^2}{2\\operatorname{Tr}[\\rho(\\mathbf{t})\\Pi_k]} \\end{aligned}$ ","date":"2024-04-22","objectID":"/state_tomography/:4:0","tags":["Quantum Optics"],"title":"Quantum state tomography - a beginner's guide","uri":"/state_tomography/"},{"categories":["Quantum Information"],"content":"Selection of measurement operators What types of measurement operators are best to use? In general, for polarization-encoded photon qubits, we simply use six projection measurements—H, V, D, A, R, L—and their combinations (they are easy to implement, requiring only wave plates). For n qubits, there are $6^n\u003e4^n$ projection operators, which is sufficient. Since $(|H\\rangle\\langle H|, |V\\rangle\\langle V|)$ form a POVM, $(|D\\rangle\\langle D|, |A\\rangle\\langle A|)$ form a POVM, and $(|R\\rangle\\langle R|, |L\\rangle\\langle L|)$ form a POVM, which means six projection measurements correspond to three POVMs. Thus, we need $3^n$ POVMs, or $3^n$ measurement settings (where “measurement settings” refer to wave plate angles). However, there is a question: is there a better way to reduce the number of POVMs, thereby reducing the time required for tomography? After all, $6^n\u003e4^n$ is redundant. From this point, you could say it’s on the academic frontier, where I could promote some of our group’s work. More on this later, haha. ","date":"2024-04-22","objectID":"/state_tomography/:5:0","tags":["Quantum Optics"],"title":"Quantum state tomography - a beginner's guide","uri":"/state_tomography/"},{"categories":["Quantum Information"],"content":"State tomography of continuous-variable photonic states Continuous-variable photonic states consider the entire Fock space, rather than just the subspace of zero-photon and single-photon Fock space. The reason we care about continuous variables is that photons are fundamentally harmonic oscillators, not two-level systems. Considering the former is more natural and aligns better with the nature of photons. Indeed, the classical electromagnetic waves in our daily life are a type of continuous-variable photonic state (coherent state). A single-mode continuous-variable photonic state can be described by quasi-probability distribution functions. There are three such functions: Glauber P-, Husimi Q-, and Wigner functions, referred to as P-, Q-, and W-, respectively. Among them, only the Wigner function’s marginal distributions correspond to actual observable probability distributions, which is why we typically consider the Wigner function. What is the Wigner function? How do we understand the Wigner function? Let’s start from classical electromagnetic waves. We know that single-mode (monochromatic) electromagnetic waves can be described by a point on the complex plane (i.e., using a phasor diagram). Superposition of electromagnetic waves directly corresponds to complex number addition (or vector addition). This so-called phasor representation can be immediately extended to quantum mechanics. For a quantized single-mode classical electromagnetic field, its phasor representation is no longer an infinitesimal point but a circle with a certain size. In fact, it is a Gaussian function on the complex plane, with the standard deviation determined by the uncertainty principle. This Gaussian function is the Wigner function of the classical electromagnetic wave, and its marginal distribution corresponds to the probability distribution of the quadrature field strength (the electric field is also an observable and has uncertainty). The quantum state obtained by quantizing a classical electromagnetic wave is called the Coherent state. For a more general quantum optical field, its Wigner function can have strange shapes and can even take on negative values. However, this does not mean we get negative probabilities—after taking the marginal distribution, we always get a non-negative probability. This is because the area occupied by these negative values is small enough to be smaller than the area allowed by the uncertainty principle, meaning the uncertainty principle ensures that we cannot observe negative probability. Some interesting Wigner functions for photon states. The left image shows a squeezed state, and the right image shows a Schrödinger cat state. State tomography of continuous-variable photon states is completed by measuring quadrature. In this case, all measurement operators have the following form: $\\begin{aligned} \\Pi_\\theta = \\frac{ae^{-i\\theta}+a^\\dag e^{i\\theta}}{2} \\end{aligned}$ This is the so-called “quadrature” (I don’t know the Chinese translation). Conceptually, it projects the entire Wigner function onto a straight line, with this line making an angle $\\theta$ with the x-axis. Quantum state tomography of continuous-variable optical states Quantum information theory tells us that if we want to reconstruct the Wigner function with infinite precision, we need infinitely many values of $\\theta$, which means countless measurement operators. Of course, we don’t need infinite precision, so we only need to take $\\theta$ at some reasonable intervals. For example, taking steps of 0.01 radians from 0 to 2pi. After obtaining the probability density function of the quadrature, we can use the inverse Radon transform to reconstruct the Wigner function. This is similar to the principle used in hospital CT scans. As for how to measure quadratures experimentally, it’s simple: like in radio technology, mix the signal with a local oscillator. This is called homodyne measurement. Specifically, this involves mixing the signal to be measured wi","date":"2024-04-22","objectID":"/state_tomography/:6:0","tags":["Quantum Optics"],"title":"Quantum state tomography - a beginner's guide","uri":"/state_tomography/"},{"categories":["Quantum Information"],"content":"Appendix: Maximum Likelihood Estimation for Poisson Distribution The likelihood function corresponding to the Poisson distribution is as follows: $ \\begin{aligned} \\mathcal{L}(\\mathbf{t}) \u0026= P(N_1=n_1,N_2=n_2,\\cdots) \\\\ \u0026= \\prod_k e^{-\\lambda_k} \\frac{\\lambda_k^{n_k}}{n_k!} \\\\ \u0026\\xrightarrow{-\\log} \\sum_k (\\lambda_k - n_k \\log \\lambda_k + \\cancel{\\log n_k!}) \\\\ \u0026\\xrightarrow{\\frac{1}{N}} \\sum_k(\\hat{p}_k(\\mathbf{t})-p_k \\log \\hat{p}_k(\\mathbf{t})) \\end{aligned}$ where $\\lambda_k = N \\operatorname{Tr}[\\rho(\\mathbf{t}) \\Pi_k] = N \\hat{p}_k(\\mathbf{t})$. By taking the derivative of the likelihood function to find the minimum, we can find that the minimum occurs at $\\hat{p}_k(\\mathbf{t}) = p_k$. This shows that both the Poisson distribution and the normal distribution yield unbiased estimators. However, the Poisson distribution’s likelihood function contains logarithms, while the normal distribution’s likelihood function contains only polynomials, making the latter more computer-friendly and converging faster. Therefore, we generally use the likelihood function corresponding to the normal distribution. ","date":"2024-04-22","objectID":"/state_tomography/:7:0","tags":["Quantum Optics"],"title":"Quantum state tomography - a beginner's guide","uri":"/state_tomography/"},{"categories":["Quantum Information"],"content":"References ^ James, D. F. V., Kwiat, P. G., Munro, W. J. \u0026 White, A. G. Measurement of qubits. Phys. Rev. A 64, 052312 (2001). ","date":"2024-04-22","objectID":"/state_tomography/:8:0","tags":["Quantum Optics"],"title":"Quantum state tomography - a beginner's guide","uri":"/state_tomography/"},{"categories":["Quantum Information"],"content":"A wave packet can correspond to a photon Example: A photon can be in a coherent superposition state of different frequencies: $|\\psi\\rangle=\\sum_{k}c_k|k\\rangle,\\quad \\sum_k|c_k|^2=1$, at which point the photon can behave as a wave packet. You can imagine an atom de-exciting and emitting a photon, and this photon will indeed behave as a wave packet. Some may argue: excluding various non-ideal factors, the linewidth of this photon only depends on natural broadening (lifetime), making it look like a single frequency, thus poorly localized, and not considered a wave packet. Indeed so. However, if we consider a single photon produced by a pulsed pump in a low-gain regime parametric process, its natural linewidth is very large. At this point, it is indeed in a coherent superposition state of different frequencies and behaves as a well-localized wave packet in the time domain. Definition of Single Photon State A state of the form $\\sum_{k}c_k|0,\\cdots,\\underbrace{1}_{k-\\text{th}},\\cdots,0\\rangle$ is called a single photon state, where $|0,\\cdots,\\underbrace{1}_{k-\\text{th}},\\cdots,0\\rangle$ indicates one photon in the k-th mode, and $\\sum_{k}|c_k|^2=1$. Intuitively, coherently superimposing states with one photon in different modes still results in a single photon state. ","date":"2024-04-13","objectID":"/photon_wave_packets/:0:1","tags":["Quantum Optics"],"title":"What is the relationship between photons and electromagnetic field wave packets?","uri":"/photon_wave_packets/"},{"categories":["Quantum Information"],"content":"But a photon is not necessarily a wave packet Counterexample 1: A photon passing through an NPBS (non-polarizing beam splitter) will travel two paths simultaneously: the transmitted path and the reflected path. At this point, the photon is non-localized. Counterexample 2: A circularly polarized photon passing through a PBS (polarizing beam splitter) will travel two paths simultaneously: the transmitted path and the reflected path. At this point, the photon is non-localized. If the paths are considered a degree of freedom of the Hilbert space, the photon is in an entangled state of path and polarization. Counterexample 3: A photon can also be two wave packets in the same path: using a PBS to split a photon into two paths, then using another PBS to recombine these two paths. If the optical paths are different, after recombining, the photon becomes two wave packets, one in front of the other. Counterexample 4: A photon can even interfere with itself: first splitting the photon into two wave packets, then controlling the two paths to be nearly identical, and finally recombining the two wave packets. This is the Michelson/Mach-Zehnder interferometer at the single-photon level. ","date":"2024-04-13","objectID":"/photon_wave_packets/:0:2","tags":["Quantum Optics"],"title":"What is the relationship between photons and electromagnetic field wave packets?","uri":"/photon_wave_packets/"},{"categories":["Quantum Information"],"content":"More interesting photon states The above three counterexamples are related to spatiotemporal modes and are well-known facts. Here are some other interesting states that might refresh your understanding of photons. Example 1: Frequency superposition state As mentioned above, a photon can be in a superposition state of different frequencies. More importantly, a photon can be two wave packets rather than one in the frequency domain. For example, a photon can be in a superposition state of wavelengths 1557nm and 1563nm. Such states can be prepared experimentally[1]. Example 2: Electric field superposition state (Schrödinger cat state) In cases of very few photons, the electric and magnetic fields also have significant uncertainties (just like momentum and position). The electric and magnetic fields form a pair of canonical variables. Experimentally, Schrödinger cat states can be prepared, where the electric field is in a superposition state of $|\\alpha\\rangle$ and $|-\\alpha\\rangle$. At this point, if you measure the electric field at a certain point (along the polarization direction), you might measure positive or negative values. When the photon number is large, such states are fragile and quickly decohere into a statistical mixture of $|\\alpha\\rangle$ and $|-\\alpha\\rangle$. However, at a low photon number level, such states can be prepared experimentally[2]. Example 3: Frequency entangled state Two photons can be in a frequency entangled state: $|\\psi\\rangle=\\frac{1}{2}(|\\mu\\rangle \\otimes|\\nu\\rangle+|\\nu\\rangle \\otimes|\\mu\\rangle)$. In short, if one photon is observed to have a wavelength of 1557nm, the other photon’s wavelength will immediately collapse to 1663nm, and vice versa[1]. Example 4: Number entangled state Experimentally, NOON states can be prepared: $|\\psi\\rangle=\\frac{1}{2}(|N,0\\rangle + |0,N\\rangle)$, meaning if one mode has N photons, the other mode has none, and vice versa. There can only be 0 or N, not other numbers. These two modes can be spatial modes. You can imagine a group of photons being in a superposition state of all going left and all going right. If you detect N photons on one side, you immediately know there are no photons on the other side, and vice versa. Of course, when the photon number is large, such states are quite fragile, making it difficult to maintain a stable phase relationship among all the photons. In summary, photons have many degrees of freedom. Any degree of freedom you can think of can be superimposed and entangled. The three difficulties at high photon numbers are preparation, decoherence, and characterization (state tomography). A photon can be a wave packet, but it doesn’t have to be; a wave packet can be a photon, but it doesn’t have to be. The key is to understand coherent superposition in quantum mechanics: $\\sum_{k}c_k|0,\\cdots,\\underbrace{1}_{k-\\text{th}},\\cdots,0\\rangle$. ","date":"2024-04-13","objectID":"/photon_wave_packets/:0:3","tags":["Quantum Optics"],"title":"What is the relationship between photons and electromagnetic field wave packets?","uri":"/photon_wave_packets/"},{"categories":["Quantum Information"],"content":"References ^abShukhin, A., Hurvitz, I., Trajtenberg-Mills, S., Arie, A. \u0026 Eisenberg, H. Two-dimensional Control of a Biphoton Joint Spectrum. Preprint at http://arxiv.org/abs/2311.09660 (2023). ^Lvovsky, A. I. et al. Production and applications of non-Gaussian quantum states of light. Preprint at http://arxiv.org/abs/2006.16985 (2020). ","date":"2024-04-13","objectID":"/photon_wave_packets/:1:0","tags":["Quantum Optics"],"title":"What is the relationship between photons and electromagnetic field wave packets?","uri":"/photon_wave_packets/"},{"categories":["Mathematical Physics"],"content":"One Differential is an Infinitesimal? Physicists like to think of differentials as very small quantities, which is convenient for calculations but gives a feeling of lack of rigor. In fact, it is indeed lacking rigor, and the second mathematical crisis arose from this. Rigor and clarity are always complementary. Treating differentials as infinitesimal satisfies intuition but cannot withstand rational scrutiny. ","date":"2024-03-13","objectID":"/understanding_differentials/:1:0","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Mathematical Physics"],"content":"Two Differential as Linear Function I like to think of differentials as a machine, for example, $f: f(x,y)=x^2+2y^2$ The differential at $(1,1)$ is a machine like this: It takes two numbers as input and produces one number as output. Written out, it looks like this: $\\mathrm{d}f|_{(1,1)}(a,b)=\\frac{\\partial f}{\\partial x}\\bigg|_{(1,1)}a+\\frac{\\partial f}{\\partial y}\\bigg|_{(1,1)}b=2a+4b$ Since the partial derivatives may vary at each point, if we say the differential at a certain point is a machine, then the differentials collectively form an ocean of machines. You may also notice that this machine is “linear”; it is simply a linear map from $\\mathbb{R}^2$ to $\\mathbb{R}$. The knowledge of linear algebra tells us that the collection of linear functions forms a linear space. In other words, linear functions themselves are “vectors” that can be added and scaled. Going back to the analogy above, it means we can add two machines: $\\mathrm{d}f|_p+\\mathrm{d}g|_p$ , and also scale a machine by a factor: $\\lambda \\cdot \\mathrm{d}f|_p $ . The subscript $p$ indicates the differential at point $p$. What does it mean to add machines? For example, if $\\mathrm{d}f|_p(a,b)=2a+4b,,, \\mathrm{d}g|_p(a,b)=a-b$ , then $\\mathrm{d}f|_p(a,b)+\\mathrm{d}g|_p(a,b)=3a+3b$ . This is similar to adding two vectors: $(2,4)+(1,-1)=(3,3)$ . In fact, they are vectors. Yes, you heard it right, differentials are vectors (fields). ","date":"2024-03-13","objectID":"/understanding_differentials/:2:0","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Mathematical Physics"],"content":"Three Differential as Covector Field ","date":"2024-03-13","objectID":"/understanding_differentials/:3:0","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Mathematical Physics"],"content":"3.1 Tangent Space If you want to study differentials on a curved surface, then you need to redefine some concepts. For example, differentials are linear, but manifolds are curved, so we need to define something linear within this curvature. This linear thing is the tangent space. As the name suggests, the tangent space is a tangent plane to a curved surface at a certain point. But how do we define it? The curvature here is not embedded in another space but intrinsic to space itself. We cannot find the equation of a tangent plane as in classical analytic geometry. Let’s try a different approach: Firstly, it is linear, which means it is a vector space. A vector space is a group equipped with scalar multiplication. Specifically, its elements (called vectors) satisfy the following properties: (1) Commutativity of vector addition (2) Associativity of vector addition (3) Existence of an identity element for vector addition (similar to zero) (4) Existence of an inverse element for vector addition (similar to the negative number) The above four properties indicate that the vector space is an abelian group with respect to vector addition. (5) Existence of a multiplicative identity for scalar multiplication (6) Associativity of scalar multiplication (7) Distributivity of scalar multiplication with respect to vector addition (8) Distributivity of scalar multiplication with respect to scalar addition A vector space does not have to be $\\mathbb{R}^n,,\\mathbb{C}^n$, as long as a set is defined with addition and scalar multiplication operations satisfying the above properties, it is a vector space. We can define such a vector space, where the elements are linear maps from a set of smooth functions to the real number field: $v:\\mathcal{F}_M\\rightarrow\\mathbb{R}$ such that (1) $v(\\lambda f+\\mu g)=\\lambda v(f)+\\mu v(g)$ (linearity) (2) $v|_p(f\\cdot g)=f|_p\\cdot v(g)+g|_p\\cdot v(f)$ (Leibniz law) where $\\mathcal{F}_M$ is the set of all smooth functions on the differential manifold $M$. It can be proven that the collection of $v$ forms a vector space. Although this definition seems complicated, it is actually just finding the directional derivative of a function at a certain point. Taking a bivariate function as an example, we can explicitly write out an instance of $v$: $\\left(a\\frac{\\partial}{\\partial x}+b\\frac{\\partial}{\\partial y}\\right)\\bigg|_p$ If $(\\frac{\\partial}{\\partial x}\\bigg|_p,\\frac{\\partial}{\\partial y}\\bigg|_p)$ is taken as the basis, then its coordinates are $(a,b)$. This linear space is the tangent space at point $p$ on the manifold, denoted as $T_pM$. All these operations are at a certain point $p$, where each point grows a tangent space. You can imagine that every point on a curved surface grows a tangent plane. ","date":"2024-03-13","objectID":"/understanding_differentials/:3:1","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Mathematical Physics"],"content":"3.2 Cotangent Space We just introduced the tangent space $T_p M$ . Now let’s call the dual space of the tangent space $T_p^*M$ the cotangent space. The differential at a point is precisely an element in the cotangent space. In other words, the differential at a certain point is a cotangent vector. Elements in the dual space (cotangent vectors) are linear functionals that map elements in the original space to a number. This is exactly what we said before about machines: But now the input should be elements in the tangent space: $\\left(a\\frac{\\partial}{\\partial x}+b\\frac{\\partial}{\\partial y}\\right)\\bigg|_p$ . We said that differentials form an ocean of machines. Therefore, the ocean of cotangent vectors (i.e., differentials) can be called the cotangent vector field. In mathematics, the differential (cotangent vector field) has a cool name called the cotangent bundle projection. For more details, see: https://zhuanlan.zhihu.com/p/629852598 After talking so much abstract nonsense, let’s define it below: The differential at a point $p$ on manifold $M$ is such a linear functional that acts on elements $v|_p\\in T_pM$ in the tangent space and produces $\\mathrm{d}f|_p(v|_p)=v|_p(f)$ Let’s take a more explicit example, for a bivariate function: $\\mathrm{d}f|_{p}:\\mathrm{d}f|_{p}(v|_p)=v|_p(f)= \\frac{\\partial f}{\\partial x} \\bigg|_{p} a+\\frac{\\partial f}{\\partial y}\\bigg|_{p}b$ , where $v|_p=\\left(a\\frac{\\partial}{\\partial x}+b\\frac{\\partial}{\\partial y}\\right)\\bigg|_p$ ","date":"2024-03-13","objectID":"/understanding_differentials/:3:2","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Mathematical Physics"],"content":"Four Differential as Linear Mapping Characterized by Jacobian Matrix So far, we have discussed functions that map to domains $\\mathbb{R},\\mathbb{C}, \\cdots$. What if the target domain can be $\\mathbb{R}^2, \\mathbb{R}^3,\\cdots$? For example, $\\bm{f}:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ . In this case, $\\bm{f}$ can be seen as $m$ functions: $f_i:\\mathbb{R}^n\\rightarrow\\mathbb{R},,,i=1,\\cdots,m$ Then we have $\\begin{bmatrix}\\mathrm{d}f_1 \\\\ \\mathrm{d}f_2 \\\\ \\vdots \\\\ \\mathrm{d}f_m \\end{bmatrix}= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1}\u0026\\frac{\\partial f_1}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1}\u0026\\frac{\\partial f_2}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1}\u0026\\frac{\\partial f_m}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}\\begin{bmatrix}\\mathrm{d}x_1 \\\\ \\mathrm{d}x_2 \\\\ \\vdots \\\\ \\mathrm{d}x_n \\end{bmatrix}$ where $\\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1}\u0026\\frac{\\partial f_1}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1}\u0026\\frac{\\partial f_2}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1}\u0026\\frac{\\partial f_m}{\\partial x_2}\u0026\\cdots\u0026\\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}$ is the Jacobian matrix. In fact, the differential is the linear mapping characterized by the Jacobian matrix. Below, we explain in detail. For a univariate function, the Jacobian matrix is just the derivative. Although we are used to thinking of derivatives as slopes, let’s change our perspective: If you consider the action of a univariate function as stretching along the number line, then the derivative is the local stretching ratio. The advantage of this perspective is its ease of generalization to multivariate functions. Because, for multivariate functions, you often cannot draw a curve as you would for univariate functions. So, as an alternative method, you can think of a multivariate function as a space being stretched, such as $f: R^2→R^2$. You can imagine that each point in $R^2$ is stretched to a new point by $f$. If you zoom in on this local stretch, it looks like the case of a univariate function—linear—parallel lines are stretched to parallel lines. And what we call the differential is precisely this local linear mapping! The matrix of this linear mapping is the Jacobian matrix. For more detailed explanations, see lectures 71-72 of Khan Academy’s Multivariable Calculus course: https://www.youtube.com/watch?v=bohL918kXQk ","date":"2024-03-13","objectID":"/understanding_differentials/:4:0","tags":["Differential Geometry"],"title":"Understanding differentials in four levels","uri":"/understanding_differentials/"},{"categories":["Quantum Information"],"content":"In the previous section, we discussed the principle of measuring the quantum second-order correlation function with a non-photon-number-resolving single-photon detector in the HBT experiment. https://zhuanlan.zhihu.com/p/679453473 In this section, let’s see what the quantum second-order correlation function is used for. In addition to the common uses such as distinguishing between super-Poissonian statistics/Poissonian statistics/sub-Poissonian statistics and distinguishing between photon bunching/anti-bunching, the HBT experiment can also be used to measure the spectral purity of multimode squeezed states. ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:0:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"Generation of Multimode Squeezed States via SPDC SPDC (Spontaneous Parametric Down-Conversion) is an experimental method used to generate correlated photon pairs. By shining a laser onto a nonlinear crystal, spontaneous parametric down-conversion occurs if the phase matching condition is satisfied. In this process, one pump photon is converted into two photons with equal frequencies, known as the signal photon and the idler photon, respectively, satisfying energy conservation and momentum conservation. The Hamiltonian for this process is given by: $H = i\\hbar g,a^\\dag_s a^\\dag_i a_p + h.c.$ where $g$ represents the interaction strength. When the pump photon is strong enough that its intensity is almost unaffected, we can focus only on the subspace of signal and idler photons. In this case, the Hamiltonian becomes: $H = i\\hbar g \\alpha_p ,a^\\dag_s a^\\dag_i + h.c.$ Without loss of generality, let $\\alpha_p$ be a real number, and let $\\chi=g\\alpha_p$. Then, $H = i\\hbar \\chi (a^\\dag_s a^\\dag_i - a_s a_i)$ In reality, due to the finite spatial extent of the pump, there will be uncertainty in momentum, manifested as a finite linewidth in the spectrum. In this case, the Hamiltonian becomes: $\\begin{aligned} H=i\\hbar \\chi \\int d\\omega_s d\\omega_i [f(\\omega_s,\\omega_i) a_s^\\dag(\\omega_s) a_i^\\dag(\\omega_i) -f^*(\\omega_s,\\omega_i) a_s(\\omega_s) a_i(\\omega_i)] \\end{aligned}$ Now, we expand $f(\\omega_s,\\omega_i)$ using a set of orthogonal basis functions (e.g., Hermite functions) ${\\varphi_k}_{k\\in \\mathbb{N}}$: $\\begin{aligned} f(\\omega_s,\\omega_i)=\\sum_{k,l} C_{kl}\\varphi_k(\\omega_s)\\varphi_l(\\omega_i) \\end{aligned}$ and denote $a_k^\\dag = \\int d\\omega_s ,\\varphi_k(\\omega_s)a_s^\\dag(\\omega_s),\\quad b_k^\\dag = \\int d\\omega_i ,\\varphi_k(\\omega_i) a_i^\\dag(\\omega_i)$ , then we have $\\begin{aligned} H=i\\hbar \\chi \\sum_{k,l}\\left( C_{kl}a^\\dag_k b^\\dag_l - C^*_{kl} a_kb_l\\right) \\end{aligned}$ Performing a Schmidt decomposition (singular value decomposition) on it, we get $\\begin{aligned} H=i\\hbar \\chi \\sum_{k}\\left( \\lambda_k A^\\dag_k B^\\dag_k - \\lambda_k A_k B_k\\right) \\end{aligned}$ where $\\sum_k\\lambda^2_k=1$. Note: Since the spectrum of singular value decomposition is always non-negative real numbers, $\\lambda_k$ is real and greater than zero. This is actually the Hamiltonian of multimode squeezed states. Review of various squeezed states: Single-mode squeezed state: $\\begin{aligned} H=i\\hbar\\left(g^*\\frac{a^2}{2}-g\\frac{a^{\\dag2}}{2}\\right) \\end{aligned}$ Two-mode squeezed state: $\\begin{aligned} H=i\\hbar\\left(g^*a b-g a^\\dag b^\\dag\\right) \\end{aligned}$ Multimode squeezed state: $\\begin{aligned} H=i\\hbar\\sum_{k}\\left(g_k^*a_k b_k-g_k a_k^\\dag b_k^\\dag\\right) \\end{aligned}$ Now let’s separately look at the system evolution in the Schrödinger picture and the Heisenberg picture. In the Schrödinger picture, $\\begin{aligned} |\\Psi_{\\text{out}}\\rangle \u0026=e^{\\frac{H\\tau}{i\\hbar}}|\\Psi_{\\text{in}}\\rangle \\\\ \u0026= \\sum_{k} \\frac{1}{\\cosh(r_k)}\\sum_{n=0}^{\\infty}\\tanh^n(r_k)|n_k,n_k\\rangle \\end{aligned}$ where $r_k=\\chi \\tau \\lambda_k$ . Note that it resembles a thermal light field. When $r_k$ is small, we have $|\\Psi_{\\text{out}}\\rangle\\approx \\sum_{k} \\left(|0_k,0_k\\rangle + r_k|1_k,1_k\\rangle\\right)$ This is also the case that most experiments conform to. Conditioned on detecting photons, the conditional state is $|\\Psi_{\\text{conditional}}\\rangle=\\sum_k\\lambda_k|1_k,1_k\\rangle$ . This is the principle of heralded single-photon sources. In the Heisenberg picture, $\\begin{aligned} A_k \u0026\\rightarrow e^{\\frac{H\\tau}{i\\hbar}} A_k e^{-\\frac{H\\tau}{i\\hbar}} \\\\ \u0026= A_k + [e^{\\frac{H\\tau}{i\\hbar}},A_k]+[e^{\\frac{H\\tau}{i\\hbar}},[e^{\\frac{H\\tau}{i\\hbar}},A_k]] + \\cdots \\\\ \u0026=\\cosh(r_k) A_k - \\sinh(r_k)B_k^\\dag \\end{aligned}$ $\\begin{aligned} B_k \u0026\\rightarrow e^{\\frac{H\\tau}{i\\hbar}} B_k e^{-\\frac{H\\tau}{i\\hbar}} \\\\ \u0026= B_k + [e^{\\frac{H\\tau}{i\\hbar}},B_k]+[e^{\\frac{H\\tau}{i\\hbar}},[e^{\\frac{H\\tau}{i\\hbar}},B_k]] + \\cdots \\\\ \u0026=\\cosh(r_k) B_k - \\sinh(r_k)A_k^\\","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:1:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"How to Characterize Spectral Correlation? Spectral correlation refers to the correlation between the spectra of the signal photon and the idler photon. An uncorrelated joint spectrum can be written as the product of the spectra of the two photons: $f(\\omega_s,\\omega_i)=\\phi(\\omega_s)\\psi(\\omega_i)$ . If it cannot be written in this form, it implies spectral correlation. How to characterize spectral correlation specifically? We can use the Schmidt number to quantify the degree of spectral correlation, defined as follows: $\\begin{aligned} K= \\frac{1}{\\sum_k\\lambda^4_k} \\end{aligned}$ Example For an uncorrelated joint spectrum, ${\\lambda_k}={1}$ , $K=1$ . For a joint spectrum with two modes and equal coefficients, ${\\lambda_k}={\\frac{1}{2},\\frac{1}{2}}$ , $K=2$ . For a joint spectrum with n modes and equal coefficients, $K=n$ . For a joint spectrum with n modes and coefficients not completely equal to each other, $1\u003cK\u003cn$ . Why do we care about spectral correlation? Because it is closely related to the purity of photons: Property When $r_k$ is small and photons are detected, the quantum state is $|\\Psi_{\\text{conditional}}\\rangle=\\sum_k\\lambda_k|1_k,1_k\\rangle$ . For this quantum state, tracing out one of the two photons (performing partial trace operation), the purity of the remaining photon state is $P=\\frac{1}{K}$ . This is because: $\\begin{aligned} P \u0026= \\operatorname{tr}[\\operatorname{tr}_2[|\\psi\\rangle\\langle\\psi|]^2] \\\\ \u0026= \\operatorname{tr}\\left[\\operatorname{tr}_B\\left(\\sum_{k}\\lambda_k \\left| 1_k, 1_k\\right\\rangle\\sum_{l}\\lambda_l \\left\\langle 1_l ,1_l\\right|\\right)^2\\right] \\\\ \u0026= \\operatorname{tr}\\left[\\left(\\sum_{k}\\lambda_k^2 \\left|1_k \\right\\rangle\\left\\langle 1_k\\right|\\right)^2\\right] \\\\ \u0026= \\operatorname{tr}\\left[\\sum_{k}\\left(\\lambda_k^2\\left| 1_k \\right\\rangle\\left\\langle 1_k\\right|\\right)^2\\right] \\\\ \u0026= \\operatorname{tr}\\left[\\sum_{k}\\lambda_k^4 |1_k\\rangle\\langle 1_k|\\right] \\\\ \u0026=\\sum_{k}\\lambda_k^4=\\frac{1}{K} \\end{aligned}$ It can be seen that the purity is inversely proportional to the Schmidt number. That is, the stronger the spectral correlation, the lower the purity of the photons in the frequency domain. This is unfavorable for heralded single-photon sources and various multi-photon interference experiments. For the former, spectral correlation will reduce the frequency-domain indistinguishability of single photons, and for the latter, spectral correlation will reduce the interference contrast. ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:2:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"Measuring Spectral Correlation with HBT Experiment In the previous section, we mentioned that the HBT experiment detects: $\\begin{aligned} \\frac{C_{12}R}{S_1S_2}=\\frac{\\langle n(n-1)\\rangle}{\\langle n\\rangle^2} \\end{aligned}$ where $C_{12}$ is the coincidence count rate of two detectors, $S_1$ and $S_2$ are the count rates of the first and second detectors, respectively, and $R$ is the photon flux. When the frequencies of various modes are the same or similar, $\\begin{aligned} \\frac{\\langle n(n-1)\\rangle}{\\langle n\\rangle^2} \\approx g^{(2)}(0) \\end{aligned}$ . The reason why we say that the HBT experiment can measure spectral correlation is because $\\begin{aligned} \\frac{\\langle n(n-1)\\rangle}{\\langle n\\rangle^2} = 1+\\frac{1}{K} \\end{aligned}$ . The proof is as follows: $\\begin{aligned} \\frac{\\langle n(n-1)\\rangle}{\\langle n\\rangle^2} \u0026=\\frac{\\langle n^2\\rangle-\\langle n\\rangle}{\\langle n\\rangle^2} \\\\ \u0026=\\frac{\\sum_{k} \\langle n_k^2 \\rangle + 2\\sum_{i\u003ej} \\langle n_i n_j \\rangle - \\langle n\\rangle}{\\langle n\\rangle^2} \\\\ \u0026= \\frac{ \\sum_{k} \\left(\\langle n_k \\rangle + 2\\langle n_k\\rangle^2 \\right) + 2\\sum_{i\u003ej} \\langle n_i \\rangle \\langle n_j \\rangle - \\langle n\\rangle}{\\langle n\\rangle^2} \\\\ \u0026= \\frac{\\langle n\\rangle + \\langle n\\rangle^2 +\\sum_{k} \\langle n_k\\rangle^2 - \\langle n\\rangle}{\\langle n\\rangle^2} \\\\ \u0026= 1+\\frac{\\sum_{k} \\langle n_k\\rangle^2}{ \\left\\langle \\sum_{k} n_k\\right\\rangle^2} \\\\ \u0026= 1 + \\frac{\\sum_k \\sinh^4(r_k)}{\\left(\\sum_k \\sinh^2(r_k)\\right)^2} \\\\ \u0026\\approx 1+\\frac{\\sum_k r_k^4}{\\left(\\sum_k r_k^2\\right)^2} \\\\ \u0026= 1 + \\frac{\\sum_k \\lambda_k^4}{\\left(\\sum_k \\lambda_k^2\\right)^2} \\\\ \u0026= 1+\\sum_k \\lambda_k^4 \\\\ \u0026= 1+\\frac{1}{K} \\\\ \u0026= 1+P \\end{aligned}$ where the third equality uses the result: for thermal light fields, $\\langle n_k^2 \\rangle =\\langle n_k \\rangle + 2\\langle n_k\\rangle^2$ , and there is no correlation between different modes $\\langle n_i n_j\\rangle=\\langle n_i\\rangle\\langle n_j\\rangle$ . The sixth equality is because in the Heisenberg picture, $\\begin{aligned} \\langle n_k\\rangle\u0026=\\langle 0|A_k^\\dag A_k|0\\rangle \\\\ \u0026\\rightarrow\\langle 0| (\\cosh(r_k) A_k^\\dag - \\sinh(r_k)B_k)(\\cosh(r_k) A_k - \\sinh(r_k)B_k^\\dag)|0\\rangle \\\\ \u0026= \\sinh^2(r_k) \\end{aligned}$ ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:3:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"Conclusion With the help of the HBT experiment, we can measure spectral correlation and purity, which is quite remarkable. Because ordinary methods require tunable filters to measure JSI (Joint Spectral ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:4:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"Note The earliest literature I could find for the derivation above is [1], but the literature [1] skips too many steps, and the logical chain in between was filled in by myself. The literature on measuring spectral correlation using HBT experiments can be found in [2] and [3], both of which cite literature [1]. ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:5:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"References [1] Christ, A., Laiho, K., Eckstein, A., Cassemiro, K. N. \u0026 Silberhorn, C. Probing multimode squeezing with correlation functions. New J. Phys. 13, 033027 (2011). [2] Faruque, I. I. et al. Estimating the Indistinguishability of Heralded Single Photons Using Second-Order Correlation. Phys. Rev. Applied 12, 054029 (2019). [3] Paesani, S. et al. Near-ideal spontaneous photon sources in silicon quantum photonics. Nat Commun 11, 2505 (2020). ","date":"2024-03-10","objectID":"/hbt_spectral_correlation/:6:0","tags":["Quantum Optics"],"title":"Probing the spectral purity with unheralded g2 measurements in HBT experiment","uri":"/hbt_spectral_correlation/"},{"categories":["Quantum Information"],"content":"HBT Experiment Those who have conducted quantum optics experiments must be familiar with the Hanbury Brown and Twiss (HBT) experiment, which can be used to measure the second-order correlation function g2. In this experiment, a beam of light is split into two using a 50:50 beamsplitter, and then each beam is detected separately by two detectors. The correlation of the intensities on both sides is measured as a function of delay, as shown in the figure below: Hanbury Brown and Twiss Experiment The classical normalized g2 is defined as: $\\begin{aligned} g^{(2)}(r_1, r_2; \\tau) \u0026= \\frac{\\langle E^*(r_1, t) E^*(r_2, t + \\tau) E(r_2, t + \\tau) E(r_1, t) \\rangle}{\\langle |E(r_1, t)|^2 \\rangle \\langle |E(r_2, t + \\tau)|^2 \\rangle} \\\\ \u0026= \\frac{\\langle I(r_1, t) I(r_2, t + \\tau) \\rangle}{\\langle I(r_1, t) \\rangle \\langle I(r_2, t + \\tau) \\rangle} \\end{aligned}$ where $E$ represents the electric field and $I$ represents the intensity. As per the definition, the HBT experiment measures the classical g2 by normalizing the correlated intensities with the product of the individual intensities. ","date":"2024-03-10","objectID":"/hbt_g2/:1:0","tags":["Quantum Optics"],"title":"Probing g2 with non-photon-number-resolving detectors","uri":"/hbt_g2/"},{"categories":["Quantum Information"],"content":"Quantum Second-Order Correlation Function The definition of the quantum normalized g2 is: $\\begin{aligned} g^{(2)}(r_1, r_2; \\tau) \u0026= \\frac{\\langle E^-(r_1, t) E^-(r_2, t + \\tau) E^+(r_2, t + \\tau) E^+(r_1, t) \\rangle}{\\langle E^-(r_1, t) E^+(r_1, t) \\rangle \\langle E^-(r_2, t + \\tau) E^+(r_2, t + \\tau) \\rangle} \\end{aligned}$ In the quantum case, the operators in the numerator should be replaced with the counter-rotating term $E^-$ and the co-rotating term $E^+$, which involve creation and annihilation operators. Additionally, the creation operator must precede the annihilation operator since the detection of a photon is an annihilation process, unlike the classical case. In classical coherent states $|\\alpha\\rangle$, the annihilation operator $a$ is an eigenstate $a|\\alpha\\rangle = \\alpha|\\alpha\\rangle$, hence the presence or absence of a photon makes no difference. In the case of single-mode, since $E^+ = i\\sqrt{\\frac{\\hbar \\omega}{2\\epsilon_0 V}}a$ and $E^- = i\\sqrt{\\frac{\\hbar \\omega}{2\\epsilon_0 V}}a^\\dagger$, the quantum g2 can be simplified to: $\\begin{aligned} g^{(2)}(r_1, r_2, \\tau) \u0026= \\frac{\\langle a^\\dagger(r_1, t) a^\\dagger(r_2, t + \\tau) a(r_2, t + \\tau) a(r_1, t) \\rangle}{\\langle n(r_1, t) \\rangle \\langle n(r_2, t + \\tau) \\rangle} \\end{aligned}$ Therefore, to measure quantum g2, we need to replace intensity detectors with photon detectors, and these photon detectors should be capable of distinguishing photon numbers. However, the problem arises because currently available single-photon detectors that can distinguish photon numbers are not very practical: TES detectors have long recovery times; schemes for Demultiplexing in space and time only offer pseudo-photon number resolution; and SNSPDs are expensive, require low temperatures, and their multi-photon efficiency may not be very high. ","date":"2024-03-10","objectID":"/hbt_g2/:2:0","tags":["Quantum Optics"],"title":"Probing g2 with non-photon-number-resolving detectors","uri":"/hbt_g2/"},{"categories":["Quantum Information"],"content":"NPNR-SPD for Measuring g2 Fortunately, single-photon detectors that cannot distinguish photon numbers (NPNR-SPDs, Non-Photon-Number-Resolving Single-Photon-Detectors) can still measure g2, even though they cannot resolve photon numbers! Comment: “Cannot distinguish photon numbers” means they can only differentiate between “no photons” and “one or more photons.” Common APD detectors cannot distinguish photon numbers. The method is simple: just make sure the quantum efficiency of the NPNR-SPD is low enough! Quantum efficiency $\\eta$ refers to the probability that the detector can detect a photon given that a photon enters it. Conversely, the probability of the photon being “lost” is $1 - \\eta$. At first, it seemed strange to me why lower efficiency is better. But with a little thought, the principle becomes clear: When the efficiency is low enough, the probability that $k$ photons are not all lost is $1 - (1 - \\eta)^k \\approx k\\eta$. In other words, the probability of the NPNR-SPD detecting a photon is proportional to $k$. Thus, we achieve “photon number resolution” indirectly through “low efficiency”. The following is a formal (boring) mathematical derivation, which can be skipped: [Derivation equations] Measuring g2 has several uses. Besides the common textbook distinctions of [super-Poissonian statistics/Poissonian statistics/sub-Poissonian statistics] and [photon bunching/antibunching], there is an unexpected application: measuring spectral correlations of photons, which we will discuss in the next part. ","date":"2024-03-10","objectID":"/hbt_g2/:3:0","tags":["Quantum Optics"],"title":"Probing g2 with non-photon-number-resolving detectors","uri":"/hbt_g2/"},{"categories":["Quantum Information"],"content":"Conclusion: Multi-Mode Scenario Previously, we assumed that the detected light was single-mode. Hence $E^+$ could be proportional to $a$: $E^+ = i\\sqrt{\\frac{\\hbar \\omega}{2\\epsilon_0 V}}a$. But what if the detected light is multi-mode? In this case, $E^+ = i\\sum_k\\sqrt{\\frac{\\hbar \\omega_k}{2\\epsilon_0 V}}a_k$, $E^+$ is no longer proportional to $a$, but has a frequency dependence: $E^+_k \\propto \\sqrt{\\omega_k}a_k$. The additional term $\\sqrt{\\omega_k}$ in the equation is not surprising since the intensity, which is proportional to the frequency times the number of photons, contributes to it. However, as long as the frequencies of the various modes are the same, we can still eliminate $\\sqrt{\\omega_k}$, and obtain: $\\begin{aligned} g^{(2)}(0) = \\frac{\\langle n(n-1) \\rangle}{\\langle n \\rangle^2} \\end{aligned}$ Thus, even if multiple modes are present, if these modes have (approximately) the same frequency, the method of measuring quantum g2 using the HBT experiment remains effective. ","date":"2024-03-10","objectID":"/hbt_g2/:4:0","tags":["Quantum Optics"],"title":"Probing g2 with non-photon-number-resolving detectors","uri":"/hbt_g2/"},{"categories":["Quantum Information"],"content":"Projection Measurement Traditionally, a measurement, in the sense of Von Neumann, is a series of projection operators. By performing spectral decomposition on the self-adjoint operator corresponding to the observable, denoted as $ O = \\sum_i \\lambda_i |\\varphi_i\\rangle\\langle\\varphi_i| $, we obtain these projection operators $ |\\varphi_i\\rangle\\langle\\varphi_i| $. This part is well-known to students who have studied elementary quantum mechanics. Besides the Von Neumann measurement, there is a more general type of measurement called Generalized Measurements or Positive Operator Valued Measures (POVMs). Generalized Measurements (POVMs) Definition: POVM A POVM is a mapping $ \\mathsf{E}: X \\rightarrow \\mathcal{L}(\\mathcal{H}) $, satisfying $ \\mathsf{E}(x) \\ge 0, \\quad \\forall x \\in X $ $ \\sum_ {x \\in X} \\mathsf{E}(x) = \\mathbb{I}_{\\mathcal{H}} $ where $ X $ represents the set of possible measurement outcomes, and $ \\mathcal{L}(\\mathcal{H}) $ represents the set of all bounded operators on $ \\mathcal{H} $. Born’s Rule The probability of obtaining result $ x $ when measuring a quantum state $ \\rho $ with a POVM $ \\mathsf{E} $ is given by $ p_{\\rho}^{\\mathsf{E}}(x) = \\operatorname{tr}[\\rho \\mathsf{E}(x)] $. When I first encountered POVMs, I was not clear about how they are implemented. This is because traditional measurements involve projection operators satisfying $ \\mathsf{E}(x)^2 = \\mathsf{E}(x), \\quad \\forall x \\in X $. However, general POVMs do not satisfy this condition. In fact, traditional projection measurements, also known as PVMs (Projection Valued Measures), are a special type of POVM. PVMs are to POVMs what pure states are to mixed states. In other words, a POVM is a statistical mixture of PVMs, just as a mixed state is a statistical mixture of pure states. Implementing POVMs So, how do we implement POVMs? We can actually use PVMs and composite systems to implement POVMs. Consider a system with a Hilbert space $ \\mathcal{H} $ and a state $ \\rho $. Next, we couple system $ \\mathcal{H} $ with another system $ \\mathcal{K} $. Suppose the initial state of system $ \\mathcal{K} $ is $ \\sigma $. Then, the initial state of the composite system is $ \\rho \\otimes \\sigma $ in $ \\mathcal{H} \\otimes \\mathcal{K} $. After that, we let the composite system evolve for a period of time, resulting in the system’s state becoming $ U(\\rho \\otimes \\sigma) U^\\dag $. Finally, we perform a PVM $ \\mathsf{Z} $ measurement on subsystem $ \\mathcal{K} $. According to Born’s rule, the probability of obtaining result $ x $ is $ \\operatorname{tr}[U(\\rho\\otimes \\sigma) U^\\dag(\\mathbb{I}_{\\mathcal{H}}\\otimes \\mathsf{Z}(x))] $. Thus, we have implemented a POVM $ \\mathsf{E} $ such that $ \\operatorname{tr}[\\rho \\mathsf{E}(x)] = \\operatorname{tr}[U(\\rho\\otimes \\sigma) U^\\dag(\\mathbb{I}_{\\mathcal{H}}\\otimes \\mathsf{Z}(x))] $. Measurement Model Let’s summarize the physical objects involved in implementing POVMs: the ancilla system $ \\mathcal{K} $, the initial state $ \\sigma $ of system $ \\mathcal{K} $, the evolution $ U $ of the composite system, and the projection measurement $ \\mathsf{Z} $ on the ancilla system. These physical objects together realize a POVM measurement. So, we can define $ \\mathfrak{M} = (\\mathcal{K},\\sigma,U,\\mathsf{Z}) $ as a Measurement Model. In actual experiments, the ancilla system $ \\mathcal{K} $ can be considered as a probe of the instrument, and $ \\mathsf{Z} $ is the readout of the probe. Post-Measurement State POVMs themselves cannot determine the post-measurement state. The actual determination of the post-measurement state depends on the specific implementation of the measurement, that is, the measurement model. For a measurement model $ \\mathfrak{M} = (\\mathcal{K},\\sigma,U,\\mathsf{Z}) $, the post-measurement state of system $ \\mathcal{H} $ is $ \\rho_x = \\operatorname{tr}_{\\mathcal{K}}[U(\\rho\\otimes \\sigma) U^\\dag(\\mathbb{I}_{\\mathcal{H}}\\otimes \\mathsf{Z}(x))] $, where $ \\operatorname{tr}_{\\mathcal{K}}[\\cdot] $ denotes the partial","date":"2024-03-10","objectID":"/povm/:0:0","tags":["Quantum Measurement Theory"],"title":"POVM: a brief introduction","uri":"/povm/"},{"categories":["Quantum Information"],"content":"The wave function of a photon in the spacetime representation is: $\\Psi(\\mathbf{r},t)=\\langle \\mathbf{r},t|\\psi\\rangle=\\langle 0 |E^{+}(\\mathbf{r},t)|\\psi\\rangle$ Where $\\begin{aligned} |\\mathbf{r},t\\rangle = E^{-}(\\mathbf{r},t) |0\\rangle = \\sum_{\\mathbf{k},\\lambda} \\sqrt{\\frac{\\hbar \\omega}{2 \\epsilon_0 V}} e^{\\mathrm{i}(\\mathbf{k}\\cdot \\mathbf{r}-\\omega_{\\mathbf{k}} t)} a^\\dag_{\\mathbf{k},\\lambda} |0\\rangle \\end{aligned}$. Intuitively, this is to let the field operator $E^{-}(\\mathbf{r},t)$ create a state $|\\mathbf{r},t\\rangle$ at the spacetime point $(\\mathbf{r},t)$, and then calculate the overlap between this state and $|\\psi\\rangle$. When we talk about the spacetime modes of photons, such as Gaussian pulses, hyperbolic secant pulses, etc., we are actually referring to the wave function in the spacetime representation described above. Of course, this wave function is not in the “Schrödinger sense” because the Schrödinger equation belongs to non-relativistic quantum mechanics, while photons are relativistic particles. When we say that photons do not have a well-defined wave function, we are actually saying that they do not have a well-defined position operator. But we do have field operators $E^{\\pm}(\\mathbf{r},t)$, so we don’t need a position operator either. The tern “photon wave function” is nowadays common in literature. Few examples: [1] Xu, Y., Choudhary, S. \u0026 Boyd, R. W. Efficient Measurement of the Bi-photon Spatial Mode Entanglement with Stimulated Emission Tomography. Preprint at http://arxiv.org/abs/2403.05036 (2024). [2] Tian, Z., Liu, Q., Tian, Y. \u0026 Gu, Y. Wavepacket interference of two photons: from temporal entanglement to wavepacket shaping. Preprint at http://arxiv.org/abs/2403.04432 (2024). [3] Moura, A. G. da C. \u0026 Monken, C. H. Einstein-Podolsky-Rosen correlations in spontaneous parametric down-conversion: Beyond the Gaussian approximation. Preprint at http://arxiv.org/abs/2403.04561 (2024). ","date":"2024-03-10","objectID":"/photon_wave_func/:0:0","tags":["Quantum Optics"],"title":"Wave function of photons","uri":"/photon_wave_func/"},{"categories":["Mathematical Physics"],"content":"Chinese version here Baker-Campbell-Hausdorff Formula can be used to compute operator evolution in the Heisenberg picture: $e^X Y e^{-X}=Y+[X,Y]+\\frac{1}{2!}[X,[X,Y]]+\\frac{1}{3!}[X,[X,[X,Y]]]+\\cdots$ This formula is actually just a younger sibling of the BCH formula. Because the evolution rule of operators in the Heisenberg picture is $A\\rightarrow UAU^{\\dag}$, where $U$ is a unitary evolution operator. If $U$ is generated by $H$, then it becomes $A\\rightarrow e^{\\frac{t}{i\\hbar}H}Ae^{-\\frac{t}{i\\hbar}H}$. Example 1: Phase Shifter The Hamiltonian is $H=\\varphi n$, and the annihilation operator $a$ evolves as: $\\begin{aligned} e^{-i\\varphi n} a e^{i\\varphi n}\u0026= a + i\\varphi [n, a] - \\frac{\\varphi}{2!} [n,[n,a]] - \\cdots \\\\ \u0026= a (1+i\\varphi -\\frac{\\varphi^2}{2!} - \\cdots)\\\\ \u0026= e^{i\\varphi} a \\end{aligned}$ Example 2: Beam Splitter The Hamiltonian is $H= \\theta e^{i\\varphi} a^{\\dag}b + \\theta e^{-i\\varphi} a b^\\dag$, the evolution is as follows: $\\begin{aligned} e^{-iH} a e^{iH} \u0026= \\cos \\theta ,a + i \\sin\\theta, e^{i\\varphi} b \\\\ e^{-iH} b e^{iH} \u0026= i \\sin\\theta ,e^{-i\\varphi} a + \\cos \\theta ,b \\end{aligned}$ This is the familiar Bogoliubov Transformation. These two examples seem like using a cannon to kill a mosquito. Let’s move on to something more useful: Example 3: Two-mode Squeezed State The Hamiltonian is: $\\begin{aligned} H=i\\hbar\\left(g^*a b-g a^\\dag b^\\dag\\right) \\end{aligned}$ $\\begin{aligned} a \u0026\\rightarrow e^{\\frac{H}{i\\hbar}} a e^{-\\frac{H}{i\\hbar}} \\\\ \u0026= a + \\left[{\\frac{H}{i\\hbar}},a\\right]+\\left[{\\frac{H}{i\\hbar}},\\left[{\\frac{H}{i\\hbar}},a\\right]\\right] + \\cdots \\\\ \u0026=\\cosh(r) a - \\sinh(r)e^{i\\xi}b^\\dag \\end{aligned}$ Where $g = re^{i\\xi}$. This is well-known for SPDC and OPO. It’s difficult to compute in the Schrödinger picture, but it’s much easier using the Heisenberg picture with BCH formula. Example 4: Rotated Quadratures The generator is the total particle number $N$, canonical position operator $Q$, and canonical momentum operator $P$ evolve as: $\\begin{aligned} e^{i\\theta N} Q e^{-i\\theta N} = \\cos \\theta, Q + i \\sin \\theta, P \\\\ e^{i\\theta N} P e^{-i\\theta N} = i \\sin \\theta, Q + \\cos \\theta P \\end{aligned}$ It looks like rotating the phase space, similar to the Phase Shifter. This transformation is useful for handling Squeezed states. Even a subordinate of the BCH formula is so useful, isn’t the original formula even more powerful? Now, let’s introduce the original: $e^{X}e^{Y}=e^Z$ $Z=X+Y+{\\frac {1}{2}}[X,Y]+{\\frac {1}{12}}[X,[X,Y]]-{\\frac {1}{12}}[Y,[X,Y]]+\\cdots $ Uh, it seems a bit less practical. In reality, we often use a special case, where $[X,Y]$ is a constant, in which case the BCH formula becomes $Z=X+Y+{\\frac {1}{2}}[X,Y]$ Example 5: Displacement Operator $\\begin{aligned} D(\\alpha)\u0026=e^{\\alpha a^\\dag-\\alpha^* a} \\\\ \u0026=e^{-\\frac{1}{2}|\\alpha|^2}e^{\\alpha a^\\dag}e^{-\\alpha^* a} \\\\ \u0026=e^{\\frac{1}{2}|\\alpha|^2}e^{-\\alpha^* a}e^{\\alpha a^\\dag} \\end{aligned}$ It’s evident that the BCH formula allows us to switch flexibly between normal ordering (creation operators before annihilation operators) and anti-normal ordering. In fact, the displacement operator itself is neither normally ordered nor anti-normally ordered but symmetrically ordered. Example 6: Weyl Operator $W(q,p) = e^{i(-qP+pQ)} = e^{\\frac{i}{2}qp}U(q)V(p) = e^{-\\frac{i}{2}qp}V(p)U(q)$ Where $U(q)=e^{-iqP}$ is the position translation operator, $V(p)=e^{ipQ}$ is the momentum translation operator. According to the BCH formula, we can decouple them. This $W:\\mathbb{R}^2\\rightarrow \\mathcal{L}(L^2(\\mathbb{R}))$ is called the Weyl Representation. More generally, the Weyl Transformation can quantize a probability density function in classical phase space into a density operator, conversely, the Wigner Transformation can map a density operator to a probability density function in phase space, which is called the Wigner Representation in quantum optics. Example 7: Glauber P-, Husimi Q- and Wigner Representation The Wigner representation of a dens","date":"2024-03-10","objectID":"/bch/:0:0","tags":["Quantum Optics"],"title":"Baker-Campbell-Hausdorff Formula","uri":"/bch/"},{"categories":["Quantum Information"],"content":"In classical mechanics, complex numbers are merely a mathematical tool used to simplify calculations. In quantum mechanics, complex numbers are not just a mathematical trick, but have a certain physical significance. Consider the classical vector potential: $\\begin{aligned} \\mathbf{A}(\\mathbf{r},t)=\\sum_{\\mathbf{k}\\lambda} \\left( A_{\\mathbf{k}\\lambda}e^{i(\\mathbf{k}\\cdot\\mathbf{r}-\\omega_{\\mathbf{k}}t)} + \\text{c.c.}\\right)\\mathbf{e}_{\\mathbf{k}\\lambda} \\end{aligned}$ where $\\mathbf{k}$ and $\\lambda$ represent the spatial and polarization modes respectively Quantizing it yields the vector potential operator in the Heisenberg picture: $\\begin{aligned} \\mathbf{A}(\\mathbf{r},t)=\\sum_{\\mathbf{k}\\lambda} \\left( C_{\\mathbf{k}\\lambda}\\hat{a}_{\\mathbf{k}\\lambda}e^{i(\\mathbf{k}\\cdot\\mathbf{r}-\\omega_{\\mathbf{k}}t)} + C_{\\mathbf{k}\\lambda}^{*}\\hat{a}^{\\dag}_{\\mathbf{k}\\lambda} e^{i(\\mathbf{k}\\cdot\\mathbf{r}+\\omega_{\\mathbf{k}}t)}\\right)\\mathbf{e}_{\\mathbf{k}\\lambda} \\end{aligned}$ It can be seen that the part rotating clockwise corresponds to the annihilation operator $\\hat{a}_{\\mathbf{k}\\lambda}$, while the counterclockwise rotating part corresponds to the creation operator $\\hat{a}_{\\mathbf{k}\\lambda}^{\\dag}$. Since these two operators do not commute ( $[\\hat{a}_{\\mathbf{k}\\lambda},\\hat{a}_{\\mathbf{k}\\lambda}^{\\dag}]=1$ ), the vacuum is no longer “vacuum”, but filled with fluctuations of the electromagnetic field. Specifically, the Hamiltonian of the electromagnetic field is: $\\begin{aligned} \\hat{H}\u0026=\\hbar\\sum_{\\mathbf{k}\\lambda}\\frac{\\omega_\\mathbf{k}}{2}\\left\\{\\hat{a}_{\\mathbf{k}\\lambda},\\hat{a}_{\\mathbf{k}\\lambda}^{\\dag}\\right\\} \\\\ \u0026= \\hbar \\sum_{\\mathbf{k}\\lambda}\\frac{\\omega_{\\mathbf{k}}}{2}\\left(\\hat{a}_{\\mathbf{k}\\lambda}^{\\dag}\\hat{a}_{\\mathbf{k}\\lambda}+\\hat{a}_{\\mathbf{k}\\lambda}\\hat{a}_{\\mathbf{k}\\lambda}^{\\dag}\\right) \\\\ \u0026= \\hbar \\sum_{\\mathbf{k}\\lambda}\\frac{\\omega_{\\mathbf{k}}}{2}\\left(\\hat{a}_{\\mathbf{k}\\lambda}^{\\dag}\\hat{a}_{\\mathbf{k}\\lambda}+\\hat{a}_{\\mathbf{k}\\lambda}^{\\dag}\\hat{a}_{\\mathbf{k}\\lambda} + [\\hat{a}_{\\mathbf{k}\\lambda},\\hat{a}_{\\mathbf{k}\\lambda}^{\\dag}]\\right) \\\\ \u0026=\\hbar \\sum_{\\mathbf{k}\\lambda}\\omega_{\\mathbf{k}}\\left(\\hat{a}_{\\mathbf{k}\\lambda}^{\\dag}\\hat{a}_{\\mathbf{k}\\lambda} + \\frac{1}{2}\\right) \\\\ \u0026= \\hbar \\sum_{\\mathbf{k}\\lambda} \\omega_{\\mathbf{k}}\\left(\\hat{n}_{\\mathbf{k}\\lambda}+\\frac{1}{2}\\right) \\end{aligned}$ When the excitation numbers of all modes are zero, which is the vacuum state, the energy of the electromagnetic field is not zero. This is caused by vacuum fluctuations. ","date":"2024-02-16","objectID":"/complex_numbers_in_em/:0:0","tags":["Quantum Optics"],"title":"What is the significance of complex numbers in describing EM waves?","uri":"/complex_numbers_in_em/"},{"categories":["Mathematical Physics"],"content":"I. Differences and Similarities in Properties Lie derivative $\\mathcal{L}_V$ and covariant derivative $\\nabla_V$ share many common points: Both $\\mathcal{L}_V$ and $\\nabla_V$ preserve the type of tensors, mapping $\\mathcal{T}^p_q(M)$ to $\\mathcal{T}^p_q(M)$. $\\mathcal{T}^p_q(M)$ represents the set of all smooth tensor fields of type (p, q) on $M$. Particularly, for (0,0) type tensor fields, i.e., scalar fields $f\\in \\mathcal{F}(M)$, we have $\\mathcal{L}_V f=\\nabla_V f=Vf$. Both satisfy linearity and the Leibniz rule: $ \\begin{aligned} \\mathcal{L}_V(\\mu A + \\lambda B) \u0026= \\mu \\mathcal{L}_V A + \\lambda \\mathcal{L}_V B, \\\\ \\mathcal{L}_V (A \\otimes B) \u0026= (\\mathcal{L}_V A)\\otimes B + A \\otimes (\\mathcal{L}_V B) \\end{aligned} $ $\\begin{aligned} \\nabla_V(\\lambda A+\\mu B) \u0026= \\lambda \\nabla_V A + \\mu \\nabla_V B \\\\ \\nabla_V(A\\otimes B) \u0026= (\\nabla_V A)\\otimes B + A \\otimes (\\nabla_V B) \\end{aligned}$ Both commute with contraction $ \\mathcal{L}_V \\circ C = C \\circ \\mathcal{L}_V $ $ \\nabla_W \\circ C = C \\circ \\nabla_W $ So, what are their differences? The most important difference is as follows: 4. Covariant derivative has $\\mathcal{F}$-linearity in $V$, while Lie derivative only has ordinary linearity: $ \\nabla_{fV+gW} = f\\nabla_V + g\\nabla_W \\quad f,g\\in\\mathcal{F}(M) $ $ \\mathcal{L}_{\\lambda V+\\mu W} = \\lambda \\mathcal{L}_V + \\mu \\mathcal{L}_W \\quad \\lambda,\\mu\\in\\mathbb{R}$ where $\\mathcal{F}(M)$ represents the set of all scalar fields on $M$. This seems interesting, but what does it mean? With this question in mind, let’s look at the next section. ","date":"2024-01-26","objectID":"/lie_derivative/:1:0","tags":["Differential Geometry"],"title":"What is the relationship between Lie derivative and covariant derivative?","uri":"/lie_derivative/"},{"categories":["Mathematical Physics"],"content":"II. Significance of Lie Derivative and Covariant Derivative As we all know, a derivative is taking the difference of objects at two points, then taking the limit as these two points approach each other. However, on a manifold, we encounter a problem: the tangent spaces/cotangent spaces at different points are completely different spaces, that is, tensors at different points are not in the same space, so we cannot compare tensors at different points (cannot take the difference). Therefore, to compare tensors at different points, we must somehow move the tensor at one point to the other point. There are two ways to perform such a movement, the first is “flow” and the second is “parallel transport”. The former corresponds to the Lie derivative, and the latter to the covariant derivative. ","date":"2024-01-26","objectID":"/lie_derivative/:2:0","tags":["Differential Geometry"],"title":"What is the relationship between Lie derivative and covariant derivative?","uri":"/lie_derivative/"},{"categories":["Mathematical Physics"],"content":"2.1 Flow—Lie Derivative Prerequisite Knowledge: Pull Back and Push Forward Suppose we have a diffeomorphism: $\\phi:M\\rightarrow N$. Differential geometry tells us that we can pull back tensor fields on $N$ to $M$. Conversely, we can also push forward tensor fields on $M$ to $N$. An integral curve family (e.g., electric field lines) of a smooth vector field $V$ on $M$ can give a diffeomorphism $\\Phi_t:M\\rightarrow M, \\quad \\gamma(t_0)\\mapsto \\gamma(t_0+t)$ This diffeomorphism can be parameterized by a real number $t$. Intuitively, every point on the manifold “flows” a certain distance downstream along the integral curve it is on. Using the pullback map, we can induce the following map: $\\Phi^*_t: \\mathcal{T}^p_q(M) \\rightarrow \\mathcal{T}^p_q(M)$ This provides a method of “moving a tensor at one point to another point”: First, let point $P$ “flow” a certain distance downstream along the integral curve to point $Q$, i.e., $\\Phi_t (P) =Q$, Then pull back the tensor at point $Q$ to point $P$ through $\\Phi^*_t$. This gives a method of moving the tensor at point $Q$ to point $P$. Thus, we can define the Lie derivative as follows: $ \\begin{aligned} \\mathcal{L}_V A \u0026:= \\left.\\frac{\\mathrm{d}}{\\mathrm{d}t}\\right|_{t=0} (\\Phi^*_t A) \\\\ \u0026= \\lim_{t\\rightarrow 0}\\frac{\\Phi^*_\\epsilon A - A}{\\epsilon} \\end{aligned} $ The idea is: first let a point flow a small distance downstream, then pull the tensor at that point back to the original place and compare it with the original tensor at the point. If the results are the same, the derivative is zero; otherwise, the derivative is non-zero. Note: Actually writing $\\Phi^*_t$ as $(\\Phi^{V}_t)^*$ would be more explicit, indicating that $\\Phi^*_t$ is about $V$, but this looks too clunky. ","date":"2024-01-26","objectID":"/lie_derivative/:2:1","tags":["Differential Geometry"],"title":"What is the relationship between Lie derivative and covariant derivative?","uri":"/lie_derivative/"},{"categories":["Mathematical Physics"],"content":"2.2 Parallel Transport—Covariant Derivative The method of “moving a tensor at one point to another point” by the Lie derivative always feels somewhat awkward. This is because, in order to move a vector at point $Q$ to point $P$ through a certain path, we need to find a vector field such that it has an integral curve coinciding with the path. This is not always convenient. More fatally, even if two different vector fields have a completely coinciding integral curve—even so, the results of the movement may not be the same. That is, $(\\Phi^{V}_{Q\\rightarrow P})^*$ and $(\\Phi^{W}_{Q\\rightarrow P})^*$ are generally different, even if the paths are exactly the same. In other words, the Lie derivative at a certain point about vector field $V$ depends on the properties of $V$ in the neighborhood of that point. This is why we say “the covariant derivative has $\\mathcal{F}$-linearity in $V$, while the Lie derivative does not”. Therefore, we prefer a method that allows us to arbitrarily move tensors directly without needing to find an integral curve first, as in the Lie derivative. In this method, we are the rule makers—meaning the rules of tensor movement are determined by us and do not require any local structure of the vector field. Given a local coordinate system, such an artificial rule can be characterized by a set of data called Christoffel symbols $\\Gamma^i_{jk}$: $\\nabla_{k} e_j =: \\Gamma^i_{jk}e_i, \\quad \\nabla_i := \\nabla_{e_i}$ The three subscripts of $\\Gamma^i_{jk}$ mean: $k$ represents the direction along the $k$-th basis vector, $j$ indicates the basis vector we are considering, and $i$ represents the component of the change along the $i$-th basis vector. However, such a set of data is too arbitrary: for an n-dimensional manifold, we need $n^3$ numbers to determine the Christoffel symbols. To be specific, we can use an additional structure on the manifold—metric. We hope the covariant derivative is metric-preserving, i.e., satisfies $ \\nabla_k g_{ij}=0 $, which can give $n^2(n+1)/2$ constraints, leaving $n^3-n^2(n+1)/2=n^2(n-1)/2$ degrees of freedom. We can also obtain the remaining $n^2(n-1)/2$ constraints by requiring the covariant derivative to be torsion-free, thus eliminating arbitrariness and obtaining a unique connection. This is the Riemann / Levi-Civita connection. Thus, we can use the fact that the covariant derivative is zero to define “parallel transport”. Since the rules of parallel transport are artificially specified, the covariant derivative does not depend on the neighborhood of the vector field, so the covariant derivative has $\\mathcal{F}$-linearity in $V$. ","date":"2024-01-26","objectID":"/lie_derivative/:2:2","tags":["Differential Geometry"],"title":"What is the relationship between Lie derivative and covariant derivative?","uri":"/lie_derivative/"},{"categories":["Mathematical Physics"],"content":"2.3 Back to Lie Derivative Of course, this does not mean that the Lie derivative is useless. The significance of the Lie derivative lies not in moving tensors but in characterizing the properties of “flows”. For example, the divergence can be defined by the action of the Lie derivative on the unit volume form: $\\begin{aligned} \\mathcal{L}_V \\omega_g =: (\\operatorname{div} V)\\omega_g \\end{aligned}$ The geometric meaning is also very clear: if we let a small volume on the manifold flow a small distance along the integral curve of the vector field, the scaling ratio of this small volume is the divergence of the vector field at that point. The Lie derivative provides a more elegant language for defining many concepts. If the divergence is not defined by the Lie derivative, it can only be defined as $\\operatorname{div} V = \\star_g \\mathrm{d} \\star_g \\flat_g V$, involving the raising and lowering operators, Hodge star operator, exterior differential operator, which at first glance looks like an indescribable object. Moreover, torsion, curvature, etc., can all be given beautiful (and coordinate-independent) definitions using the Lie derivative, as mentioned in the existing answers and won’t be repeated here. Additionally, as the name suggests, the Lie derivative has deep connections with Lie groups and Lie algebras. The Lie derivative can be used to prove that the Lie bracket of left-invariant vector fields is also a left-invariant vector field, thus inducing a Lie bracket on the tangent space, making the tangent space a Lie algebra. ","date":"2024-01-26","objectID":"/lie_derivative/:2:3","tags":["Differential Geometry"],"title":"What is the relationship between Lie derivative and covariant derivative?","uri":"/lie_derivative/"},{"categories":["Mathematical Physics"],"content":"III. Explicit Formulas Finally, let’s compare the explicit formulas of Lie derivative and covariant derivative: Lie derivative: $ \\begin{aligned} (\\mathcal{L}_V A)^{i…j}_{k…l} \u0026= V^m A^{i…j}_{k…l,m} + \\\\ \u0026+ V^m_{,k} A^{i…j}_{m…l} + \\cdots + V^m_{,l} A^{i…j}_{k…m} \\\\ \u0026- V^i_{,m}A^{m…j}_{k…l} + \\cdots - V^j_{,m} A^{i…m}_{k…l} \\end{aligned} $ Covariant derivative: $ \\begin{aligned} (\\nabla_V A)^{i…j}_{k…l} \u0026 = V^{m} A^{i…j}_{k…l,m} \\\\ \u0026 - \\Gamma^{n}_{km} V^{m} A^{i…j}_{n…l} - \\cdots - \\Gamma^{n}_{lm} V^m A^{i…j}_{k…n} \\\\ \u0026 + \\Gamma^{i}_{nm} V^{m} A^{n…j}_{k…l} + \\cdots + \\Gamma^{j}_{nm} V^m A^{i…n}_{k…l} \\end{aligned} $ In the formula, a comma with a subscript indicates partial differentiation, i.e., $A_{,k}:=\\frac{\\partial}{\\partial k}A$. From the formulas, it can be seen that the Lie derivative $\\mathcal{L}_V$ at a point depends on the properties of $V$ in the neighborhood of that point because it involves the partial derivatives of the components of the vector field $V^m_{,k}$. In the covariant derivative, only the components of the vector field $V^m$ appear, not the partial derivatives of the components $V^m_{,k}$. This means that the covariant derivative does not depend on the neighborhood. This is why the covariant derivative has $\\mathcal{F}$-linearity, while the Lie derivative does not. ","date":"2024-01-26","objectID":"/lie_derivative/:3:0","tags":["Differential Geometry"],"title":"What is the relationship between Lie derivative and covariant derivative?","uri":"/lie_derivative/"},{"categories":["Mathematical Methods in Physics"],"content":"Link: [https://www.zhihu.com/question/640869828/answer/3373753127] Here is a preliminary exploration. ","date":"2024-01-26","objectID":"/parseval/:0:0","tags":["Symmetry"],"title":"Since Parseval's Theorem for the Fourier Transform Represents a Conservation Law, What Symmetry Corresponds to It?","uri":"/parseval/"},{"categories":["Mathematical Methods in Physics"],"content":"Fourier-Plancherel Operator Define the Fourier-Plancherel operator $\\mathcal{F} \\in \\mathcal{L}(L^2(\\mathbb{R}))$ as: $(\\mathcal{F}\\varphi)(p)=\\frac{1}{\\sqrt{2\\pi}}\\int_{\\mathbb{R}}\\mathrm{e}^{-ipx}\\varphi(x)\\mathrm{d}x$ It is actually a unitary operator: $\\mathcal{F}^\\dag\\mathcal{F}=\\mathbb{I}_{L^2(\\mathbb{R})}$ And Parseval’s theorem is merely a restatement of this fact: $\\langle\\varphi \\mathcal{F}^\\dag|\\mathcal{F\\varphi}\\rangle=\\langle\\varphi|\\mathcal{F}^\\dag\\mathcal{F}|\\varphi\\rangle=\\langle\\varphi|\\varphi\\rangle=1$ Does this theorem involve any conservation of physical quantities? Of course, I can also say the conservation associated with the “identity operator” (that is, conservation of probability). $\\langle\\varphi|\\mathcal{F}^\\dag\\mathbb{I}_{L^2(\\mathbb{R})}\\mathcal{F}|\\varphi\\rangle=\\langle\\varphi|\\mathbb{I}_{L^2(\\mathbb{R})}|\\varphi\\rangle=1$ This is because $[\\mathcal{F},\\mathbb{I}_{L^2(\\mathbb{R})}]=0$ . But all operators commute with the identity operator, so this is nothing special. ","date":"2024-01-26","objectID":"/parseval/:1:0","tags":["Symmetry"],"title":"Since Parseval's Theorem for the Fourier Transform Represents a Conservation Law, What Symmetry Corresponds to It?","uri":"/parseval/"},{"categories":["Mathematical Methods in Physics"],"content":"Some Properties of the Fourier-Plancherel Operator Property 1: $\\mathcal{F}^\\dag Q\\mathcal{F} = P$ , $\\mathcal{F}P\\mathcal{F}^\\dag = Q$ Where $Q$ and $P$ are the position and momentum operators, defined as: $(Q\\varphi)(x)=x\\varphi(x)$ $(P\\varphi)(x) = -\\mathrm{i}\\varphi^\\prime(x)$ Property 2: $\\mathcal{F}^2=(\\mathcal{F}^*)^2=\\mathcal{P}$ Where $\\mathcal{P}$ is the parity operator, defined as: $(\\mathcal{P}\\varphi)(x)=\\varphi(-x)$ Property 3: $\\mathcal{F}^4=\\mathbb{I}_{L^2(\\mathbb{R})}$ This is because $\\mathcal{P}^2=\\mathbb{I}_{L^2(\\mathbb{R})}$ ","date":"2024-01-26","objectID":"/parseval/:2:0","tags":["Symmetry"],"title":"Since Parseval's Theorem for the Fourier Transform Represents a Conservation Law, What Symmetry Corresponds to It?","uri":"/parseval/"},{"categories":["Mathematical Methods in Physics"],"content":"Conserved Quantities We know that the conservation corresponding to parity symmetry is parity symmetry itself, i.e., the Hamiltonian is invariant under parity transformation. As a review, let’s briefly derive parity conservation. Parity transformation is represented by the parity operator $\\mathcal{P}$. The Hamiltonian is invariant under parity transformation, which can be written as: $\\mathcal{P}\\mathcal{H}\\mathcal{P}^\\dag=\\mathcal{H}$ or $[\\mathcal{H},\\mathcal{P}]=0$ Thus: $\\frac{\\mathrm{d}}{\\mathrm{d}t}\\langle \\mathcal{P} \\rangle = \\frac{\\mathrm{i}}{\\hbar}\\langle[\\mathcal{H},\\mathcal{P}]\\rangle = 0$ i.e., parity is conserved. So, is there a conserved quantity corresponding to the Fourier-Plancherel operator $\\mathcal{F}$? Unfortunately, $\\mathcal{F}$ is not an observable because it is not a self-adjoint operator. However, we can define $\\mathcal{G} = \\frac{\\mathcal{F}+\\mathcal{F}^\\dag}{2}$, making $\\mathcal{G}$ a self-adjoint operator. It corresponds to the Fourier cosine transform. That said, although $\\mathcal{F}$ is not self-adjoint, it is a normal operator, meaning it can be orthogonally diagonalized, although its eigenvalues are not necessarily real. Some authors define observables as normal operators rather than self-adjoint ones. In this sense, $\\mathcal{F}$ can be an observable. If we must have a conserved quantity, it seems not impossible. Suppose there is a Hamiltonian invariant under the Fourier-Plancherel transform: $\\mathcal{F}^\\dag\\mathcal{H}\\mathcal{F} = \\mathcal{H}$ i.e., $[\\mathcal{H},\\mathcal{F}]=0$ then: $\\frac{\\mathrm{d}}{\\mathrm{d}t}\\langle \\mathcal{F} \\rangle = \\frac{\\mathrm{i}}{\\hbar}\\langle[\\mathcal{H},\\mathcal{F}]\\rangle = 0$ At this time, $\\langle\\mathcal{F}\\rangle$ is a conserved quantity. However, I’m not sure what significance this conserved quantity has at the moment. You may wonder if there exists a Hamiltonian satisfying $\\mathcal{F}^\\dag\\mathcal{H}\\mathcal{F} = \\mathcal{H}$. Of course! The well-known harmonic oscillator Hamiltonian $\\mathcal{H}=P^2+Q^2$ satisfies this condition because $$ \\begin{aligned} \\mathcal{F}^\\dag\\mathcal{H}\\mathcal{F}\u0026=\\mathcal{F}^\\dag(P^2+Q^2)\\mathcal{F} \\\\ \u0026=(\\mathcal{F}^\\dag P\\mathcal{F})^2+(\\mathcal{F}^\\dag Q\\mathcal{F})^2 \\\\ \u0026=(\\mathcal{P}^\\dag Q\\mathcal{P})^2+P^2 \\\\ \u0026=(-Q)^2+P^2 \\\\ \u0026=\\mathcal{H} \\end{aligned} $$ ","date":"2024-01-26","objectID":"/parseval/:3:0","tags":["Symmetry"],"title":"Since Parseval's Theorem for the Fourier Transform Represents a Conservation Law, What Symmetry Corresponds to It?","uri":"/parseval/"},{"categories":["Diary"],"content":" Projects Date Remark An FM Radio Receiver based on PYNQ-Z2 and RTL-SDR Jun 2023 Jupyter Notebook App rather than web app WIFI Weather Clock Jan 2023 Haven’t learned LvGL yet … The GUI is rather simple Light Cube Jan 2023 Ordered the BOM and soldered them together, but the firmware is not mine … Conway’s Game of Life Dec 2022 Touch screen used~ ","date":"2023-07-06","objectID":"/side_proj/:0:0","tags":["Embedded Systems"],"title":"Some fun side-projects","uri":"/side_proj/"},{"categories":["EECS"],"content":"1 Basics SPI has three-wire mode and four-wire mode. The three-wire mode consists of three wires - SS (Slave Select), SCK (SPI Clock), and MOSI (Master-In-Slave-Out). The four wire mode has an extra line called MISO (Master-In-Slave-Out). Signals Full Name SS Slave Select SCK SPI Clock MOSI Master-Out-Slave-In MISO Master-In-Slave-Out SPI clock has four modes: CPOL=0/1 and CPHA=0/1. CPOL stands for clock polarity – clock low or high when in idle. CPHA stands for clock phase – data valid at 0 degree or 180 degrees. ","date":"2023-07-01","objectID":"/spi/:0:1","tags":["SPI","Embedded Systems"],"title":"HW \u0026 SW design of the AXI Quad SPI IP core","uri":"/spi/"},{"categories":["EECS"],"content":"2 AXI Quad SPI IP Core When we run out of Zynq PS SPI controllers for some reason, we can turn to PL SPI IP cores, which is called AXI Quad SPI. AXI Quad SPI IP core In the picture, io1_i connects to MISO; ext_spi_clk and s_axi_clk can be connected to a same system clock. ip2intc_irpt can be connected to the Zynq interrupt pl_ps_irq. You can double-click on the IP core to configure clock divider, number of slave selects, and more. ","date":"2023-07-01","objectID":"/spi/:0:2","tags":["SPI","Embedded Systems"],"title":"HW \u0026 SW design of the AXI Quad SPI IP core","uri":"/spi/"},{"categories":["EECS"],"content":"3 Bare-Metal Programming The programming sequence is as follows: First, initialize the SPI controller. Set the value of all registers to their default values. Next, depending on your needs, configure the SPI controller. For example, the clock mode (CPHA and CPOL) and slave select mode (auto or manual). Then, depending on the number of bytes need to be written and read, write to the DTR (Data Transmit Register) the corresponding number of bytes. Each written byte will be shifted into the TX FIFO. Note: If the number of written bytes is n_tx and the number of read bytes is n_rx, then we should not only write n_tx bytes to the DTR, but also write another n_rx “dummy” bytes. This is because SPI, in nature, is a full-duplex protocol – in order to receive n_rx bytes, you also need to send n_rx bytes. Finally, read the bytes out of the DRR (Data Receive Register). When the DRR is read, the read byte will be dequed. Note: depending on the scene, when receiving from the slave, the programmer may need to discard the first n_tx bytes. This is because when the master is sending data/instructions, the slave may not yet respond. Rather, the slave will not respond until the master has sent all of the bytes (instruction \u0026 data). But due to the full-duplex nature of the SPI protocol, the first n_tx dummy bytes will also be pushed into the RX FIFO. Therefore, one may need to discard those bytes. If we use the manual slave select mode, we should assert the SS signal before the transfer and de-assert the SS signal after the transfer. The following code sends two bytes [0x00 0x37] to the slave and reads one byte from the slave. #include \"xspi.h\" // axi quad spi #include \"xparameters.h\" #include \"xstatus.h\" #include \"xplatform_info.h\" #include \"xil_printf.h\" #include \"sleep.h\" #define SPI_DEVICE_ID XPAR_SPI_0_DEVICE_ID #define SPI_BASEADDR XPAR_SPI_0_BASEADDR XSpi Spi; int main() { /* * SPI Initialize */ XSpi_Config *spi_config_ptr; spi_config_ptr = XSpi_LookupConfig(SPI_DEVICE_ID); if (spi_config_ptr == NULL) { return XST_DEVICE_NOT_FOUND; } status = XSpi_CfgInitialize(\u0026Spi, spi_config_ptr, spi_config_ptr-\u003eBaseAddress); if (status != XST_SUCCESS) { return XST_FAILURE; } // Start the SPI driver so that the device is enabled. XSpi_Start(\u0026Spi); // Disable Global interrupt to use polled mode operation XSpi_IntrGlobalDisable(\u0026Spi); /* * 1. Enable master mode. * 2. CPHA = 1, CPOL = 0 * 3. Manual Slave Select * 4. TX/RX FIFO Reset */ u32 control; control = XSpi_ReadReg(SPI_BASEADDR, XSP_CR_OFFSET); control |= XSP_CR_MASTER_MODE_MASK | // Master Mode XSP_CR_CLK_PHASE_MASK | // Clock Phase XSP_CR_MANUAL_SS_MASK | // Manual Slave Select XSP_CR_TXFIFO_RESET_MASK| // TX FIFO Reset XSP_CR_RXFIFO_RESET_MASK // RX FIFO Reset ; XSpi_WriteReg(SPI_BASEADDR, XSP_CR_OFFSET, control); // write [0x00 0x37] and then read one byte XSpi_WriteReg(SPI_BASEADDR, XSP_DTR_OFFSET, 0x00); XSpi_WriteReg(SPI_BASEADDR, XSP_DTR_OFFSET, 0x37); XSpi_WriteReg(SPI_BASEADDR, XSP_DTR_OFFSET, 0x00); /* * SPI write */ XSpi_WriteReg(SPI_BASEADDR, XSP_SSR_OFFSET, 0xE); // slave select // 0xE: 0b1110 // initiate a transfer control = XSpi_ReadReg(SPI_BASEADDR, XSP_CR_OFFSET); control |= XSP_CR_ENABLE_MASK; control \u0026= ~XSP_CR_TRANS_INHIBIT_MASK; XSpi_WriteReg(SPI_BASEADDR, XSP_CR_OFFSET, control); /* * SPI read */ // wait for the transmit FIFO to be empty while (!(XSpi_ReadReg(SPI_BASEADDR, XSP_SR_OFFSET) \u0026 XSP_SR_TX_EMPTY_MASK)); control = XSpi_ReadReg(SPI_BASEADDR, XSP_CR_OFFSET); control |= XSP_CR_TRANS_INHIBIT_MASK; XSpi_WriteReg(SPI_BASEADDR, XSP_CR_OFFSET, control); // read data receive register while ((XSpi_ReadReg(SPI_BASEADDR, XSP_SR_OFFSET) \u0026 XSP_SR_RX_EMPTY_MASK) == 0) { data = XSpi_ReadReg(SPI_BASEADDR, XSP_DRR_OFFSET); } // we know (in advance) that the slave will return one byte, so we know this loop will be executed three times. // slave de-select XSpi_WriteReg(SPI_BASEADDR, XSP_SSR_OFFSET, 0xF); // 0xF: 0b1111 xil_printf(\"MISO: 0x%x\\n\\r\", data); return 0; } ","date":"2023-07-01","objectID":"/spi/:0:3","tags":["SPI","Embedded Systems"],"title":"HW \u0026 SW design of the AXI Quad SPI IP core","uri":"/spi/"},{"categories":["EECS"],"content":"4 Linux Programming In order to use the AXI Quad SPI IP core in Linux, we should add a spidev node to the device tree, so that we could achieve SPI communication by reading from or writing to the /dev/spidevx.y device. In /dev/spidevx.y, x stands for the x-th SPI controller, and y stands for the y-th chip. The device tree is usually initialized during boot-up and is read-only. However, after the 4.14 version of the Linux kernel, we can use the “device tree overlay” (briefed as DTO from now on) to dynamically add incremental device trees. Below is a code snippet of DTO. /dts-v1/; /plugin/; / { fragment@0 { target = \u003c\u0026amba\u003e; overlay0: __overlay__ { axi_quad_spi_0: axi_quad_spi@80000000 { ... status = \"okay\"; #address-cells = \u003c1\u003e; #size-cells = \u003c0\u003e; spidev0: spidev@0 { compatible = \"spidev\"; reg = \u003c0\u003e; spi-max-frequency = \u003c5000000\u003e; }; }; }; }; }; Some points to be noted: We should add one more line /plugin/; after /dts-v1/; to show that this is an DTO file rather than an ordinary device tree file. target stands for which node is to be modified. Here it is \u003c\u0026amba\u003e, and it will be extended to the phandle of the node which has the symbol amba. For example, if the phandle of amba is 70, then \u003c\u0026amba\u003e virtually stands for \u003c70\u003e. A phandle uniquely denotes a node and is usually allocated by the device tree compiler. In most cases, we should add the -@ compiler option, which enables support for symbol. Otherwise we can only reference the nodes by their phandle, which requires de-compiling the device tree binaries. ","date":"2023-07-01","objectID":"/spi/:0:4","tags":["SPI","Embedded Systems"],"title":"HW \u0026 SW design of the AXI Quad SPI IP core","uri":"/spi/"},{"categories":["CS"],"content":"Let’s look at a code snippet in the Jupyter Notebook: import time import ipywidgets as widgets from IPython.display import display slider = widgets.IntSlider() display(slider) while True: print(slider.value) time.sleep(1) IntSlider is an interactive Jupyter Notebook widget. When the user interacts with the slider, the slider value should change. However, if you run the above codes, you will find that the printed value of the slider won’t change at all – it will be stuck at its initial value. In fact, the UI elements won’t be updated until all code blocks are finished executing. In other words, the update of the UI elements is blocked by the execution of any code block. In order to poll the states of UI elements inside a running loop, we can use the jupyter-ui-poll library. The modified codes are as follows: import time import ipywidgets as widgets from IPython.display import display from jupyter_ui_poll import ui_events slider = widgets.IntSlider() display(slider) while True: with ui_events() as poll: poll(1) # poll one event print(slider.value) time.sleep(1) ","date":"2023-07-01","objectID":"/ui_poll/:0:0","tags":["Python","Jupyter Notebook"],"title":"Polling Jupyter widget UI events in runtime","uri":"/ui_poll/"},{"categories":null,"content":"Still Working on it :) ","date":"2023-06-22","objectID":"/publication/:0:1","tags":null,"title":"Publication","uri":"/publication/"},{"categories":["CS"],"content":"Recently, I have been using Python coroutines/asynchronous programming in a project. Now, I will summarize my experience. ","date":"2023-06-13","objectID":"/python_asyncio/:0:0","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["CS"],"content":"Import import asyncio If asyncio is to be used in an IPython environment, we have to add two more lines： import nest_asyncio nest_asyncio.apply() import asyncio ","date":"2023-06-13","objectID":"/python_asyncio/:1:0","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["CS"],"content":"Coroutines Coroutines are the core of asynchronous programming in Python. To define a coroutine, you need to use async def. async def main(): # do something print(\"Hello world!\") To execute the coroutine, you cannot directly call main(). Instead, you need to use run(): asyncio.run(main()) To nest one coroutine within another, similar to nesting one function within another, you can use await: import asyncio async def coro_1(): print(\"I am the coroutine 1.\") async def coro_2(): print(\"I am the coroutine 2.\") async def main(): await coro_1() await coro_2() print(\"Hello World!\") asyncio.run(main()) When nesting coroutines within another coroutine, you need to use await to invoke them. If you write it directly as follows: async def main(): coro_1() coro_2() print(\"Hello World!\") You will receive the following warnings: 01.py:10: RuntimeWarning: coroutine 'coro_1' was never awaited coro_1() RuntimeWarning: Enable tracemalloc to get the object allocation traceback 01.py:11: RuntimeWarning: coroutine 'coro_2' was never awaited coro_2() RuntimeWarning: Enable tracemalloc to get the object allocation traceback Hello World! As can been seen, coro_1 and coro_2 has not been called. ","date":"2023-06-13","objectID":"/python_asyncio/:2:0","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["CS"],"content":"Tasks We would lose the point if we use coroutines in the way as shown in the previous section. The true significance of coroutines lies in their ability to be executed concurrently. Consider the following code: import asyncio async def coro_1(): print(\"I am the coroutine 1.\") async def coro_2(): print(\"I am the coroutine 2.\") async def main(): task_1 = asyncio.create_task(coro_1()) task_2 = asyncio.create_task(coro_2()) await task_1 await task_2 print(\"Hello World!\") asyncio.run(main()) In this code, we used create_task to create tasks task_1 and task_2 for coro_1 and coro_2 respectively. They are actually executed together. To verify this, we can add some delays: import asyncio import time async def coro_1(): print(\"I am the coroutine 1.\") await asyncio.sleep(1) async def coro_2(): print(\"I am the coroutine 2.\") await asyncio.sleep(1) async def main(): st = time.time() task_1 = asyncio.create_task(coro_1()) task_2 = asyncio.create_task(coro_2()) await task_1 await task_2 et = time.time() print(\"Elapsed: %f s\" % (et - st)) asyncio.run(main()) The results are: I am the coroutine 1. I am the coroutine 2. Elapsed: 1.001298 s If we do not use create_task but rather directly await the two coroutines: import asyncio import time async def coro_1(): print(\"I am the coroutine 1.\") await asyncio.sleep(1) async def coro_2(): print(\"I am the coroutine 2.\") await asyncio.sleep(1) async def main(): st = time.time() await coro_1() await coro_2() et = time.time() print(\"Elapsed: %f s\" % (et - st)) asyncio.run(main()) Then the outcomes are: I am the coroutine 1. I am the coroutine 2. Elapsed: 2.002527 s In other words, if we don’t use create_task to create tasks, coro_2() will not be executed until coro_1() has finished. From this example, we can observe that the true meaning of await is “wait for the task to complete”. Furthermore, we can use gather to run multiple coroutines concurrently: import asyncio async def coro_1(): print(\"Coroutine 1 starts\") await asyncio.sleep(1) print(\"Coroutine 1 finishes\") async def coro_2(): print(\"Coroutine 2 starts\") await asyncio.sleep(2) print(\"Coroutine 2 finishes\") async def main(): print(\"Starting main coroutine\") await asyncio.gather(coro_1(), coro_2()) print(\"Main coroutine finished\") asyncio.run(main()) ","date":"2023-06-13","objectID":"/python_asyncio/:3:0","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["CS"],"content":"Asynchronous For loop Now we can look at async for. import asyncio import time async def async_generator(): for i in range(10): await asyncio.sleep(1) yield i async def custom_coroutine(): async for item in async_generator(): print(item) time.sleep(1) st = time.time() asyncio.run(custom_coroutine()) et = time.time() print(\"Elasped: %f s\" % (et - st)) The result is: 0 1 2 3 4 5 6 7 8 9 Elasped: 10.013134 s By combining async for and yield, we can create an asynchronous generator. In reality, an asynchronous generator is an instance of a class that has __aiter__ and __anext__ methods. Here’s an equivalent implementation of the previous code: import asyncio import time class AsyncGenerator: def __init__(self, N): self.i = 0 self.N = N def __aiter__(self): return self async def __anext__(self): i = self.i if i \u003e= self.N: raise StopAsyncIteration await asyncio.sleep(1) self.i += 1 return i async def main(): async for p in AsyncGenerator(10): print(p) st = time.time() asyncio.run(main()) et = time.time() print(\"Elasped: %f s\" % (et - st)) ","date":"2023-06-13","objectID":"/python_asyncio/:3:1","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["CS"],"content":"Example At last, I will give an example of using asyncio in a project: sdr = RtlSdr() sdr.center_freq = 92_700_000 # an FM radio station running at 92.7MHz async def main(): async for data in sdr.stream(): # perform FM demodulation audio = fm_demodulation(data) # play the audio display(Audio(audio, autoplay=True, rate=48000, normalize=False)) asyncio.run(main()) This is an FM radio application that uses async for to read data from the receiver, demodulate the data into audio signals and finally play the audio in a streamed manner. In other words, we are essentially processing and streaming the audio data in real time. ","date":"2023-06-13","objectID":"/python_asyncio/:4:0","tags":["Python"],"title":"Python coroutines and asyncio","uri":"/python_asyncio/"},{"categories":["EECS"],"content":"Windows Set a network adapter that can access the internet in the Control Panel and share it with Ethernet. The IP address of Ethernet will change to 192.168.137.1 (this is the default behavior in Windows). Then, set the gateway as 192.168.137.1 in the terminal of the development board: sudo route add default gw 192.168.137.1 Set the IP address as 192.168.137.x, where x is any value except 1 and 255 (gateway address and broadcast address): sudo ifconfig \u003cyour-ifdev\u003e 192.168.3.x If it still doesn’t work, check if the DNS in /etc/resolv.conf is correct. You can add a line: nameserver 8.8.8.8 ","date":"2023-06-12","objectID":"/boards_networking/:0:1","tags":["Networking","Linux"],"title":"Access the Internet from evaluation boards via PC","uri":"/boards_networking/"},{"categories":["EECS"],"content":"Ubuntu Change the IPv4 of Ethernet in the PC’s network settings to shared mode. The IP address of Ethernet will change to 10.42.0.1 (this is default in Ubuntu Linux, but I’m not sure if it is the same in other Linux distributions). Then, set the gateway as 10.42.0.1 in the terminal of the development board: sudo route add default gw 10.42.0.1 Set the IP address to any IP within the same subnet, as long as the subnet part of the IP is not the gateway address and broadcast address: sudo ifconfig 10.42.0.2 If it still doesn’t work, check if the DNS in /etc/resolv.conf is correct. You can add a line: nameserver 8.8.8.8 ","date":"2023-06-12","objectID":"/boards_networking/:0:2","tags":["Networking","Linux"],"title":"Access the Internet from evaluation boards via PC","uri":"/boards_networking/"},{"categories":["EECS"],"content":"Troubleshooting First, check if you can ping the host. If you can’t ping the host, the gateway may not be set correctly. If you can ping the host but not the external IP, it may be because you haven’t shared the Ethernet for the development board in the PC, or the proxy is not set correctly, or the gateway is not set correctly. If you can ping the external IP but not the domain name, it is most likely a DNS configuration issue. ","date":"2023-06-12","objectID":"/boards_networking/:0:3","tags":["Networking","Linux"],"title":"Access the Internet from evaluation boards via PC","uri":"/boards_networking/"},{"categories":["CS"],"content":"Xilinx’s toolchain consumes soooo much memory! Sometimes it causes the system to freeze… After all, my laptop only has 8GB of RAM. So there’s no other choice but to add virtual memories. After increase the swapfile, the system performance has improved a lot: sudo swapoff /swapfile sudo dd if=/dev/zero of=/swapfile bs=1M count=16384 sudo mkswap /swapfile sudo swapon /swapfile ","date":"2023-06-12","objectID":"/increase_swapfile/:0:0","tags":["Storage/Partition","Linux"],"title":"Increasing the swapfile for Linux","uri":"/increase_swapfile/"},{"categories":["EECS"],"content":"To partition and format an SD card in Linux, follow these steps: First, connect the SD card to your PC. Then, use the fdisk command in the bash command line to partition the SD card. Finally, use the mkfs command to create a file system (format) on the SD card. The main commands are as follows: First, use sudo fdisk -l to confirm which device in /dev corresponds to the SD card. Assuming we have determined that the SD card corresponds to /dev/sde, we can enter sudo fdisk /dev/sde to start the partitioning operation: Enter d to delete partitions. Keep entering until all partitions are deleted. Enter n to create the first partition as the boot partition. Enter t to change the partition type to W95 FAT32. Boot partitions are generally of this type. Enter a to set the partition as the boot partition. Enter n to create the second partition as the root file system partition. Since the root file system partition type is usually Linux, and the default partition type is already Linux, there is no need to enter t to change the partition type. Enter w to save and exit. Finally, after partitioning, we can create the file systems: sudo mkfs.vfat -F 32 -n \"BOOT\" /dev/sde1 sudo mkfs.ext4 -L \"rootfs\" /dev/sde2 In the above code, the first line creates a FAT32 file system for the boot partition and names it as BOOT. The second line creates an ext4 file system for the root file system partition and names it as rootfs. ","date":"2023-06-12","objectID":"/linux_sd_card/:0:0","tags":["Storage/Partition","Linux"],"title":"Partitioning and formatting SD cards on Linux","uri":"/linux_sd_card/"},{"categories":["CS"],"content":"To mount the EFI partitions, run the following commands in PowerShell: diskpart list disk # make sure which is the disk that contains the EFI partition. usually 0 select disk 0 list partition # make sure which is the EFI partition. normally 0 select disk 0 assign letter=z Then we will mount the EFI partition as drive Z. This operation may be useful when using a dual-boot system with Windows and Ubuntu: Sometimes, after removing the Ubuntu system, only the Windows system remains, but the GRUB interface still appears every time the computer boots. In this case, you need to delete the old Ubuntu EFI partition. ","date":"2023-06-12","objectID":"/efi_partition/:0:0","tags":["Storage/Partition","Linux"],"title":"Mounting the EFI partition in Windows OS","uri":"/efi_partition/"},{"categories":["Mathematical Physics"],"content":"1 Differential Forms Before introducing the concept of curl, we need to first introduce differential forms and the exterior derivative. An n-form can be defined as an alternating multilinear mapping $\\omega:(T_p^*M)^n\\rightarrow \\mathbb{R}$. It maps multiple vectors to a real number. Additionally, it satisfies the alternating property, meaning that exchanging two input vectors results in an output multiplied by a negative sign. Therefore, an n-form can be explicitly defined as follows: $$ \\omega^1\\wedge \\omega^2\\wedge\\cdots\\wedge \\omega^n(v_1,v_2,\\cdots,v_n)= \\begin{vmatrix} \\omega^1(v_1) \u0026 \\cdots \u0026 \\omega^{1}(v_n) \\\\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\\ \\omega^n(v_1) \u0026 \\cdots \u0026 \\omega^n(v_n) \\end{vmatrix}\\in \\mathbb{R} $$ The set of all n-forms on $T_p^*M$ can be written as $\\bigwedge_n(T_p^*M)$. As for “differential forms,” they differ from “forms” in that they are the “forms’ fields,” meaning that at each point on the manifold, there resides a form. In other words, “forms” are specific to a point, while “differential forms” are specific to the entire manifold. ","date":"2023-06-10","objectID":"/curl_in_n_dimension/:0:1","tags":["Vector Calculus"],"title":"Curl in high dimensions","uri":"/curl_in_n_dimension/"},{"categories":["Mathematical Physics"],"content":"2 Exterior Derivative The definition of the exterior derivative is as follows: The exterior derivative is a mapping $\\mathrm{d}: \\bigwedge^n(T_p^*M) \\rightarrow \\bigwedge^{(n+1)}(T_p^*M)$, which acts on an differential n-form: $\\varphi = \\sum_I f_I \\mathrm{d}x^I = \\sum_{(i_1,\\cdots,i_n)} f_{(i_1,\\cdots,i_n)} \\mathrm{d}x^{i_1}\\wedge\\cdots\\wedge\\mathrm{d}x^{i_n}$, and yields an differential (n+1)-form: $\\mathrm{d}\\varphi = \\sum_I \\sum_i \\frac{\\partial f_I}{\\partial x_i} \\mathrm{d}x_i\\wedge x_I$. Comment These indices are quite all over the place. Let’s see the example below instead. Example For a differential 2-form on a 3-dimensional manifold: $\\varphi = z^2\\mathrm{d}x\\wedge\\mathrm{d}y + x\\sin y\\,\\mathrm{d}y\\wedge \\mathrm{d}z$, its exterior derivative is given by: $$\\begin{aligned} \\mathrm{d}\\varphi \u0026= \\left(\\frac{\\partial z^2}{\\partial x}\\mathrm{d}x + \\frac{\\partial z^2}{\\partial y}\\mathrm{d}y + \\frac{\\partial z^2}{\\partial z}\\mathrm{d}z\\right)\\wedge\\mathrm{d}x\\wedge\\mathrm{d}y \\\\\\ \u0026 + \\left(\\frac{\\partial (x\\sin y)}{\\partial x}\\mathrm{d}x + \\frac{\\partial (x\\sin y)}{\\partial y}\\mathrm{d}y + \\frac{\\partial (x\\sin y)}{\\partial z}\\mathrm{d}z\\right)\\wedge\\mathrm{d}y\\wedge\\mathrm{d}z \\\\\\ \u0026= (2z + \\sin y)\\,\\mathrm{d}x\\wedge\\mathrm{d}y\\wedge\\mathrm{d}z \\end{aligned} $$ ","date":"2023-06-10","objectID":"/curl_in_n_dimension/:0:2","tags":["Vector Calculus"],"title":"Curl in high dimensions","uri":"/curl_in_n_dimension/"},{"categories":["Mathematical Physics"],"content":"3 Curl in N Dimensions In the following example, we will explore the relationship between curl and exterior derivative: Example: On a three-dimensional manifold, when the exterior derivative operator $\\mathrm{d}$ acts on a 1-form, it yields a 2-form: $$ \\begin{aligned} \\mathrm{d}(f_i\\mathrm{d}x^i) \u0026= \\frac{\\partial f_1}{\\partial x^1}\\mathrm{d}x^1\\wedge\\mathrm{d}x^1 + \\frac{\\partial f_1}{\\partial x^2}\\mathrm{d}x^2\\wedge\\mathrm{d}x^1 + \\frac{\\partial f_1}{\\partial x^3}\\mathrm{d}x^3\\wedge\\mathrm{d}x^1 \\\\ \u0026\\quad+ \\frac{\\partial f_2}{\\partial x^1}\\mathrm{d}x^1\\wedge\\mathrm{d}x^2 + \\frac{\\partial f_2}{\\partial x^2}\\mathrm{d}x^2\\wedge\\mathrm{d}x^2 + \\frac{\\partial f_2}{\\partial x^3}\\mathrm{d}x^3\\wedge\\mathrm{d}x^2 \\\\ \u0026\\quad+ \\frac{\\partial f_3}{\\partial x^1}\\mathrm{d}x^1\\wedge\\mathrm{d}x^3 + \\frac{\\partial f_3}{\\partial x^2}\\mathrm{d}x^2\\wedge\\mathrm{d}x^3 + \\frac{\\partial f_3}{\\partial x^3}\\mathrm{d}x^3\\wedge\\mathrm{d}x^3 \\end{aligned} $$ Applying the Hodge star operator once, it yields a 1-form: $$ \\begin{aligned} \\star \\mathrm{d}(f_i\\mathrm{d}x^i) \u0026= \\phantom{+\\frac{\\partial f_1}{\\partial x^2}\\mathrm{d}x^3} -\\frac{\\partial f_1}{\\partial x^2}\\mathrm{d}x^3 + \\frac{\\partial f_1}{\\partial x^3}\\mathrm{d}x^2 \\\\ \u0026\\phantom{=}+ \\frac{\\partial f_2}{\\partial x^1}\\mathrm{d}x^3 \\phantom{\\,\\,\\,-\\frac{\\partial f_1}{\\partial x^2}\\mathrm{d}x^3} -\\frac{\\partial f_2}{\\partial x^3}\\mathrm{d}x^1 \\\\ \u0026\\phantom{=} -\\frac{\\partial f_3}{\\partial x^1}\\mathrm{d}x^2 + \\frac{\\partial f_3}{\\partial x^2}\\mathrm{d}x^1 \\\\ \u0026= \\left(\\frac{\\partial f_3}{\\partial x^2}-\\frac{\\partial f_2}{\\partial x^3}\\right)\\mathrm{d}x^1 + \\left(\\frac{\\partial f_1}{\\partial x^3}-\\frac{\\partial f_3}{\\partial x^1}\\right)\\mathrm{d}x^2 + \\left(\\frac{\\partial f_2}{\\partial x^1}-\\frac{\\partial f_1}{\\partial x^2}\\right)\\mathrm{d}x^3 \\end{aligned} $$ It can be expressed as the inner product of the curl and a tangent vector: $\\star\\mathrm{d}f:\\star\\mathrm{d}f(v)=\\langle \\nabla \\times f^{\\sharp}\\mid v \\rangle$ For now, without introducing the Hodge star operator, you only need to know that on a three-dimensional manifold, the Hodge operator applied to an differential n-form yields an differential (3-n)-form. Specifically, if the manifold is equipped with an inner product $\\langle\\mathrm{d}x^i,\\mathrm{d}x^j\\rangle=\\delta^{j}_i$, then we have: $$ \\star(\\mathrm{d}x^i \\wedge \\mathrm{d}x^j) = \\mathrm{d}x^k $$ $$\\star\\mathrm{d}x^i = \\mathrm{d}x^j\\wedge \\mathrm{d}x^k$$ $$\\star(f\\, \\mathrm{d}x^i \\wedge\\mathrm{d}x^j\\wedge \\mathrm{d}x^k) = f$$ $$\\star f = f\\mathrm{d}x^i \\wedge\\mathrm{d}x^j\\wedge \\mathrm{d}x^k$$ where $(i,j,k)$ is an even permutation of $(1,2,3)$. Also, note that we use a raised symbol: $\\sharp$. This is because the curl operates on tangent vector fields, not cotangent vector fields. Specifically, the curl $\\nabla\\times$ maps a tangent vector field to another tangent vector field: $\\Gamma(TM)\\rightarrow \\Gamma(TM)$. However, $f\\in \\Gamma(T^*M)$ is a cotangent vector field (a 1-form), so we first need to “raise” it to a tangent vector field: $f^\\sharp\\in \\Gamma(TM)$. This is actually the well-known index raising and lowering in physics: $g^{ij}X_i=X^j$. Similarly, $\\flat$ represents lowering a tangent vector field to a cotangent vector field: $g_{ij}X^i=X_j$. Returning to the previous example, we have a canonical isomorphism induced by the inner product: $$ \\sharp: \\star \\mathrm{d}f \\mapsto \\nabla\\times f^\\sharp $$ or written as: $$ (\\star \\mathrm{d}f)^\\sharp = \\nabla\\times f^\\sharp $$ or written as: $$ \\star \\mathrm{d}f(v) = \\langle\\nabla\\times f^\\sharp\\mid v\\rangle $$ where $f$ is a 1-form. We can also write it as: $\\nabla\\times F=(\\star \\mathrm{d} (F^\\flat))^\\sharp$ where $F=f^\\sharp$ is a tangent vector field, and $f$ is a cotangent vector field (a 1-form). Inspired by the above example, we can define the curl as: $$ \\begin{aligned} (\\nabla\\times) : \\quad \u0026 \\Gamma(TM)\\rightarrow \\Gamma\\left(\\bigwedge^{n-2}TM\\right) \\\\ \u0026 F \\mapsto (\\star \\mathrm{d} (F^\\flat))^\\sharp \\end{aligned} $$ whe","date":"2023-06-10","objectID":"/curl_in_n_dimension/:0:3","tags":["Vector Calculus"],"title":"Curl in high dimensions","uri":"/curl_in_n_dimension/"},{"categories":["Mathematical Physics"],"content":"1 Introduction In physics textbooks, we often come across the terms “pseudo-vector” and “pseudo-scalar.” In fact, on a 3-dimensional manifold, a “pseudo-vector” is the exterior product of two tangent vectors, denoted as $v\\in T_pM\\wedge T_pM=\\bigwedge^2(T_pM)$, while a “pseudo-scalar” is the exterior product of three tangent vectors, denoted as $s\\in T_pM\\wedge T_pM\\wedge T_pM=\\bigwedge^3(T_pM)$. When equipped with an inner product (or non-degenerate bilinear form), there exists a Hodge duality between $\\bigwedge^2(T_pM)$ and $\\bigwedge^1(T_pM)$, which leads us to mistakenly consider the pseudo-vector as a vector. Similarly, due to the Hodge duality between $\\bigwedge^3(T_pM)$ and $\\bigwedge^0(T_pM)$ (scalar fields), we mistakenly treat the pseudo-scalar as a scalar. In fact, a pseudo-vector on a 3-dimensional manifold is mathematically equivalent to a 2-vector (bivector). It is similar in definition to a 2-form, with the distinction that a 2-vector belongs to the exterior product of the tangent space, $v\\in\\bigwedge^2(T_pM)$, while a 2-form belongs to the exterior product of the cotangent space, $\\omega\\in\\bigwedge^2(T_p^*M)$. ","date":"2023-06-10","objectID":"/pseudovectors/:1:0","tags":["Exterior Algebra"],"title":"What on earth are the pseudovectors?","uri":"/pseudovectors/"},{"categories":["Mathematical Physics"],"content":"2 Space Inversion Recalling why we call “pseudo-vectors” pseudo-vectors in the first place. It is because they exhibit exotic behaviors under space inversion transformations. However, if we consider them as the exterior product of two vectors, all the peculiar behaviors can be explained. Specifically, as shown in the figure below, under a space inversion transformation, the magnetic field reverses its direction. It’s similar to you moving in one direction, but the reflection of you in the mirror moves in the opposite direction, which is a spooky and paranormal event. The magnetic field is a bivector. If you consider it as a vector, you will encounter this spooky phenomenon in the picture: the magnetic field takes a look in the mirror and finds its head turned into feet. In fact, the magnetic field at point $p$ is not a vector but a bivector, denoted as $B|_p\\in (T_pM)\\wedge (T_pM)$. Its basis consists of $\\frac{\\partial}{\\partial x}\\wedge \\frac{\\partial}{\\partial y},\\,\\frac{\\partial}{\\partial y}\\wedge \\frac{\\partial}{\\partial z},\\,\\frac{\\partial}{\\partial z}\\wedge \\frac{\\partial}{\\partial x}$. So, there is no paranormal event happening. Under a space inversion transformation, both vectors corresponding to the magnetic field actually point in the correct directions. It’s just that humans insist on using the right-hand rule and treat a bivector as a vector, which leads to the appearance of this strange phenomenon. Although bivectors are well-defined in mathematics, we still want to ask the question: How do we visualize a bivector? The answer is: We can represent a bivector as an oriented surface element. Different-shaped surface elements that are parallel to each other and have the same area represent the same bivector. In other words, the same bivector can be depicted in infinitely many ways, as long as they are parallel, have the same area, and have the same orientation. Parallel surface elements with the same area and orientation represent the same bivector. In practice, treating a bivector as a vector using the right-hand rule is only applicable in three-dimensional manifolds. This is because only in three-dimensional manifolds does three minus two exactly equal one. In four-dimensional manifolds, since four minus two equals two, we can only dualize a bivector into another bivector, not a vector. In five-dimensional manifolds, a bivector can be dualized into a trivector, and so on. In an n-dimensional manifold, a bivector can be dualized into an (n-2)-vector. ","date":"2023-06-10","objectID":"/pseudovectors/:2:0","tags":["Exterior Algebra"],"title":"What on earth are the pseudovectors?","uri":"/pseudovectors/"},{"categories":["Mathematical Physics"],"content":"3 Maxwell Equations in Exterior Algebra By the way, the electric field is also a bivector. Its basis consists of $\\frac{\\partial}{\\partial t}\\wedge \\frac{\\partial}{\\partial x},\\,\\frac{\\partial}{\\partial t}\\wedge \\frac{\\partial}{\\partial y},\\,\\frac{\\partial}{\\partial t}\\wedge \\frac{\\partial}{\\partial z}$, involving a time dimension. The reason it doesn’t appear as a “pseudo-vector” is that we only consider space inversion transformations rather than time inversion transformations. If we consider the electromagnetic 2-form in the cotangent bundle (using natural units): $$ \\begin{aligned} F\u0026=E_x\\mathrm{d}t\\wedge\\mathrm{d}x+E_y\\mathrm{d}t\\wedge\\mathrm{d}y+E_z\\mathrm{d}t\\wedge\\mathrm{d}z \\\\ \u0026+B_x \\mathrm{d}y\\wedge\\mathrm{d}z + B_y \\mathrm{d}z\\wedge\\mathrm{d}x + B_z\\mathrm{d}x\\wedge\\mathrm{d}y \\end{aligned} $$ Since we can raise or lower the indices of a tensor with the help of a metric tensor, we can also write a $(2,0)$-type electromagnetic tensor with the basis formed by the exterior product of $\\frac{\\partial}{\\partial t},\\frac{\\partial}{\\partial x},\\frac{\\partial}{\\partial y},\\frac{\\partial}{\\partial z}$. However, for now, let’s adopt the $(0,2)$-type antisymmetric tensor (differential form) because it allows us to utilize the symbol $\\mathrm{d}$ for the exterior derivative. Now, let’s consider the current 1-form: $J=-\\rho\\mathrm{d}t+J_x\\mathrm{d}x+J_y\\mathrm{d}y+J_z\\mathrm{d}z$. Thus, under the Minkowski metric $\\text{diag}(-1,1,1,1)$, Maxwell’s equations can be expressed in a concise form: $$ \\left\\{\\quad \\begin{aligned} \\mathrm{d}F\u0026=0 \\\\ \\star \\, \\mathrm{d}\\star F \u0026= J \\end{aligned} \\right. $$ Here, $\\star$ represents the Hodge star operator (also known as Hodge duality). As can be seen, in the language of exterior algebra, the electromagnetic field is a 2-vector or a 2-form, rather than a vector or a 1-form. The current is indeed a vector (or a 1-form), though. ","date":"2023-06-10","objectID":"/pseudovectors/:3:0","tags":["Exterior Algebra"],"title":"What on earth are the pseudovectors?","uri":"/pseudovectors/"},{"categories":["Diary"],"content":"I recently discovered this fantastic tool for creating personal websites - Hugo. Feels really cool and user-friendly! Plan to gradually migrate my blogs from Zhihu to here in the future. ","date":"2023-06-10","objectID":"/blog_migration/:0:0","tags":["Diary"],"title":"Blog Migration","uri":"/blog_migration/"},{"categories":null,"content":"\rThis is Haifei's personal website. He loves physics (his major), mathematics (not very good at it), and EECS (just for fun). In terms of EECS, he is particularly interested in the embedded systems. In his spare time, he enjoys playing the keyboard, drums, and appreciating jazz music.\rHe graduated from Wuhan University with a bachelor's degree and is currently pursuing a Ph.D. at the National University of Singapore - Centre for Quantum Technologies.\r","date":"2023-06-09","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":["Mathematical Physics"],"content":"TBD Chinese version here ","date":"2023-06-09","objectID":"/differential_geometry/:0:0","tags":["Differential Geometry","Lie Algebra"],"title":"A beginner's guide to differential geometry and Lie algebras","uri":"/differential_geometry/"}]